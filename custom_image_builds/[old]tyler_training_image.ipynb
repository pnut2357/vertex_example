{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b94b317-f824-42f6-836d-a1782199b6b4",
   "metadata": {},
   "source": [
    "# Create Training Model Image for The Containers of The Training Pipeline\n",
    "- utils.py: util functions for training model\n",
    "- diagnosis_utils.py: evaluation metric functions as diagnosis for training model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070655f-d7cf-4f5e-9832-f51e4c5d28ea",
   "metadata": {},
   "source": [
    "## Contigurations from Settings.yml for The Training Model Docker Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dce6e3a-1373-4bca-908b-1de047f2d5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: wmt-mlp-p-oyi-ds-or-oyi-dsns, \n",
      "BASE_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest, \n",
      "FOLDER_NANE: tyler_trainingModelImage\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from util.pipeline_utils import yaml_import\n",
    "\n",
    "file_path = \"/home/jupyter/oyi-ds-vertex/settings.yml\"\n",
    "PARAMS = yaml_import(file_path)\n",
    "\n",
    "ENV = PARAMS['env_flag']\n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "BASE_IMAGE = PARAMS['envs'][ENV]['BASE_IMAGE']\n",
    "FOLDER_NANE = \"tyler_trainingModelImage\"\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}, \\nBASE_IMAGE: {BASE_IMAGE}, \\nFOLDER_NANE: {FOLDER_NANE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "914d45c5-135a-4a82-a129-89e7a039cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_ID = \"wmt-mlp-p-oyi-ds-or-oyi-dsns\"\n",
    "# TAG = \"latest\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30543a2f-b369-440a-8265-92536ef0b4ba",
   "metadata": {},
   "source": [
    "## Write **utils.py** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78353640-f00e-4710-ab29-97924d23c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $FOLDER_NANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404dc4cb-e791-47da-adce-a25201ff1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir -p $FOLDER_NANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da159e5c-f920-451d-b235-9293c6ba96c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainingModelImage/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $FOLDER_NANE/utils.py\n",
    "# Databricks notebook source\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, FunctionTransformer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "from pytz import timezone, utc\n",
    "from datetime import timedelta, datetime\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# assert tf.__version__=='2.3.0'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "import mlflow.pyfunc\n",
    "\n",
    "\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "eligible_clubs= (4738,4787,6309,6310,6311,6312,6321,6324,6436,6535,8149,8164,\n",
    "        8167,8183,8185, 4724,4776,4817,4836,4989,4990,6329,6361,6435,8102,8106,\n",
    "        8107,8119,8196, 8213,8236,8247,4969,4808,6449)\n",
    "\n",
    "eligible_cats= (19, 28, 55, 22, 23, 33, 34, 95, 8, 41, 43, 46, 49, 51, 40, 52, 1, 58, 78, 5,\n",
    "          6, 20, 29, 31, 64, 61, 70, 83, 10, 14, 15, 16, 7, 11, 12, 17, 18, 21,\n",
    "          60, 89, 3, 4, 13, 53, 94, 98, 66, 67, 68, 2, 27, 47, 54, 36, 9, 86)\n",
    "\n",
    "eligible_cats_PI= (19, 28, 55, 22, 23, 33, 34, 95, 8, 41, 43, 46, 49, 51, 40, 52, 1, 58, 78, 5,\n",
    "          6, 20, 29, 31, 64, 61, 70, 83, 10, 14, 15, 16, 7, 11, 12, 17, 18, 21,\n",
    "          60, 89, 3, 4, 13, 53, 94, 98, 66, 67, 68, 2, 27, 47, 54, 36, 9, 86,\n",
    "          37, 38, 39, 42, 44, 48, 56, 57, 72, 93, 96)\n",
    "\n",
    "CONFIG_DIR = \"/dbfs/OYI/prod_artifacts/\"\n",
    "\n",
    "\"\"\"\n",
    "The below classes override TransformerMixin to create transformers either equivalent to\n",
    "their scikit-learn preprocessing/pipeline counter parts but are dataframe aware\n",
    "And all these functions have no side-effects, ie. don't modify the original dataframe\n",
    "\"\"\"\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select columns from pandas dataframe by specifying a list of column names\n",
    "    From this source:\n",
    "    https://github.com/philipmgoddard/pipelines/blob/master/custom_transformers.py\n",
    "    '''\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.loc[:, self.attribute_names].copy()\n",
    "\n",
    "\n",
    "class LogFeaturizer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Log1p transforms inputs, filling NAs with zeroes\n",
    "    '''\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        X[X < 0] = 0\n",
    "        res= np.log1p(X.fillna(0)).values\n",
    "        return pd.DataFrame(res, columns= [i+'_log' for i in X.columns])\n",
    "\n",
    "class ClipFeaturizer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Clips input values below min_value to min_value, and/or\n",
    "    max_value to max value.\n",
    "    '''\n",
    "    def __init__(self, min_value=None, max_value=None):\n",
    "        self.min_value=min_value\n",
    "        self.max_value=max_value\n",
    "\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        X= X.copy()\n",
    "        if self.min_value is not None:\n",
    "            X[X< self.min_value]= self.min_value\n",
    "        if self.max_value is not None:\n",
    "            X[X>self.max_value]= self.max_value\n",
    "        return X\n",
    "\n",
    "# LocationExtractor is used in cancelled items ml model \n",
    "class LocationExtractor(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Extracts location from the Reserve and Sales_Floor_Location column\n",
    "    Currently does not enforce the columns to be strings\n",
    "    Also untested when providing just one column (might break if DataFrameSelector\n",
    "    on a single column returns a Series instead of a DataFrame)\n",
    "    '''\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        newdf= {}\n",
    "        for col in X.columns:\n",
    "            extract= X.loc[:,col].str.extract('(\\w+)-.*').iloc[:,0]\n",
    "            extract= extract.fillna(value='na_{0}'.format(col))\n",
    "            newdf[col+'_proc']= extract\n",
    "        return pd.DataFrame(newdf)\n",
    "\n",
    "class TimeExtractor(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    converts date time to weekday and adds it as a new column\n",
    "    '''\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        newdf= {}\n",
    "        for col in X.columns:\n",
    "            extract= X.loc[:,col].apply(lambda x: x.weekday())\n",
    "            newdf[col+'_proc']= extract\n",
    "        return pd.DataFrame(newdf)\n",
    "\n",
    "class CategoryFeaturizer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Returns Dummy variables of categorical inputs (assumes that they are categorical for now)\n",
    "    Accepts strings and integers\n",
    "    Important: Will work even if the testing dataset that the object is transforming has fewer\n",
    "    categories than the fitted dataset, and so will have the same number of columns as the latter\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.onehot_enc= OneHotEncoder(sparse=False,dtype='int', handle_unknown='ignore')\n",
    "\n",
    "    def fit(self, X,y=None):\n",
    "        self.onehot_enc.fit(X)\n",
    "        self.colnames=[]\n",
    "        for i,col in enumerate(X.columns):\n",
    "            for level in self.onehot_enc.categories_[i]:\n",
    "                self.colnames.append(col+'_'+str(level))\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        res= self.onehot_enc.transform(X)\n",
    "        return pd.DataFrame(res, columns= self.colnames)\n",
    "\n",
    "\n",
    "class ColumnMerge(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Like scikit-learn's FeatureUnion but dataframe aware\n",
    "    '''\n",
    "    def __init__(self,transformer_list, n_jobs=None, transformer_weights=None):\n",
    "        self.tf_list= transformer_list\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for tf_name,tf in self.tf_list:\n",
    "            tf.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        res=[]\n",
    "        for tf_name,tf in self.tf_list:\n",
    "            res.append(tf.transform(X).reset_index(drop=True))\n",
    "        res= pd.concat(res, axis=1)\n",
    "        return res\n",
    "\n",
    "class ModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        df =  pd.DataFrame(self.model.predict(X), columns=['result']).reset_index(drop=True)\n",
    "        df.index = list(df.index)\n",
    "        return df\n",
    "\n",
    "\n",
    "class Stage1_NeuralNetwork(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_shape=None, num_classes=5, batch_size=128, epochs=20, verbose=2):\n",
    "        self.num_classes=num_classes\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.verbose=verbose\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # prepare data for keras-NN\n",
    "        x_train= X.values\n",
    "        self.classes_ = np.unique(y)\n",
    "        y_train= pd.get_dummies(y).values\n",
    "\n",
    "        # define model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, activation='relu', input_shape=(x_train.shape[1],), kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(50, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(25, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(12, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(9, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(self.num_classes, activation='softmax', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(),  metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        self.model.fit(x_train, y_train, batch_size=self.batch_size, epochs=self.epochs, verbose=self.verbose)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        x_test= X.values\n",
    "        return self.model.predict(x_test)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "\n",
    "def stage2_nn(input_dimen=45):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=input_dimen,  activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(70, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class EnsembleClassifierWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, context, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, stage1_preprocessor, stage1_classifiers, stage2_preprocessor, stage2_classifier):\n",
    "        self.stage1_classifiers = stage1_classifiers\n",
    "        self.stage1_preprocessor = stage1_preprocessor\n",
    "        self.stage2_preprocessor = stage2_preprocessor\n",
    "        self.stage2_classifier = stage2_classifier\n",
    "        self.classes_ = None\n",
    "\n",
    "    def _prepare_stage2_data(self, X, y):\n",
    "        X_transformed = self.stage1_preprocessor.transform(X)\n",
    "        s2_x = []\n",
    "        for clf in self.stage1_classifiers:\n",
    "            s2_x.append(clf.predict_proba(X_transformed))\n",
    "        s2_x = np.hstack(s2_x)\n",
    "\n",
    "        X_transformed2 = self.stage2_preprocessor.fit_transform(X)\n",
    "        s2_x = np.hstack([s2_x, X_transformed2.values])\n",
    "        s2_y= y.values\n",
    "        return s2_x, s2_y\n",
    "\n",
    "    def fit(self, X, y, val_x=None, val_y=None):\n",
    "        self.classes_ = np.unique(y)\n",
    "        s2_train_x, s2_train_y = self._prepare_stage2_data(X, y)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        self.stage2_classifier.fit(s2_train_x, s2_train_y, validation_split=0.2, callbacks=[es, mc])\n",
    "        self.stage2_classifier = load_model('best_model.h5')\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Use self.s1_models to get s2 test-data\n",
    "        X_transformed = self.stage1_preprocessor.transform(X)\n",
    "        s2_x = []\n",
    "        for clf in self.stage1_classifiers:\n",
    "            s2_x.append(clf.predict_proba(X_transformed))\n",
    "        s2_test_x = np.hstack(s2_x)\n",
    "        X_transformed2 = self.stage2_preprocessor.transform(X)\n",
    "        s2_test_x = np.hstack([s2_test_x, X_transformed2.values])\n",
    "        return self.stage2_classifier.predict_proba(s2_test_x)\n",
    "\n",
    "class MinMaxScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.min_max_scalar= MinMaxScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.min_max_scalar.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        arr = self.min_max_scalar.transform(X)\n",
    "        return pd.DataFrame(arr, columns=list(X.columns))\n",
    "\n",
    "\n",
    "# Functions for tpr-calculations\n",
    "\n",
    "# input: pandas.core.series.Series Eg: ( [2,4,6,7,8], [2,5,1,8,9], [1,15,3,5,2], [2,7,10,9,1] )\n",
    "# output: [7, 31, 20, 29, 20]\n",
    "def _sum_value_counts(pd_series):\n",
    "    return np.sum(list(pd_series), axis=0).tolist()\n",
    "\n",
    "\n",
    "# input: pandas.core.series.Series Eg: ['No Action Taken, already out for sale', 'Add to picklist', 'Add to picklist', 'Location updated for the item',....]\n",
    "# output: [c1, c2, c3, c4, c5] where (c1-c5) are the value_counts of target-values in the series\n",
    "def _value_counts(pd_series):\n",
    "    arr = []\n",
    "    target_values = ['No Action Taken, already out for sale', 'Add to picklist', 'Updated the on hands quantity for the item', 'Location updated for the item',\n",
    "                     'New price print sign has been printed']\n",
    "    c = Counter(pd_series)\n",
    "    for v in target_values:\n",
    "        arr.append(c[v])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def calculate_daily_tpr(df, level, env, pipeline_root, path='', save=False):\n",
    "    \n",
    "    csv_file_path = f\"{pipeline_root}/all_level_tpr_{env}\"\n",
    "    level_name = '_'.join(_ for _ in level)\n",
    "    groupbycol = level.copy()\n",
    "    groupbycol.append('run_date')\n",
    "\n",
    "    grouped = df.groupby(groupbycol).agg({'log_id':'count','event_note': lambda x:_value_counts(x) }).\\\n",
    "            reset_index().rename(columns={'log_id':'total_action_cnt','event_note':'action_distr'})\n",
    "\n",
    "    joined = grouped.merge(grouped,on=level,how='left',suffixes=('','_1'))\n",
    "\n",
    "    joined = joined.loc[joined['run_date']>joined['run_date_1']]\n",
    "\n",
    "    joined = joined.groupby(groupbycol).agg({'total_action_cnt_1':'sum','action_distr_1': lambda x:_sum_value_counts(x) }).\\\n",
    "        reset_index().rename(columns={'total_action_cnt_1':'total_action_cnt','action_distr_1':'action_distr'})\n",
    "\n",
    "    joined[level_name + '_tpr'] = joined.apply(lambda row: [round(x/row['total_action_cnt'], 3) for x in row['action_distr']], axis=1)\n",
    "    joined.drop(columns=['action_distr', 'total_action_cnt'], inplace=True)\n",
    "\n",
    "    target_values = ['no_action_taken', 'add_to_picklist', 'update_ohq', 'update_loc', 'new_price_sign']\n",
    "    target_cols = [level_name + \"_\" + x + \"_tpr\" for x in target_values]\n",
    "    joined[target_cols] = pd.DataFrame(joined[level_name + '_tpr'].values.tolist(), index= joined.index)\n",
    "\n",
    "    select_col = groupbycol.copy()\n",
    "    select_col += target_cols\n",
    "\n",
    "    if save:\n",
    "        to_dump = joined.loc[joined['run_date']==joined.run_date.max()]\n",
    "        to_dump = to_dump[select_col]\n",
    "        local_path = level_name + '_tpr.joblib'\n",
    "        local_path_csv = level_name + '_tpr.csv'\n",
    "        dump(to_dump, local_path)\n",
    "        storage_path = os.path.join(path, local_path)\n",
    "        blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "        blob.upload_from_filename(local_path)\n",
    "        to_dump.to_csv(f\"{csv_file_path}/{local_path_csv}\", index=False)\n",
    "        \n",
    "\n",
    "    df = df.merge(joined[select_col], on=groupbycol, how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_all_level_tpr(df, env, pipeline_root, path='', save=False):\n",
    "#     if the path is empty construct a path for GCS\n",
    "    if path==\"\":\n",
    "        path = f\"{pipeline_root}/all_level_tpr_{env}\"\n",
    "    \n",
    "    levels = [ ['mkt'], ['reg'], ['club_nbr'], ['cat'], ['item_nbr'], ['club_nbr','cat'], ['state','cat'], ['mkt','cat'], ['reg','cat'] ]\n",
    "    \n",
    "    for level in levels:\n",
    "        df = calculate_daily_tpr(df, level, env, pipeline_root, path=path, save=save)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def load_tpr_features(nosales_test, path, config=None):\n",
    "    df = nosales_test.copy()\n",
    "    levels = [ ['mkt'], ['reg'], ['club_nbr'], ['cat'], ['item_nbr'],\n",
    "              ['club_nbr','cat'], ['state','cat'], ['mkt','cat'], ['reg','cat'] ]\n",
    "\n",
    "    for level in levels:\n",
    "        level_name = '_'.join(_ for _ in level)\n",
    "        file_name = level_name + '_tpr.joblib'\n",
    "\n",
    "        if config is None:\n",
    "            tmp = load(os.path.join(path, file_name)) #not used in production\n",
    "        else:\n",
    "            tmp = load(\"{config['model_path']}/{0}\".format(file_name))\n",
    "\n",
    "        tmp = tmp.drop('run_date', axis=1)\n",
    "        df = df.merge(tmp, on=level, how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_raw_score_thresholds(train):\n",
    "    club_thresh = {}\n",
    "\n",
    "    mins, maxs= {},{}\n",
    "\n",
    "    for club in train.club_nbr.unique():\n",
    "        train_club = train[train.club_nbr==club]\n",
    "        thresholds = np.sort(list(set(np.round(train_club.raw_score.unique(), 4))))\n",
    "\n",
    "        f1_arr = []\n",
    "        prec_arr = []\n",
    "        recall_arr= []\n",
    "        for th in thresholds:\n",
    "            y_pred = list(train_club.raw_score >= th)\n",
    "            y_true = list(train_club.action == True)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1_arr.append(f1)\n",
    "            prec_arr.append(prec)\n",
    "            recall_arr.append(recall)\n",
    "\n",
    "        club_thresh[club] = thresholds[np.argmax(f1_arr)]\n",
    "\n",
    "    return club_thresh\n",
    "\n",
    "\n",
    "\n",
    "def gen_thresholds(df, predictions, classes):\n",
    "    cutoff = (df.run_date.max() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    df = df.reset_index(drop=True)\n",
    "    scores = pd.DataFrame(predictions, columns=classes)\n",
    "\n",
    "    action_cols= ['Add to picklist', 'Location updated for the item',\n",
    "                  'New price print sign has been printed', 'Updated the on hands quantity for the item']\n",
    "    scores['total_score'] = scores.loc[:,action_cols].sum(axis=1)\n",
    "\n",
    "    # 'Location updated for the item', 'New price print sign has been printed'\n",
    "    df['act_bool']= df.event_note.isin(action_cols)*1\n",
    "    df['raw_score'] = scores['total_score']\n",
    "    df['action']= ~(df.event_note.isin(['No Action Taken, already out for sale','No Action Taken, already OFS']))\n",
    "\n",
    "    cols = [\"central_dt\", \"club_nbr\", \"item_nbr\", \"event_note\", \"action\", \"act_bool\", \"run_date\", \"old_nbr\", \"raw_score\"]\n",
    "    df = df[cols]\n",
    "    df_subset = df[pd.to_datetime(df.central_dt) >= cutoff]\n",
    "    np.sort(df_subset.central_dt.unique())\n",
    "    thresh = get_raw_score_thresholds(df_subset)\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def get_config(mode=1):\n",
    "\n",
    "    config_file = os.path.join(CONFIG_DIR,'config.json')\n",
    "\n",
    "    with open(config_file,'r') as f:\n",
    "        config= json.load(f)\n",
    "    return config\n",
    "\n",
    "class CustomizedGaussianNB(GaussianNB):\n",
    "    \"\"\"Cast dtype to 128 float to avoid numerical underflow\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return log-probability estimates for the test vector X.\n",
    "        Overriding function. Check value in jll and set small value to -inf.\n",
    "        Cause np.exp(very small value) will cause numerical underflow.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like, shape = [n_samples, n_classes]\n",
    "            Returns the log-probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # normalize by P(x) = P(f_1, ..., f_n)\n",
    "\n",
    "        # Replace all value smaller than -10000 to -np.inf, when the shape have\n",
    "        # more than 1 column\n",
    "        if len(jll.shape) > 1 and jll.shape[1] > 1:\n",
    "            jll[jll<=-10000] = -np.inf\n",
    "        jll = jll.astype('float128')\n",
    "        log_prob_x = logsumexp(jll, axis=1)\n",
    "        jll = jll.astype('float128')\n",
    "        log_prob_x = log_prob_x.astype('float128')\n",
    "        return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like of shape (n_samples, n_classes)\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.exp(self.predict_log_proba(X).astype('float128'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ad999-3e62-42eb-a868-01fb7a7e6c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c96eff3f-83f3-4f4b-9c5e-d74a9153fda7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write **diagnosis_utils.py** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e55b134-891e-4443-a14d-205c73207a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainingModelImage/diagnosis_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $FOLDER_NANE/diagnosis_utils.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "# from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, f1_score, confusion_matrix, precision_score, auc, recall_score\n",
    "\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds=5):\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    fold_size = int(dataset.shape[0]/n_folds)\n",
    "    folds = list()\n",
    "    start = 0\n",
    "    end = start+fold_size\n",
    "    while(end<dataset.shape[0]):\n",
    "        folds.append(dataset[start:end])\n",
    "        start = end+1\n",
    "        end = start+fold_size\n",
    "    folds.append(dataset[start:dataset.shape[0]])\n",
    "    return folds\n",
    "\n",
    "def cross_validation_score(dataset, n_folds, pipeline, param_grid):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    auc_scores = []\n",
    "    pipelines = []\n",
    "    for param_value in param_grid['C']:\n",
    "        pipeline.set_params(logistic_clf__C = param_value)\n",
    "        cum_score = 0\n",
    "        for n in range(len(folds)):\n",
    "            test_df = folds[n]\n",
    "            train_df = pd.concat(folds[0:n] + folds[n+1:], axis=0)\n",
    "            pipeline.fit(train_df, train_df.event_note)\n",
    "            cum_score += score(test_df, pipeline)\n",
    "        print(\" C:{}  avg auc score:{}\".format(pipeline.get_params()['logistic_clf'].C, cum_score/len(folds)))\n",
    "        auc_scores.append(cum_score/len(folds))\n",
    "        pipelines.append(pipeline)\n",
    "    return auc_scores, pipelines\n",
    "\n",
    "\n",
    "def model_diag(df, predictions, classes): # classes = encoder.classes_   |   pipeline.classes_\n",
    "    df = df.reset_index(drop=True)\n",
    "    ranks = pd.DataFrame(predictions, columns=classes)\n",
    "\n",
    "    ranks['rank']= ranks.loc[:,['Add to picklist', 'Updated the on hands quantity for the item']].sum(axis=1)\n",
    "\n",
    "    # 'Location updated for the item', 'New price print sign has been printed'\n",
    "    df['act_bool']= df.event_note.isin(['Add to picklist', 'Updated the on hands quantity for the item'])*1\n",
    "    df['rank'] = ranks['rank']\n",
    "    #results_df = df.loc[:,['run_date','club_nbr','OLD_NBR','rank','event_note']]\n",
    "\n",
    "    auc_score=roc_auc_score(df['act_bool'], df['rank'])\n",
    "\n",
    "    print(\"AUC under ROC Curve:\\n\", auc_score)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f068fcd6-fc16-4070-98fb-f19a4aad40a3",
   "metadata": {},
   "source": [
    "## Write The Training Model Docker Image of **Dockerfile**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c647f9bb-2c92-4db6-aa18-386b491770e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingModelImage\n"
     ]
    }
   ],
   "source": [
    "!echo $FOLDER_NANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc842fa-02c1-4ff5-9082-365528f11260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainingModelImage/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $FOLDER_NANE/Dockerfile\n",
    "FROM python:3.7-slim\n",
    "\n",
    "RUN pwd\n",
    "RUN ls\n",
    "RUN pwd\n",
    "COPY ./trainingModelImage/utils.py ./app/utils.py\n",
    "COPY ./trainingModelImage/diagnosis_utils.py ./app/diagnosis_utils.py\n",
    "COPY ./requirements.txt ./app/requirements.txt\n",
    "RUN ls\n",
    "\n",
    "WORKDIR ./app\n",
    "RUN apt-get update && apt-get install gcc libffi-dev -y\n",
    "\n",
    "RUN pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f16c9-e377-4c73-a236-76163b552d65",
   "metadata": {},
   "source": [
    "## Build The Training Model Docker Image of **Dockerfile** with Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38f13042-fcf5-4e54-90f0-b9c3ed137e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb21966-b5db-44ab-8e42-00b86ebfcfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_URI=f\"gcr.io/{PROJECT_ID}/oyi-vertex-pipeline:{TAG}\"\n",
    "!docker build -t {BASE_IMAGE} . -f ./$FOLDER_NANE/Dockerfile --progress=plain --no-cache\n",
    "# docker build -t gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest . "
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb4b278c-0482-4ecb-92ba-e361c8383b86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a909a482-a788-41fc-b1db-4082232af190",
   "metadata": {},
   "source": [
    "## Push The Training Model Docker Image to Google Container Registry\n",
    "url of Google Container Registry: https://console.cloud.google.com/gcr/images/wmt-mlp-p-oyi-ds-or-oyi-dsns?project=wmt-mlp-p-oyi-ds-or-oyi-dsns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2fdcb-821b-4377-9286-1c94c481209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push {BASE_IMAGE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55adac2-4c2c-458a-9d64-fb0a9ef362fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716c61a-f1f1-4819-a133-efa8b8c40432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d41d2e-1f04-40db-ba13-250126e679bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
