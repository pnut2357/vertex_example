{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb45a59-a175-4950-80e3-737c198d2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\".\").absolute().parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent) + \"/src/utils\")\n",
    "import pipeline_utils \n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413b3d3f-dbae-48b6-9cba-dbb85dc05797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --COMMIT_ID COMMIT_ID --BRANCH BRANCH\n",
      "                             --is_prod IS_PROD\n",
      "ipykernel_launcher.py: error: the following arguments are required: --COMMIT_ID, --BRANCH, --is_prod\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    args = pipeline_utils.get_args()\n",
    "except:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--COMMIT_ID\", required=True, type=str)\n",
    "    parser.add_argument(\"--BRANCH\", required=True, type=str)\n",
    "    parser.add_argument(\"--is_prod\", required=False, type=str)\n",
    "    sys.args = [\n",
    "        \"--COMMIT_ID\", \"1234\",\n",
    "        \"--BRANCH\", \"dev\",\n",
    "        \"--is_prod\", \"False\",\n",
    "    ]\n",
    "    args = parser.parse_args(sys.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db18f65-5fd1-487b-a4e0-f0121d51abf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS = pipeline_utils.yaml_import(\"/home/jupyter/oyi-ds-vertex/settings.yml\")\n",
    "\n",
    "\n",
    "BRANCH_ID = args.BRANCH\n",
    "is_prod = args.is_prod\n",
    "\n",
    "\n",
    "if BRANCH_ID == \"stage\" and is_prod == \"True\":\n",
    "    BRANCH_ID = \"prod\"\n",
    "    \n",
    "ENV = BRANCH_ID\n",
    "ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fa2de3-250f-4069-938a-11fa1c9e83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: wmt-mlp-p-oyi-ds-or-oyi-dsns, \n",
      "BASE_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest\n",
      "PIPELINE_ROOT: gs://oyi-ds-vertex-pipeline-bucket-nonprod, \n",
      "CLUB_THRESH_PIPELINE_ROOT: gs://oyi-ds-club-score-cutoff-pipeline-bucket-nonprod\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "BASE_IMAGE = PARAMS['envs'][ENV]['BASE_IMAGE']\n",
    "\n",
    "PIPELINE_ROOT = PARAMS['envs']['dev']['PIPELINE_ROOT']\n",
    "CLUB_THRESH_PIPELINE_ROOT = PARAMS['envs']['dev']['CLUB_THRESH_PIPELINE_ROOT']\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}, \\nBASE_IMAGE: {BASE_IMAGE}\")\n",
    "print(f\"PIPELINE_ROOT: {PIPELINE_ROOT}, \\nCLUB_THRESH_PIPELINE_ROOT: {CLUB_THRESH_PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc4e3c3a-857e-4825-881a-dd081fd7ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./baseimage-requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./baseimage-requirements.txt\n",
    "attrs==21.2.0\n",
    "numpy==1.18.1\n",
    "pandas==1.1.4\n",
    "mlflow\n",
    "setuptools==45.2.0\n",
    "h5py==2.10.0\n",
    "keras==2.3.1\n",
    "joblib==0.17.0\n",
    "scikit-learn==0.24.1\n",
    "tensorflow==1.15.4\n",
    "google-cloud-storage==1.44.0\n",
    "google-cloud-aiplatform==1.13.0\n",
    "google-cloud-bigquery\n",
    "fsspec\n",
    "gcsfs\n",
    "db-dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f648d-fcbd-47f7-988e-a0c1d0dd2313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrs==21.2.0\n",
    "# numpy==1.18.1\n",
    "# pandas==1.1.4\n",
    "# mlflow\n",
    "# setuptools==45.2.0\n",
    "# h5py==2.10.0\n",
    "# keras==2.3.1\n",
    "# joblib==1.1.0\n",
    "# scikit-learn==0.24.1\n",
    "# tensorflow==1.15.4\n",
    "# google-cloud-storage==1.44.0\n",
    "# google-cloud-aiplatform==1.13.0\n",
    "# google-cloud-bigquery\n",
    "# fsspec\n",
    "# gcsfs\n",
    "# db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8080a68-2c5c-4190-9aa7-64a60ccdf916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./utils.py\n",
    "# Databricks notebook source\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, FunctionTransformer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "from pytz import timezone, utc\n",
    "from datetime import timedelta, datetime\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# assert tf.__version__=='2.3.0'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "import mlflow.pyfunc\n",
    "\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "eligible_clubs= (4738,4787,6309,6310,6311,6312,6321,6324,6436,6535,8149,8164,\n",
    "        8167,8183,8185, 4724,4776,4817,4836,4989,4990,6329,6361,6435,8102,8106,\n",
    "        8107,8119,8196, 8213,8236,8247,4969,4808,6449)\n",
    "\n",
    "eligible_cats= (19, 28, 55, 22, 23, 33, 34, 95, 8, 41, 43, 46, 49, 51, 40, 52, 1, 58, 78, 5,\n",
    "          6, 20, 29, 31, 64, 61, 70, 83, 10, 14, 15, 16, 7, 11, 12, 17, 18, 21,\n",
    "          60, 89, 3, 4, 13, 53, 94, 98, 66, 67, 68, 2, 27, 47, 54, 36, 9, 86)\n",
    "\n",
    "eligible_cats_PI= (19, 28, 55, 22, 23, 33, 34, 95, 8, 41, 43, 46, 49, 51, 40, 52, 1, 58, 78, 5,\n",
    "          6, 20, 29, 31, 64, 61, 70, 83, 10, 14, 15, 16, 7, 11, 12, 17, 18, 21,\n",
    "          60, 89, 3, 4, 13, 53, 94, 98, 66, 67, 68, 2, 27, 47, 54, 36, 9, 86,\n",
    "          37, 38, 39, 42, 44, 48, 56, 57, 72, 93, 96)\n",
    "\n",
    "CONFIG_DIR = \"/dbfs/OYI/prod_artifacts/\"\n",
    "\n",
    "\"\"\"\n",
    "The below classes override TransformerMixin to create transformers either equivalent to\n",
    "their scikit-learn preprocessing/pipeline counter parts but are dataframe aware\n",
    "And all these functions have no side-effects, ie. don't modify the original dataframe\n",
    "\"\"\"\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select columns from pandas dataframe by specifying a list of column names\n",
    "    From this source:\n",
    "    https://github.com/philipmgoddard/pipelines/blob/master/custom_transformers.py\n",
    "    '''\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.loc[:, self.attribute_names].copy()\n",
    "\n",
    "\n",
    "class LogFeaturizer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Log1p transforms inputs, filling NAs with zeroes\n",
    "    '''\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        X[X < 0] = 0\n",
    "        res= np.log1p(X.fillna(0)).values\n",
    "        return pd.DataFrame(res, columns= [i+'_log' for i in X.columns])\n",
    "\n",
    "class ClipFeaturizer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Clips input values below min_value to min_value, and/or\n",
    "    max_value to max value.\n",
    "    '''\n",
    "    def __init__(self, min_value=None, max_value=None):\n",
    "        self.min_value=min_value\n",
    "        self.max_value=max_value\n",
    "\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        X= X.copy()\n",
    "        if self.min_value is not None:\n",
    "            X[X< self.min_value]= self.min_value\n",
    "        if self.max_value is not None:\n",
    "            X[X>self.max_value]= self.max_value\n",
    "        return X\n",
    "\n",
    "# LocationExtractor is used in cancelled items ml model \n",
    "class LocationExtractor(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Extracts location from the Reserve and Sales_Floor_Location column\n",
    "    Currently does not enforce the columns to be strings\n",
    "    Also untested when providing just one column (might break if DataFrameSelector\n",
    "    on a single column returns a Series instead of a DataFrame)\n",
    "    '''\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        newdf= {}\n",
    "        for col in X.columns:\n",
    "            extract= X.loc[:,col].str.extract('(\\w+)-.*').iloc[:,0]\n",
    "            extract= extract.fillna(value='na_{0}'.format(col))\n",
    "            newdf[col+'_proc']= extract\n",
    "        return pd.DataFrame(newdf)\n",
    "\n",
    "class TimeExtractor(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    converts date time to weekday and adds it as a new column\n",
    "    '''\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        newdf= {}\n",
    "        for col in X.columns:\n",
    "            extract= X.loc[:,col].apply(lambda x: x.weekday())\n",
    "            newdf[col+'_proc']= extract\n",
    "        return pd.DataFrame(newdf)\n",
    "\n",
    "class CategoryFeaturizer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Returns Dummy variables of categorical inputs (assumes that they are categorical for now)\n",
    "    Accepts strings and integers\n",
    "    Important: Will work even if the testing dataset that the object is transforming has fewer\n",
    "    categories than the fitted dataset, and so will have the same number of columns as the latter\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.onehot_enc= OneHotEncoder(sparse=False,dtype='int', handle_unknown='ignore')\n",
    "\n",
    "    def fit(self, X,y=None):\n",
    "        self.onehot_enc.fit(X)\n",
    "        self.colnames=[]\n",
    "        for i,col in enumerate(X.columns):\n",
    "            for level in self.onehot_enc.categories_[i]:\n",
    "                self.colnames.append(col+'_'+str(level))\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        res= self.onehot_enc.transform(X)\n",
    "        return pd.DataFrame(res, columns= self.colnames)\n",
    "\n",
    "\n",
    "class ColumnMerge(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Like scikit-learn's FeatureUnion but dataframe aware\n",
    "    '''\n",
    "    def __init__(self,transformer_list, n_jobs=None, transformer_weights=None):\n",
    "        self.tf_list= transformer_list\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for tf_name,tf in self.tf_list:\n",
    "            tf.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        res=[]\n",
    "        for tf_name,tf in self.tf_list:\n",
    "            res.append(tf.transform(X).reset_index(drop=True))\n",
    "        res= pd.concat(res, axis=1)\n",
    "        return res\n",
    "\n",
    "class ModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        df =  pd.DataFrame(self.model.predict(X), columns=['result']).reset_index(drop=True)\n",
    "        df.index = list(df.index)\n",
    "        return df\n",
    "\n",
    "\n",
    "class Stage1_NeuralNetwork(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_shape=None, num_classes=5, batch_size=128, epochs=20, verbose=2):\n",
    "        self.num_classes=num_classes\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.verbose=verbose\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # prepare data for keras-NN\n",
    "        x_train= X.values\n",
    "        self.classes_ = np.unique(y)\n",
    "        y_train= pd.get_dummies(y).values\n",
    "\n",
    "        # define model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, activation='relu', input_shape=(x_train.shape[1],), kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(50, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(25, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(12, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(9, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(self.num_classes, activation='softmax', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(),  metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        self.model.fit(x_train, y_train, batch_size=self.batch_size, epochs=self.epochs, verbose=self.verbose)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        x_test= X.values\n",
    "        return self.model.predict(x_test)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "\n",
    "def stage2_nn(input_dimen=45):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=input_dimen,  activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(70, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class EnsembleClassifierWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, context, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, stage1_preprocessor, stage1_classifiers, stage2_preprocessor, stage2_classifier):\n",
    "        self.stage1_classifiers = stage1_classifiers\n",
    "        self.stage1_preprocessor = stage1_preprocessor\n",
    "        self.stage2_preprocessor = stage2_preprocessor\n",
    "        self.stage2_classifier = stage2_classifier\n",
    "        self.classes_ = None\n",
    "\n",
    "    def _prepare_stage2_data(self, X, y):\n",
    "        X_transformed = self.stage1_preprocessor.transform(X)\n",
    "        s2_x = []\n",
    "        for clf in self.stage1_classifiers:\n",
    "            s2_x.append(clf.predict_proba(X_transformed))\n",
    "        s2_x = np.hstack(s2_x)\n",
    "\n",
    "        X_transformed2 = self.stage2_preprocessor.fit_transform(X)\n",
    "        s2_x = np.hstack([s2_x, X_transformed2.values])\n",
    "        s2_y= y.values\n",
    "        return s2_x, s2_y\n",
    "\n",
    "    def fit(self, X, y, val_x=None, val_y=None):\n",
    "        self.classes_ = np.unique(y)\n",
    "        s2_train_x, s2_train_y = self._prepare_stage2_data(X, y)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        self.stage2_classifier.fit(s2_train_x, s2_train_y, validation_split=0.2, callbacks=[es, mc])\n",
    "        self.stage2_classifier = load_model('best_model.h5')\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Use self.s1_models to get s2 test-data\n",
    "        X_transformed = self.stage1_preprocessor.transform(X)\n",
    "        s2_x = []\n",
    "        for clf in self.stage1_classifiers:\n",
    "            s2_x.append(clf.predict_proba(X_transformed))\n",
    "        s2_test_x = np.hstack(s2_x)\n",
    "        X_transformed2 = self.stage2_preprocessor.transform(X)\n",
    "        s2_test_x = np.hstack([s2_test_x, X_transformed2.values])\n",
    "        return self.stage2_classifier.predict_proba(s2_test_x)\n",
    "\n",
    "class MinMaxScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.min_max_scalar= MinMaxScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.min_max_scalar.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        arr = self.min_max_scalar.transform(X)\n",
    "        return pd.DataFrame(arr, columns=list(X.columns))\n",
    "\n",
    "\n",
    "# Functions for tpr-calculations\n",
    "\n",
    "# input: pandas.core.series.Series Eg: ( [2,4,6,7,8], [2,5,1,8,9], [1,15,3,5,2], [2,7,10,9,1] )\n",
    "# output: [7, 31, 20, 29, 20]\n",
    "def _sum_value_counts(pd_series):\n",
    "    return np.sum(list(pd_series), axis=0).tolist()\n",
    "\n",
    "\n",
    "# input: pandas.core.series.Series Eg: ['No Action Taken, already out for sale', 'Add to picklist', 'Add to picklist', 'Location updated for the item',....]\n",
    "# output: [c1, c2, c3, c4, c5] where (c1-c5) are the value_counts of target-values in the series\n",
    "def _value_counts(pd_series):\n",
    "    arr = []\n",
    "    target_values = ['No Action Taken, already out for sale', 'Add to picklist', 'Updated the on hands quantity for the item', 'Location updated for the item',\n",
    "                     'New price print sign has been printed']\n",
    "    c = Counter(pd_series)\n",
    "    for v in target_values:\n",
    "        arr.append(c[v])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def calculate_daily_tpr(df, level, env, pipeline_root, path='', save=False):\n",
    "    \n",
    "    csv_file_path = f\"{pipeline_root}/all_level_tpr_{env}\"\n",
    "    level_name = '_'.join(_ for _ in level)\n",
    "    groupbycol = level.copy()\n",
    "    groupbycol.append('run_date')\n",
    "\n",
    "    grouped = df.groupby(groupbycol).agg({'log_id':'count','event_note': lambda x:_value_counts(x) }).\\\n",
    "            reset_index().rename(columns={'log_id':'total_action_cnt','event_note':'action_distr'})\n",
    "\n",
    "    joined = grouped.merge(grouped,on=level,how='left',suffixes=('','_1'))\n",
    "\n",
    "    joined = joined.loc[joined['run_date']>joined['run_date_1']]\n",
    "\n",
    "    joined = joined.groupby(groupbycol).agg({'total_action_cnt_1':'sum','action_distr_1': lambda x:_sum_value_counts(x) }).\\\n",
    "        reset_index().rename(columns={'total_action_cnt_1':'total_action_cnt','action_distr_1':'action_distr'})\n",
    "\n",
    "    joined[level_name + '_tpr'] = joined.apply(lambda row: [round(x/row['total_action_cnt'], 3) for x in row['action_distr']], axis=1)\n",
    "    joined.drop(columns=['action_distr', 'total_action_cnt'], inplace=True)\n",
    "\n",
    "    target_values = ['no_action_taken', 'add_to_picklist', 'update_ohq', 'update_loc', 'new_price_sign']\n",
    "    target_cols = [level_name + \"_\" + x + \"_tpr\" for x in target_values]\n",
    "    joined[target_cols] = pd.DataFrame(joined[level_name + '_tpr'].values.tolist(), index= joined.index)\n",
    "\n",
    "    select_col = groupbycol.copy()\n",
    "    select_col += target_cols\n",
    "\n",
    "    if save:\n",
    "        to_dump = joined.loc[joined['run_date']==joined.run_date.max()]\n",
    "        to_dump = to_dump[select_col]\n",
    "        local_path = level_name + '_tpr.joblib'\n",
    "        local_path_csv = level_name + '_tpr.csv'\n",
    "        dump(to_dump, local_path)\n",
    "        storage_path = os.path.join(path, local_path)\n",
    "        blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "        blob.upload_from_filename(local_path)\n",
    "        to_dump.to_csv(f\"{csv_file_path}/{local_path_csv}\", index=False)\n",
    "        \n",
    "\n",
    "    df = df.merge(joined[select_col], on=groupbycol, how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_all_level_tpr(df, env, pipeline_root, path='', save=False):\n",
    "#     if the path is empty construct a path for GCS\n",
    "    if path==\"\":\n",
    "        path = f\"{pipeline_root}/all_level_tpr_{env}\"\n",
    "    \n",
    "    levels = [ ['mkt'], ['reg'], ['club_nbr'], ['cat'], ['item_nbr'], ['club_nbr','cat'], ['state','cat'], ['mkt','cat'], ['reg','cat'] ]\n",
    "    \n",
    "    for level in levels:\n",
    "        df = calculate_daily_tpr(df, level, env, pipeline_root, path=path, save=save)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def load_tpr_features(nosales_test, path, config=None):\n",
    "    df = nosales_test.copy()\n",
    "    levels = [ ['mkt'], ['reg'], ['club_nbr'], ['cat'], ['item_nbr'],\n",
    "              ['club_nbr','cat'], ['state','cat'], ['mkt','cat'], ['reg','cat'] ]\n",
    "\n",
    "    for level in levels:\n",
    "        level_name = '_'.join(_ for _ in level)\n",
    "        file_name = level_name + '_tpr.joblib'\n",
    "\n",
    "        if config is None:\n",
    "            tmp = load(os.path.join(path, file_name)) #not used in production\n",
    "        else:\n",
    "            tmp = load(\"{config['model_path']}/{0}\".format(file_name))\n",
    "\n",
    "        tmp = tmp.drop('run_date', axis=1)\n",
    "        df = df.merge(tmp, on=level, how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_raw_score_thresholds(train):\n",
    "    club_thresh = {}\n",
    "\n",
    "    mins, maxs= {},{}\n",
    "\n",
    "    for club in train.club_nbr.unique():\n",
    "        train_club = train[train.club_nbr==club]\n",
    "        thresholds = np.sort(list(set(np.round(train_club.raw_score.unique(), 4))))\n",
    "\n",
    "        f1_arr = []\n",
    "        prec_arr = []\n",
    "        recall_arr= []\n",
    "        for th in thresholds:\n",
    "            y_pred = list(train_club.raw_score >= th)\n",
    "            y_true = list(train_club.action == True)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1_arr.append(f1)\n",
    "            prec_arr.append(prec)\n",
    "            recall_arr.append(recall)\n",
    "\n",
    "        club_thresh[club] = thresholds[np.argmax(f1_arr)]\n",
    "\n",
    "    return club_thresh\n",
    "\n",
    "\n",
    "\n",
    "def gen_thresholds(df, predictions, classes):\n",
    "    cutoff = (df.run_date.max() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    df = df.reset_index(drop=True)\n",
    "    scores = pd.DataFrame(predictions, columns=classes)\n",
    "\n",
    "    action_cols= ['Add to picklist', 'Location updated for the item',\n",
    "                  'New price print sign has been printed', 'Updated the on hands quantity for the item']\n",
    "    scores['total_score'] = scores.loc[:,action_cols].sum(axis=1)\n",
    "\n",
    "    # 'Location updated for the item', 'New price print sign has been printed'\n",
    "    df['act_bool']= df.event_note.isin(action_cols)*1\n",
    "    df['raw_score'] = scores['total_score']\n",
    "    df['action']= ~(df.event_note.isin(['No Action Taken, already out for sale','No Action Taken, already OFS']))\n",
    "\n",
    "    cols = [\"central_dt\", \"club_nbr\", \"item_nbr\", \"event_note\", \"action\", \"act_bool\", \"run_date\", \"old_nbr\", \"raw_score\"]\n",
    "    df = df[cols]\n",
    "    df_subset = df[pd.to_datetime(df.central_dt) >= cutoff]\n",
    "    np.sort(df_subset.central_dt.unique())\n",
    "    thresh = get_raw_score_thresholds(df_subset)\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def get_config(mode=1):\n",
    "\n",
    "    config_file = os.path.join(CONFIG_DIR,'config.json')\n",
    "\n",
    "    with open(config_file,'r') as f:\n",
    "        config= json.load(f)\n",
    "    return config\n",
    "\n",
    "class CustomizedGaussianNB(GaussianNB):\n",
    "    \"\"\"Cast dtype to 128 float to avoid numerical underflow\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return log-probability estimates for the test vector X.\n",
    "        Overriding function. Check value in jll and set small value to -inf.\n",
    "        Cause np.exp(very small value) will cause numerical underflow.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like, shape = [n_samples, n_classes]\n",
    "            Returns the log-probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # normalize by P(x) = P(f_1, ..., f_n)\n",
    "\n",
    "        # Replace all value smaller than -10000 to -np.inf, when the shape have\n",
    "        # more than 1 column\n",
    "        if len(jll.shape) > 1 and jll.shape[1] > 1:\n",
    "            jll[jll<=-10000] = -np.inf\n",
    "        jll = jll.astype('float128')\n",
    "        log_prob_x = logsumexp(jll, axis=1)\n",
    "        jll = jll.astype('float128')\n",
    "        log_prob_x = log_prob_x.astype('float128')\n",
    "        return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like of shape (n_samples, n_classes)\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.exp(self.predict_log_proba(X).astype('float128'))\n",
    "    \n",
    "\n",
    "class MLFlowFunc: \n",
    "\n",
    "    def __init__(self, client):\n",
    "        self.mlf_client = client\n",
    "    \n",
    "    def find_latest_version(self, model_name):\n",
    "        return self.mlf_client.get_latest_versions(model_name, stages=[\"None\"])[0].version\n",
    "    \n",
    "    def find_version_status(self, model_name, version):\n",
    "        return self.mlf_client.get_model_version(name=model_name, version=version)\n",
    "    \n",
    "    def model_version_registry(self, model_name, version, stage):\n",
    "        status = self.find_version_status(model_name, version)\n",
    "        self.mlf_client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            stage=stage\n",
    "        )\n",
    "        print(f\"{model_name}/{version}: {status} -> {stage}\")\n",
    "    \n",
    "    def latest_to_registry(self, model_name, stage):\n",
    "        lastest_version = self.find_latest_version(model_name)\n",
    "        self.mlf_client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=lastest_version,\n",
    "            stage=stage\n",
    "        )\n",
    "        \n",
    "    def find_metrics(self, model_name: str, stage: str = \"None\"):\n",
    "        \n",
    "        for metadata in self.mlf_client.get_latest_versions(model_name, stages=[stage]):\n",
    "            if metadata.name == model_name:\n",
    "                run_id = metadata.run_id\n",
    "                metrics = dict(self.mlf_client.get_run(run_id).data.metrics)\n",
    "                return metrics, metadata.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51d4fe-7fa3-405e-a035-4b437d91a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./utils.py\n",
    "# # Databricks notebook source\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, FunctionTransformer, MinMaxScaler\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "# from sklearn.base import TransformerMixin, BaseEstimator, ClassifierMixin\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# # from tensorflow.keras.models import Sequential\n",
    "# # from tensorflow.keras.optimizers import Adam\n",
    "# # from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "# # from tensorflow.keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "# # from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# # from tensorflow.keras.models import load_model\n",
    "\n",
    "# from pytz import timezone, utc\n",
    "# from datetime import timedelta, datetime\n",
    "# import datetime\n",
    "# import time\n",
    "# import logging\n",
    "# import os\n",
    "# import json\n",
    "# from joblib import dump, load\n",
    "# import pickle\n",
    "# from google.cloud import storage\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "# # import tensorflow as tf\n",
    "# # assert tf.__version__=='2.3.0'\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import Adam\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "# from keras.layers import Dense, Activation, Dropout\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras.models import load_model\n",
    "\n",
    "# import mlflow.pyfunc\n",
    "\n",
    "\n",
    "\n",
    "# np.seterr(all='raise')\n",
    "\n",
    "# eligible_clubs= (4738,4787,6309,6310,6311,6312,6321,6324,6436,6535,8149,8164,\n",
    "#         8167,8183,8185, 4724,4776,4817,4836,4989,4990,6329,6361,6435,8102,8106,\n",
    "#         8107,8119,8196, 8213,8236,8247,4969,4808,6449)\n",
    "\n",
    "# eligible_cats= (19, 28, 55, 22, 23, 33, 34, 95, 8, 41, 43, 46, 49, 51, 40, 52, 1, 58, 78, 5,\n",
    "#           6, 20, 29, 31, 64, 61, 70, 83, 10, 14, 15, 16, 7, 11, 12, 17, 18, 21,\n",
    "#           60, 89, 3, 4, 13, 53, 94, 98, 66, 67, 68, 2, 27, 47, 54, 36, 9, 86)\n",
    "\n",
    "# eligible_cats_PI= (19, 28, 55, 22, 23, 33, 34, 95, 8, 41, 43, 46, 49, 51, 40, 52, 1, 58, 78, 5,\n",
    "#           6, 20, 29, 31, 64, 61, 70, 83, 10, 14, 15, 16, 7, 11, 12, 17, 18, 21,\n",
    "#           60, 89, 3, 4, 13, 53, 94, 98, 66, 67, 68, 2, 27, 47, 54, 36, 9, 86,\n",
    "#           37, 38, 39, 42, 44, 48, 56, 57, 72, 93, 96)\n",
    "\n",
    "# CONFIG_DIR = \"/dbfs/OYI/prod_artifacts/\"\n",
    "\n",
    "# \"\"\"\n",
    "# The below classes override TransformerMixin to create transformers either equivalent to\n",
    "# their scikit-learn preprocessing/pipeline counter parts but are dataframe aware\n",
    "# And all these functions have no side-effects, ie. don't modify the original dataframe\n",
    "# \"\"\"\n",
    "\n",
    "# class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "#     '''\n",
    "#     Select columns from pandas dataframe by specifying a list of column names\n",
    "#     From this source:\n",
    "#     https://github.com/philipmgoddard/pipelines/blob/master/custom_transformers.py\n",
    "#     '''\n",
    "#     def __init__(self, attribute_names):\n",
    "#         self.attribute_names = attribute_names\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X.loc[:, self.attribute_names].copy()\n",
    "\n",
    "\n",
    "# class LogFeaturizer(BaseEstimator,TransformerMixin):\n",
    "#     '''\n",
    "#     Log1p transforms inputs, filling NAs with zeroes\n",
    "#     '''\n",
    "#     def fit(self, X,y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self,X):\n",
    "#         X[X < 0] = 0\n",
    "#         res= np.log1p(X.fillna(0)).values\n",
    "#         return pd.DataFrame(res, columns= [i+'_log' for i in X.columns])\n",
    "\n",
    "# class ClipFeaturizer(BaseEstimator,TransformerMixin):\n",
    "#     '''\n",
    "#     Clips input values below min_value to min_value, and/or\n",
    "#     max_value to max value.\n",
    "#     '''\n",
    "#     def __init__(self, min_value=None, max_value=None):\n",
    "#         self.min_value=min_value\n",
    "#         self.max_value=max_value\n",
    "\n",
    "#     def fit(self, X,y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self,X):\n",
    "#         X= X.copy()\n",
    "#         if self.min_value is not None:\n",
    "#             X[X< self.min_value]= self.min_value\n",
    "#         if self.max_value is not None:\n",
    "#             X[X>self.max_value]= self.max_value\n",
    "#         return X\n",
    "\n",
    "# # LocationExtractor is used in cancelled items ml model \n",
    "# class LocationExtractor(BaseEstimator,TransformerMixin):\n",
    "#     '''\n",
    "#     Extracts location from the Reserve and Sales_Floor_Location column\n",
    "#     Currently does not enforce the columns to be strings\n",
    "#     Also untested when providing just one column (might break if DataFrameSelector\n",
    "#     on a single column returns a Series instead of a DataFrame)\n",
    "#     '''\n",
    "#     def fit(self, X,y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self,X):\n",
    "#         newdf= {}\n",
    "#         for col in X.columns:\n",
    "#             extract= X.loc[:,col].str.extract('(\\w+)-.*').iloc[:,0]\n",
    "#             extract= extract.fillna(value='na_{0}'.format(col))\n",
    "#             newdf[col+'_proc']= extract\n",
    "#         return pd.DataFrame(newdf)\n",
    "\n",
    "# class TimeExtractor(BaseEstimator,TransformerMixin):\n",
    "#     '''\n",
    "#     converts date time to weekday and adds it as a new column\n",
    "#     '''\n",
    "#     def fit(self, X,y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self,X):\n",
    "#         newdf= {}\n",
    "#         for col in X.columns:\n",
    "#             extract= X.loc[:,col].apply(lambda x: x.weekday())\n",
    "#             newdf[col+'_proc']= extract\n",
    "#         return pd.DataFrame(newdf)\n",
    "\n",
    "# class CategoryFeaturizer(BaseEstimator,TransformerMixin):\n",
    "#     '''\n",
    "#     Returns Dummy variables of categorical inputs (assumes that they are categorical for now)\n",
    "#     Accepts strings and integers\n",
    "#     Important: Will work even if the testing dataset that the object is transforming has fewer\n",
    "#     categories than the fitted dataset, and so will have the same number of columns as the latter\n",
    "#     '''\n",
    "#     def __init__(self):\n",
    "#         self.onehot_enc= OneHotEncoder(sparse=False,dtype='int', handle_unknown='ignore')\n",
    "\n",
    "#     def fit(self, X,y=None):\n",
    "#         self.onehot_enc.fit(X)\n",
    "#         self.colnames=[]\n",
    "#         for i,col in enumerate(X.columns):\n",
    "#             for level in self.onehot_enc.categories_[i]:\n",
    "#                 self.colnames.append(col+'_'+str(level))\n",
    "#         return self\n",
    "\n",
    "#     def transform(self,X):\n",
    "#         res= self.onehot_enc.transform(X)\n",
    "#         return pd.DataFrame(res, columns= self.colnames)\n",
    "\n",
    "\n",
    "# class ColumnMerge(BaseEstimator, TransformerMixin):\n",
    "#     '''\n",
    "#     Like scikit-learn's FeatureUnion but dataframe aware\n",
    "#     '''\n",
    "#     def __init__(self,transformer_list, n_jobs=None, transformer_weights=None):\n",
    "#         self.tf_list= transformer_list\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         for tf_name,tf in self.tf_list:\n",
    "#             tf.fit(X)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         res=[]\n",
    "#         for tf_name,tf in self.tf_list:\n",
    "#             res.append(tf.transform(X).reset_index(drop=True))\n",
    "#         res= pd.concat(res, axis=1)\n",
    "#         return res\n",
    "\n",
    "# class ModelTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         self.model.fit(X)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         df =  pd.DataFrame(self.model.predict(X), columns=['result']).reset_index(drop=True)\n",
    "#         df.index = list(df.index)\n",
    "#         return df\n",
    "\n",
    "\n",
    "# class Stage1_NeuralNetwork(BaseEstimator, ClassifierMixin):\n",
    "#     def __init__(self, input_shape=None, num_classes=5, batch_size=128, epochs=20, verbose=2):\n",
    "#         self.num_classes=num_classes\n",
    "#         self.batch_size=batch_size\n",
    "#         self.epochs=epochs\n",
    "#         self.verbose=verbose\n",
    "#         self.input_shape = input_shape\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         # prepare data for keras-NN\n",
    "#         x_train= X.values\n",
    "#         self.classes_ = np.unique(y)\n",
    "#         y_train= pd.get_dummies(y).values\n",
    "\n",
    "#         # define model\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(100, activation='relu', input_shape=(x_train.shape[1],), kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(Dense(50, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(Dense(25, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(Dense(12, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(Dense(9, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(Dense(self.num_classes, activation='softmax', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer=Adam(),  metrics=['accuracy'])\n",
    "#         self.model = model\n",
    "#         self.model.fit(x_train, y_train, batch_size=self.batch_size, epochs=self.epochs, verbose=self.verbose)\n",
    "#         return self\n",
    "\n",
    "#     def predict_proba(self, X):\n",
    "#         x_test= X.values\n",
    "#         return self.model.predict(x_test)\n",
    "\n",
    "#     def save(self, path):\n",
    "#         self.model.save(path)\n",
    "\n",
    "\n",
    "# def stage2_nn(input_dimen=45):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(100, input_dim=input_dimen,  activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(70, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(30, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(10, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(5, activation='softmax'))\n",
    "#     model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# class EnsembleClassifierWrapper(mlflow.pyfunc.PythonModel):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def predict(self, context, X):\n",
    "#         return self.model.predict_proba(X)\n",
    "\n",
    "# class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "#     def __init__(self, stage1_preprocessor, stage1_classifiers, stage2_preprocessor, stage2_classifier):\n",
    "#         self.stage1_classifiers = stage1_classifiers\n",
    "#         self.stage1_preprocessor = stage1_preprocessor\n",
    "#         self.stage2_preprocessor = stage2_preprocessor\n",
    "#         self.stage2_classifier = stage2_classifier\n",
    "#         self.classes_ = None\n",
    "\n",
    "#     def _prepare_stage2_data(self, X, y):\n",
    "#         X_transformed = self.stage1_preprocessor.transform(X)\n",
    "#         s2_x = []\n",
    "#         for clf in self.stage1_classifiers:\n",
    "#             s2_x.append(clf.predict_proba(X_transformed))\n",
    "#         s2_x = np.hstack(s2_x)\n",
    "\n",
    "#         X_transformed2 = self.stage2_preprocessor.fit_transform(X)\n",
    "#         s2_x = np.hstack([s2_x, X_transformed2.values])\n",
    "#         s2_y= y.values\n",
    "#         return s2_x, s2_y\n",
    "\n",
    "#     def fit(self, X, y, val_x=None, val_y=None):\n",
    "#         self.classes_ = np.unique(y)\n",
    "#         s2_train_x, s2_train_y = self._prepare_stage2_data(X, y)\n",
    "\n",
    "#         es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "#         mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "#         self.stage2_classifier.fit(s2_train_x, s2_train_y, validation_split=0.2, callbacks=[es, mc])\n",
    "#         self.stage2_classifier = load_model('best_model.h5')\n",
    "#         return self\n",
    "\n",
    "#     def predict_proba(self, X):\n",
    "#         # Use self.s1_models to get s2 test-data\n",
    "#         X_transformed = self.stage1_preprocessor.transform(X)\n",
    "#         s2_x = []\n",
    "#         for clf in self.stage1_classifiers:\n",
    "#             s2_x.append(clf.predict_proba(X_transformed))\n",
    "#         s2_test_x = np.hstack(s2_x)\n",
    "#         X_transformed2 = self.stage2_preprocessor.transform(X)\n",
    "#         s2_test_x = np.hstack([s2_test_x, X_transformed2.values])\n",
    "#         return self.stage2_classifier.predict_proba(s2_test_x)\n",
    "\n",
    "# class MinMaxScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         self.min_max_scalar= MinMaxScaler()\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         self.min_max_scalar.fit(X)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         arr = self.min_max_scalar.transform(X)\n",
    "#         return pd.DataFrame(arr, columns=list(X.columns))\n",
    "\n",
    "\n",
    "# # Functions for tpr-calculations\n",
    "\n",
    "# # input: pandas.core.series.Series Eg: ( [2,4,6,7,8], [2,5,1,8,9], [1,15,3,5,2], [2,7,10,9,1] )\n",
    "# # output: [7, 31, 20, 29, 20]\n",
    "# def _sum_value_counts(pd_series):\n",
    "#     return np.sum(list(pd_series), axis=0).tolist()\n",
    "\n",
    "\n",
    "# # input: pandas.core.series.Series Eg: ['No Action Taken, already out for sale', 'Add to picklist', 'Add to picklist', 'Location updated for the item',....]\n",
    "# # output: [c1, c2, c3, c4, c5] where (c1-c5) are the value_counts of target-values in the series\n",
    "# def _value_counts(pd_series):\n",
    "#     arr = []\n",
    "#     target_values = ['No Action Taken, already out for sale', 'Add to picklist', 'Updated the on hands quantity for the item', 'Location updated for the item',\n",
    "#                      'New price print sign has been printed']\n",
    "#     c = Counter(pd_series)\n",
    "#     for v in target_values:\n",
    "#         arr.append(c[v])\n",
    "#     return arr\n",
    "\n",
    "\n",
    "# def calculate_daily_tpr(df, level, env, pipeline_root, path='', save=False):\n",
    "    \n",
    "#     csv_file_path = f\"{pipeline_root}/all_level_tpr_{env}\"\n",
    "#     level_name = '_'.join(_ for _ in level)\n",
    "#     groupbycol = level.copy()\n",
    "#     groupbycol.append('run_date')\n",
    "\n",
    "#     grouped = df.groupby(groupbycol).agg({'log_id':'count','event_note': lambda x:_value_counts(x) }).\\\n",
    "#             reset_index().rename(columns={'log_id':'total_action_cnt','event_note':'action_distr'})\n",
    "\n",
    "#     joined = grouped.merge(grouped,on=level,how='left',suffixes=('','_1'))\n",
    "\n",
    "#     joined = joined.loc[joined['run_date']>joined['run_date_1']]\n",
    "\n",
    "#     joined = joined.groupby(groupbycol).agg({'total_action_cnt_1':'sum','action_distr_1': lambda x:_sum_value_counts(x) }).\\\n",
    "#         reset_index().rename(columns={'total_action_cnt_1':'total_action_cnt','action_distr_1':'action_distr'})\n",
    "\n",
    "#     joined[level_name + '_tpr'] = joined.apply(lambda row: [round(x/row['total_action_cnt'], 3) for x in row['action_distr']], axis=1)\n",
    "#     joined.drop(columns=['action_distr', 'total_action_cnt'], inplace=True)\n",
    "\n",
    "#     target_values = ['no_action_taken', 'add_to_picklist', 'update_ohq', 'update_loc', 'new_price_sign']\n",
    "#     target_cols = [level_name + \"_\" + x + \"_tpr\" for x in target_values]\n",
    "#     joined[target_cols] = pd.DataFrame(joined[level_name + '_tpr'].values.tolist(), index= joined.index)\n",
    "\n",
    "#     select_col = groupbycol.copy()\n",
    "#     select_col += target_cols\n",
    "\n",
    "#     if save:\n",
    "#         to_dump = joined.loc[joined['run_date']==joined.run_date.max()]\n",
    "#         to_dump = to_dump[select_col]\n",
    "#         local_path = level_name + '_tpr.joblib'\n",
    "#         local_path_csv = level_name + '_tpr.csv'\n",
    "#         dump(to_dump, local_path)\n",
    "#         storage_path = os.path.join(path, local_path)\n",
    "#         blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "#         blob.upload_from_filename(local_path)\n",
    "#         to_dump.to_csv(f\"{csv_file_path}/{local_path_csv}\", index=False)\n",
    "        \n",
    "\n",
    "#     df = df.merge(joined[select_col], on=groupbycol, how='left')\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def calculate_all_level_tpr(df, env, pipeline_root, path='', save=False):\n",
    "# #     if the path is empty construct a path for GCS\n",
    "#     if path==\"\":\n",
    "#         path = f\"{pipeline_root}/all_level_tpr_{env}\"\n",
    "    \n",
    "#     levels = [ ['mkt'], ['reg'], ['club_nbr'], ['cat'], ['item_nbr'], ['club_nbr','cat'], ['state','cat'], ['mkt','cat'], ['reg','cat'] ]\n",
    "    \n",
    "#     for level in levels:\n",
    "#         df = calculate_daily_tpr(df, level, env, pipeline_root, path=path, save=save)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# def load_tpr_features(nosales_test, path, config=None):\n",
    "#     df = nosales_test.copy()\n",
    "#     levels = [ ['mkt'], ['reg'], ['club_nbr'], ['cat'], ['item_nbr'],\n",
    "#               ['club_nbr','cat'], ['state','cat'], ['mkt','cat'], ['reg','cat'] ]\n",
    "\n",
    "#     for level in levels:\n",
    "#         level_name = '_'.join(_ for _ in level)\n",
    "#         file_name = level_name + '_tpr.joblib'\n",
    "\n",
    "#         if config is None:\n",
    "#             tmp = load(os.path.join(path, file_name)) #not used in production\n",
    "#         else:\n",
    "#             tmp = load(\"{config['model_path']}/{0}\".format(file_name))\n",
    "\n",
    "#         tmp = tmp.drop('run_date', axis=1)\n",
    "#         df = df.merge(tmp, on=level, how='left')\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# def get_raw_score_thresholds(train):\n",
    "#     club_thresh = {}\n",
    "\n",
    "#     mins, maxs= {},{}\n",
    "\n",
    "#     for club in train.club_nbr.unique():\n",
    "#         train_club = train[train.club_nbr==club]\n",
    "#         thresholds = np.sort(list(set(np.round(train_club.raw_score.unique(), 4))))\n",
    "\n",
    "#         f1_arr = []\n",
    "#         prec_arr = []\n",
    "#         recall_arr= []\n",
    "#         for th in thresholds:\n",
    "#             y_pred = list(train_club.raw_score >= th)\n",
    "#             y_true = list(train_club.action == True)\n",
    "#             f1 = f1_score(y_true, y_pred)\n",
    "#             prec = precision_score(y_true, y_pred)\n",
    "#             recall = recall_score(y_true, y_pred)\n",
    "#             f1_arr.append(f1)\n",
    "#             prec_arr.append(prec)\n",
    "#             recall_arr.append(recall)\n",
    "\n",
    "#         club_thresh[club] = thresholds[np.argmax(f1_arr)]\n",
    "\n",
    "#     return club_thresh\n",
    "\n",
    "\n",
    "\n",
    "# def gen_thresholds(df, predictions, classes):\n",
    "#     cutoff = (df.run_date.max() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     scores = pd.DataFrame(predictions, columns=classes)\n",
    "\n",
    "#     action_cols= ['Add to picklist', 'Location updated for the item',\n",
    "#                   'New price print sign has been printed', 'Updated the on hands quantity for the item']\n",
    "#     scores['total_score'] = scores.loc[:,action_cols].sum(axis=1)\n",
    "\n",
    "#     # 'Location updated for the item', 'New price print sign has been printed'\n",
    "#     df['act_bool']= df.event_note.isin(action_cols)*1\n",
    "#     df['raw_score'] = scores['total_score']\n",
    "#     df['action']= ~(df.event_note.isin(['No Action Taken, already out for sale','No Action Taken, already OFS']))\n",
    "\n",
    "#     cols = [\"central_dt\", \"club_nbr\", \"item_nbr\", \"event_note\", \"action\", \"act_bool\", \"run_date\", \"old_nbr\", \"raw_score\"]\n",
    "#     df = df[cols]\n",
    "#     df_subset = df[pd.to_datetime(df.central_dt) >= cutoff]\n",
    "#     np.sort(df_subset.central_dt.unique())\n",
    "#     thresh = get_raw_score_thresholds(df_subset)\n",
    "#     return thresh\n",
    "\n",
    "\n",
    "# def get_config(mode=1):\n",
    "\n",
    "#     config_file = os.path.join(CONFIG_DIR,'config.json')\n",
    "\n",
    "#     with open(config_file,'r') as f:\n",
    "#         config= json.load(f)\n",
    "#     return config\n",
    "\n",
    "# class CustomizedGaussianNB(GaussianNB):\n",
    "#     \"\"\"Cast dtype to 128 float to avoid numerical underflow\"\"\"\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def predict_log_proba(self, X):\n",
    "#         \"\"\"\n",
    "#         Return log-probability estimates for the test vector X.\n",
    "#         Overriding function. Check value in jll and set small value to -inf.\n",
    "#         Cause np.exp(very small value) will cause numerical underflow.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         C : array-like, shape = [n_samples, n_classes]\n",
    "#             Returns the log-probability of the samples for each class in\n",
    "#             the model. The columns correspond to the classes in sorted\n",
    "#             order, as they appear in the attribute `classes_`.\n",
    "#         \"\"\"\n",
    "#         jll = self._joint_log_likelihood(X)\n",
    "#         # normalize by P(x) = P(f_1, ..., f_n)\n",
    "\n",
    "#         # Replace all value smaller than -10000 to -np.inf, when the shape have\n",
    "#         # more than 1 column\n",
    "#         if len(jll.shape) > 1 and jll.shape[1] > 1:\n",
    "#             jll[jll<=-10000] = -np.inf\n",
    "#         jll = jll.astype('float128')\n",
    "#         log_prob_x = logsumexp(jll, axis=1)\n",
    "#         jll = jll.astype('float128')\n",
    "#         log_prob_x = log_prob_x.astype('float128')\n",
    "#         return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "#     def predict_proba(self, X):\n",
    "#         \"\"\"\n",
    "#         Return probability estimates for the test vector X.\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : array-like of shape (n_samples, n_features)\n",
    "#         Returns\n",
    "#         -------\n",
    "#         C : array-like of shape (n_samples, n_classes)\n",
    "#             Returns the probability of the samples for each class in\n",
    "#             the model. The columns correspond to the classes in sorted\n",
    "#             order, as they appear in the attribute :term:`classes_`.\n",
    "#         \"\"\"\n",
    "\n",
    "#         return np.exp(self.predict_log_proba(X).astype('float128'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc5049d7-8c44-4635-b3b2-2a32a5df572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./diagnosis_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./diagnosis_utils.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "# from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, f1_score, confusion_matrix, precision_score, auc, recall_score\n",
    "\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds=5):\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    fold_size = int(dataset.shape[0]/n_folds)\n",
    "    folds = list()\n",
    "    start = 0\n",
    "    end = start+fold_size\n",
    "    while(end<dataset.shape[0]):\n",
    "        folds.append(dataset[start:end])\n",
    "        start = end+1\n",
    "        end = start+fold_size\n",
    "    folds.append(dataset[start:dataset.shape[0]])\n",
    "    return folds\n",
    "\n",
    "def cross_validation_score(dataset, n_folds, pipeline, param_grid):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    auc_scores = []\n",
    "    pipelines = []\n",
    "    for param_value in param_grid['C']:\n",
    "        pipeline.set_params(logistic_clf__C = param_value)\n",
    "        cum_score = 0\n",
    "        for n in range(len(folds)):\n",
    "            test_df = folds[n]\n",
    "            train_df = pd.concat(folds[0:n] + folds[n+1:], axis=0)\n",
    "            pipeline.fit(train_df, train_df.event_note)\n",
    "            cum_score += score(test_df, pipeline)\n",
    "        print(\" C:{}  avg auc score:{}\".format(pipeline.get_params()['logistic_clf'].C, cum_score/len(folds)))\n",
    "        auc_scores.append(cum_score/len(folds))\n",
    "        pipelines.append(pipeline)\n",
    "    return auc_scores, pipelines\n",
    "\n",
    "\n",
    "def model_diag(df, predictions, classes): # classes = encoder.classes_   |   pipeline.classes_\n",
    "    df = df.reset_index(drop=True)\n",
    "    ranks = pd.DataFrame(predictions, columns=classes)\n",
    "\n",
    "    ranks['rank']= ranks.loc[:,['Add to picklist', 'Updated the on hands quantity for the item']].sum(axis=1)\n",
    "\n",
    "    # 'Location updated for the item', 'New price print sign has been printed'\n",
    "    df['act_bool']= df.event_note.isin(['Add to picklist', 'Updated the on hands quantity for the item'])*1\n",
    "    df['rank'] = ranks['rank']\n",
    "    #results_df = df.loc[:,['run_date','club_nbr','OLD_NBR','rank','event_note']]\n",
    "\n",
    "    auc_score=roc_auc_score(df['act_bool'], df['rank'])\n",
    "\n",
    "    print(\"AUC under ROC Curve:\\n\", auc_score)\n",
    "\n",
    "    return df, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fdcb1-a5c1-4791-b2b7-799161e08fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./Dockerfile\n",
    "FROM python:3.7\n",
    "\n",
    "RUN pwd\n",
    "RUN ls\n",
    "\n",
    "COPY ./utils.py ./app/utils.py\n",
    "COPY ./diagnosis_utils.py ./app/diagnosis_utils.py\n",
    "COPY ./baseimage-requirements.txt ./app/baseimage-requirements.txt\n",
    "\n",
    "WORKDIR ./app\n",
    "RUN ls\n",
    "RUN apt-get update && apt-get install gcc libffi-dev -y\n",
    "\n",
    "RUN pip install -r baseimage-requirements.txt\n",
    "RUN pip install pandas==1.1.4 fsspec gcsfs kfp==1.8.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b27b2-2a31-4f26-8333-39d40b6076d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./Dockerfile\n",
    "FROM python:3.7\n",
    "\n",
    "RUN pwd\n",
    "RUN ls\n",
    "\n",
    "COPY ./utils.py ./app/utils.py\n",
    "COPY ./diagnosis_utils.py ./app/diagnosis_utils.py\n",
    "COPY ./baseimage-requirements.txt ./app/baseimage-requirements.txt\n",
    "\n",
    "RUN ls\n",
    "\n",
    "WORKDIR ./app\n",
    "RUN apt-get update && apt-get install gcc libffi-dev -y\n",
    "\n",
    "RUN pip install -r baseimage-requirements.txt\n",
    "RUN pip install pandas==1.1.4 fsspec gcsfs kfp==1.8.14\n",
    "ENV PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774bc8cd-fcb8-4329-97d0-0b0b91f1e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker build -t gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest . -f ./$FOLDER_NANE/Dockerfile --progress=plain --no-cache\n",
    "\n",
    "# docker build -t gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/prework_test:latest . --progress=plain --no-cache\n",
    "\n",
    "\n",
    "\n",
    "# docker build -t gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest . --progress=plain --no-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235b839-7a7a-4c2b-a046-4e314359898f",
   "metadata": {},
   "source": [
    "### Create the image to Google Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208f4f29-d13d-41f6-9ff2-3413c3b91037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853ec1fe-2d2a-483c-bb73-7429b8df009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  1.361MB\n",
      "Step 1/11 : FROM python:3.7-slim\n",
      " ---> 7a3c5ef5d31d\n",
      "Step 2/11 : RUN pwd\n",
      " ---> Running in 7ecc633228d1\n",
      "/\n",
      "Removing intermediate container 7ecc633228d1\n",
      " ---> 4731af96a7d8\n",
      "Step 3/11 : RUN ls\n",
      " ---> Running in 78d262c2c03e\n",
      "bin\n",
      "boot\n",
      "dev\n",
      "etc\n",
      "home\n",
      "lib\n",
      "lib64\n",
      "media\n",
      "mnt\n",
      "opt\n",
      "proc\n",
      "root\n",
      "run\n",
      "sbin\n",
      "srv\n",
      "sys\n",
      "tmp\n",
      "usr\n",
      "var\n",
      "Removing intermediate container 78d262c2c03e\n",
      " ---> 77ab3e6d2fc2\n",
      "Step 4/11 : RUN apt-get update && apt-get install gcc libffi-dev -y\n",
      " ---> Running in 02da4f2c71dc\n",
      "Get:1 http://deb.debian.org/debian bullseye InRelease [116 kB]\n",
      "Get:2 http://deb.debian.org/debian-security bullseye-security InRelease [48.4 kB]\n",
      "Get:3 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\n",
      "Get:4 http://deb.debian.org/debian bullseye/main amd64 Packages [8183 kB]\n",
      "Get:5 http://deb.debian.org/debian-security bullseye-security/main amd64 Packages [210 kB]\n",
      "Get:6 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [14.6 kB]\n",
      "Fetched 8616 kB in 2s (3456 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu cpp cpp-10\n",
      "  fontconfig-config fonts-dejavu-core gcc-10 libasan6 libatomic1 libbinutils\n",
      "  libbrotli1 libbsd0 libc-dev-bin libc-devtools libc6 libc6-dev libcc1-0\n",
      "  libcrypt-dev libctf-nobfd0 libctf0 libdeflate0 libfontconfig1 libfreetype6\n",
      "  libgcc-10-dev libgd3 libgomp1 libisl23 libitm1 libjbig0 libjpeg62-turbo\n",
      "  liblsan0 libmd0 libmpc3 libmpfr6 libnsl-dev libpng16-16 libquadmath0\n",
      "  libtiff5 libtirpc-common libtirpc-dev libtirpc3 libtsan0 libubsan1 libwebp6\n",
      "  libx11-6 libx11-data libxau6 libxcb1 libxdmcp6 libxpm4 linux-libc-dev\n",
      "  manpages manpages-dev sensible-utils ucf\n",
      "Suggested packages:\n",
      "  binutils-doc cpp-doc gcc-10-locales gcc-multilib make autoconf automake\n",
      "  libtool flex bison gdb gcc-doc gcc-10-multilib gcc-10-doc glibc-doc\n",
      "  libc-l10n locales libgd-tools man-browser\n",
      "Recommended packages:\n",
      "  libnss-nis libnss-nisplus\n",
      "The following NEW packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu cpp cpp-10\n",
      "  fontconfig-config fonts-dejavu-core gcc gcc-10 libasan6 libatomic1\n",
      "  libbinutils libbrotli1 libbsd0 libc-dev-bin libc-devtools libc6-dev libcc1-0\n",
      "  libcrypt-dev libctf-nobfd0 libctf0 libdeflate0 libffi-dev libfontconfig1\n",
      "  libfreetype6 libgcc-10-dev libgd3 libgomp1 libisl23 libitm1 libjbig0\n",
      "  libjpeg62-turbo liblsan0 libmd0 libmpc3 libmpfr6 libnsl-dev libpng16-16\n",
      "  libquadmath0 libtiff5 libtirpc-dev libtsan0 libubsan1 libwebp6 libx11-6\n",
      "  libx11-data libxau6 libxcb1 libxdmcp6 libxpm4 linux-libc-dev manpages\n",
      "  manpages-dev sensible-utils ucf\n",
      "The following packages will be upgraded:\n",
      "  libc6 libtirpc-common libtirpc3\n",
      "3 upgraded, 55 newly installed, 0 to remove and 15 not upgraded.\n",
      "Need to get 58.1 MB of archives.\n",
      "After this operation, 193 MB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian bullseye/main amd64 libc6 amd64 2.31-13+deb11u5 [2825 kB]\n",
      "Get:2 http://deb.debian.org/debian bullseye/main amd64 libtirpc-common all 1.3.1-1+deb11u1 [13.5 kB]\n",
      "Get:3 http://deb.debian.org/debian bullseye/main amd64 libtirpc3 amd64 1.3.1-1+deb11u1 [84.1 kB]\n",
      "Get:4 http://deb.debian.org/debian bullseye/main amd64 sensible-utils all 0.0.14 [14.8 kB]\n",
      "Get:5 http://deb.debian.org/debian bullseye/main amd64 manpages all 5.10-1 [1412 kB]\n",
      "Get:6 http://deb.debian.org/debian bullseye/main amd64 ucf all 3.0043 [74.0 kB]\n",
      "Get:7 http://deb.debian.org/debian bullseye/main amd64 binutils-common amd64 2.35.2-2 [2220 kB]\n",
      "Get:8 http://deb.debian.org/debian bullseye/main amd64 libbinutils amd64 2.35.2-2 [570 kB]\n",
      "Get:9 http://deb.debian.org/debian bullseye/main amd64 libctf-nobfd0 amd64 2.35.2-2 [110 kB]\n",
      "Get:10 http://deb.debian.org/debian bullseye/main amd64 libctf0 amd64 2.35.2-2 [53.2 kB]\n",
      "Get:11 http://deb.debian.org/debian bullseye/main amd64 binutils-x86-64-linux-gnu amd64 2.35.2-2 [1809 kB]\n",
      "Get:12 http://deb.debian.org/debian bullseye/main amd64 binutils amd64 2.35.2-2 [61.2 kB]\n",
      "Get:13 http://deb.debian.org/debian bullseye/main amd64 libisl23 amd64 0.23-1 [676 kB]\n",
      "Get:14 http://deb.debian.org/debian bullseye/main amd64 libmpfr6 amd64 4.1.0-3 [2012 kB]\n",
      "Get:15 http://deb.debian.org/debian bullseye/main amd64 libmpc3 amd64 1.2.0-1 [45.0 kB]\n",
      "Get:16 http://deb.debian.org/debian bullseye/main amd64 cpp-10 amd64 10.2.1-6 [8528 kB]\n",
      "Get:17 http://deb.debian.org/debian bullseye/main amd64 cpp amd64 4:10.2.1-1 [19.7 kB]\n",
      "Get:18 http://deb.debian.org/debian bullseye/main amd64 fonts-dejavu-core all 2.37-2 [1069 kB]\n",
      "Get:19 http://deb.debian.org/debian bullseye/main amd64 fontconfig-config all 2.13.1-4.2 [281 kB]\n",
      "Get:20 http://deb.debian.org/debian bullseye/main amd64 libcc1-0 amd64 10.2.1-6 [47.0 kB]\n",
      "Get:21 http://deb.debian.org/debian bullseye/main amd64 libgomp1 amd64 10.2.1-6 [99.9 kB]\n",
      "Get:22 http://deb.debian.org/debian bullseye/main amd64 libitm1 amd64 10.2.1-6 [25.8 kB]\n",
      "Get:23 http://deb.debian.org/debian bullseye/main amd64 libatomic1 amd64 10.2.1-6 [9008 B]\n",
      "Get:24 http://deb.debian.org/debian bullseye/main amd64 libasan6 amd64 10.2.1-6 [2065 kB]\n",
      "Get:25 http://deb.debian.org/debian bullseye/main amd64 liblsan0 amd64 10.2.1-6 [828 kB]\n",
      "Get:26 http://deb.debian.org/debian bullseye/main amd64 libtsan0 amd64 10.2.1-6 [2000 kB]\n",
      "Get:27 http://deb.debian.org/debian bullseye/main amd64 libubsan1 amd64 10.2.1-6 [777 kB]\n",
      "Get:28 http://deb.debian.org/debian bullseye/main amd64 libquadmath0 amd64 10.2.1-6 [145 kB]\n",
      "Get:29 http://deb.debian.org/debian bullseye/main amd64 libgcc-10-dev amd64 10.2.1-6 [2328 kB]\n",
      "Get:30 http://deb.debian.org/debian bullseye/main amd64 gcc-10 amd64 10.2.1-6 [17.0 MB]\n",
      "Get:31 http://deb.debian.org/debian bullseye/main amd64 gcc amd64 4:10.2.1-1 [5192 B]\n",
      "Get:32 http://deb.debian.org/debian bullseye/main amd64 libbrotli1 amd64 1.0.9-2+b2 [279 kB]\n",
      "Get:33 http://deb.debian.org/debian bullseye/main amd64 libmd0 amd64 1.0.3-3 [28.0 kB]\n",
      "Get:34 http://deb.debian.org/debian bullseye/main amd64 libbsd0 amd64 0.11.3-1 [108 kB]\n",
      "Get:35 http://deb.debian.org/debian bullseye/main amd64 libc-dev-bin amd64 2.31-13+deb11u5 [276 kB]\n",
      "Get:36 http://deb.debian.org/debian bullseye/main amd64 libpng16-16 amd64 1.6.37-3 [294 kB]\n",
      "Get:37 http://deb.debian.org/debian bullseye/main amd64 libfreetype6 amd64 2.10.4+dfsg-1+deb11u1 [418 kB]\n",
      "Get:38 http://deb.debian.org/debian bullseye/main amd64 libfontconfig1 amd64 2.13.1-4.2 [347 kB]\n",
      "Get:39 http://deb.debian.org/debian bullseye/main amd64 libjpeg62-turbo amd64 1:2.0.6-4 [151 kB]\n",
      "Get:40 http://deb.debian.org/debian bullseye/main amd64 libdeflate0 amd64 1.7-1 [53.1 kB]\n",
      "Get:41 http://deb.debian.org/debian bullseye/main amd64 libjbig0 amd64 2.1-3.1+b2 [31.0 kB]\n",
      "Get:42 http://deb.debian.org/debian bullseye/main amd64 libwebp6 amd64 0.6.1-2.1 [258 kB]\n",
      "Get:43 http://deb.debian.org/debian bullseye/main amd64 libtiff5 amd64 4.2.0-1+deb11u1 [289 kB]\n",
      "Get:44 http://deb.debian.org/debian bullseye/main amd64 libxau6 amd64 1:1.0.9-1 [19.7 kB]\n",
      "Get:45 http://deb.debian.org/debian bullseye/main amd64 libxdmcp6 amd64 1:1.1.2-3 [26.3 kB]\n",
      "Get:46 http://deb.debian.org/debian bullseye/main amd64 libxcb1 amd64 1.14-3 [140 kB]\n",
      "Get:47 http://deb.debian.org/debian bullseye/main amd64 libx11-data all 2:1.7.2-1 [311 kB]\n",
      "Get:48 http://deb.debian.org/debian bullseye/main amd64 libx11-6 amd64 2:1.7.2-1 [772 kB]\n",
      "Get:49 http://deb.debian.org/debian bullseye/main amd64 libxpm4 amd64 1:3.5.12-1 [49.1 kB]\n",
      "Get:50 http://deb.debian.org/debian bullseye/main amd64 libgd3 amd64 2.3.0-2 [137 kB]\n",
      "Get:51 http://deb.debian.org/debian bullseye/main amd64 libc-devtools amd64 2.31-13+deb11u5 [246 kB]\n",
      "Get:52 http://deb.debian.org/debian bullseye/main amd64 linux-libc-dev amd64 5.10.158-2 [1571 kB]\n",
      "Get:53 http://deb.debian.org/debian bullseye/main amd64 libcrypt-dev amd64 1:4.4.18-4 [104 kB]\n",
      "Get:54 http://deb.debian.org/debian bullseye/main amd64 libtirpc-dev amd64 1.3.1-1+deb11u1 [191 kB]\n",
      "Get:55 http://deb.debian.org/debian bullseye/main amd64 libnsl-dev amd64 1.3.0-2 [66.4 kB]\n",
      "Get:56 http://deb.debian.org/debian bullseye/main amd64 libc6-dev amd64 2.31-13+deb11u5 [2359 kB]\n",
      "Get:57 http://deb.debian.org/debian bullseye/main amd64 libffi-dev amd64 3.3-6 [56.5 kB]\n",
      "Get:58 http://deb.debian.org/debian bullseye/main amd64 manpages-dev all 5.10-1 [2309 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 58.1 MB in 1s (44.8 MB/s)\n",
      "(Reading database ... 7031 files and directories currently installed.)\n",
      "Preparing to unpack .../libc6_2.31-13+deb11u5_amd64.deb ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.32.1 /usr/local/share/perl/5.32.1 /usr/lib/x86_64-linux-gnu/perl5/5.32 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl-base /usr/lib/x86_64-linux-gnu/perl/5.32 /usr/share/perl/5.32 /usr/local/lib/site_perl) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.32.1 /usr/local/share/perl/5.32.1 /usr/lib/x86_64-linux-gnu/perl5/5.32 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl-base /usr/lib/x86_64-linux-gnu/perl/5.32 /usr/share/perl/5.32 /usr/local/lib/site_perl) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "Unpacking libc6:amd64 (2.31-13+deb11u5) over (2.31-13+deb11u3) ...\n",
      "Setting up libc6:amd64 (2.31-13+deb11u5) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.32.1 /usr/local/share/perl/5.32.1 /usr/lib/x86_64-linux-gnu/perl5/5.32 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl-base /usr/lib/x86_64-linux-gnu/perl/5.32 /usr/share/perl/5.32 /usr/local/lib/site_perl) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "(Reading database ... 7031 files and directories currently installed.)\n",
      "Preparing to unpack .../libtirpc-common_1.3.1-1+deb11u1_all.deb ...\n",
      "Unpacking libtirpc-common (1.3.1-1+deb11u1) over (1.3.1-1) ...\n",
      "Setting up libtirpc-common (1.3.1-1+deb11u1) ...\n",
      "(Reading database ... 7031 files and directories currently installed.)\n",
      "Preparing to unpack .../libtirpc3_1.3.1-1+deb11u1_amd64.deb ...\n",
      "Unpacking libtirpc3:amd64 (1.3.1-1+deb11u1) over (1.3.1-1) ...\n",
      "Setting up libtirpc3:amd64 (1.3.1-1+deb11u1) ...\n",
      "Selecting previously unselected package sensible-utils.\n",
      "(Reading database ... 7031 files and directories currently installed.)\n",
      "Preparing to unpack .../00-sensible-utils_0.0.14_all.deb ...\n",
      "Unpacking sensible-utils (0.0.14) ...\n",
      "Selecting previously unselected package manpages.\n",
      "Preparing to unpack .../01-manpages_5.10-1_all.deb ...\n",
      "Unpacking manpages (5.10-1) ...\n",
      "Selecting previously unselected package ucf.\n",
      "Preparing to unpack .../02-ucf_3.0043_all.deb ...\n",
      "Moving old data out of the way\n",
      "Unpacking ucf (3.0043) ...\n",
      "Selecting previously unselected package binutils-common:amd64.\n",
      "Preparing to unpack .../03-binutils-common_2.35.2-2_amd64.deb ...\n",
      "Unpacking binutils-common:amd64 (2.35.2-2) ...\n",
      "Selecting previously unselected package libbinutils:amd64.\n",
      "Preparing to unpack .../04-libbinutils_2.35.2-2_amd64.deb ...\n",
      "Unpacking libbinutils:amd64 (2.35.2-2) ...\n",
      "Selecting previously unselected package libctf-nobfd0:amd64.\n",
      "Preparing to unpack .../05-libctf-nobfd0_2.35.2-2_amd64.deb ...\n",
      "Unpacking libctf-nobfd0:amd64 (2.35.2-2) ...\n",
      "Selecting previously unselected package libctf0:amd64.\n",
      "Preparing to unpack .../06-libctf0_2.35.2-2_amd64.deb ...\n",
      "Unpacking libctf0:amd64 (2.35.2-2) ...\n",
      "Selecting previously unselected package binutils-x86-64-linux-gnu.\n",
      "Preparing to unpack .../07-binutils-x86-64-linux-gnu_2.35.2-2_amd64.deb ...\n",
      "Unpacking binutils-x86-64-linux-gnu (2.35.2-2) ...\n",
      "Selecting previously unselected package binutils.\n",
      "Preparing to unpack .../08-binutils_2.35.2-2_amd64.deb ...\n",
      "Unpacking binutils (2.35.2-2) ...\n",
      "Selecting previously unselected package libisl23:amd64.\n",
      "Preparing to unpack .../09-libisl23_0.23-1_amd64.deb ...\n",
      "Unpacking libisl23:amd64 (0.23-1) ...\n",
      "Selecting previously unselected package libmpfr6:amd64.\n",
      "Preparing to unpack .../10-libmpfr6_4.1.0-3_amd64.deb ...\n",
      "Unpacking libmpfr6:amd64 (4.1.0-3) ...\n",
      "Selecting previously unselected package libmpc3:amd64.\n",
      "Preparing to unpack .../11-libmpc3_1.2.0-1_amd64.deb ...\n",
      "Unpacking libmpc3:amd64 (1.2.0-1) ...\n",
      "Selecting previously unselected package cpp-10.\n",
      "Preparing to unpack .../12-cpp-10_10.2.1-6_amd64.deb ...\n",
      "Unpacking cpp-10 (10.2.1-6) ...\n",
      "Selecting previously unselected package cpp.\n",
      "Preparing to unpack .../13-cpp_4%3a10.2.1-1_amd64.deb ...\n",
      "Unpacking cpp (4:10.2.1-1) ...\n",
      "Selecting previously unselected package fonts-dejavu-core.\n",
      "Preparing to unpack .../14-fonts-dejavu-core_2.37-2_all.deb ...\n",
      "Unpacking fonts-dejavu-core (2.37-2) ...\n",
      "Selecting previously unselected package fontconfig-config.\n",
      "Preparing to unpack .../15-fontconfig-config_2.13.1-4.2_all.deb ...\n",
      "Unpacking fontconfig-config (2.13.1-4.2) ...\n",
      "Selecting previously unselected package libcc1-0:amd64.\n",
      "Preparing to unpack .../16-libcc1-0_10.2.1-6_amd64.deb ...\n",
      "Unpacking libcc1-0:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package libgomp1:amd64.\n",
      "Preparing to unpack .../17-libgomp1_10.2.1-6_amd64.deb ...\n",
      "Unpacking libgomp1:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package libitm1:amd64.\n",
      "Preparing to unpack .../18-libitm1_10.2.1-6_amd64.deb ...\n",
      "Unpacking libitm1:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package libatomic1:amd64.\n",
      "Preparing to unpack .../19-libatomic1_10.2.1-6_amd64.deb ...\n",
      "Unpacking libatomic1:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package libasan6:amd64.\n",
      "Preparing to unpack .../20-libasan6_10.2.1-6_amd64.deb ...\n",
      "Unpacking libasan6:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package liblsan0:amd64.\n",
      "Preparing to unpack .../21-liblsan0_10.2.1-6_amd64.deb ...\n",
      "Unpacking liblsan0:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package libtsan0:amd64.\n",
      "Preparing to unpack .../22-libtsan0_10.2.1-6_amd64.deb ...\n",
      "Unpacking libtsan0:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package libubsan1:amd64.\n",
      "Preparing to unpack .../23-libubsan1_10.2.1-6_amd64.deb ...\n",
      "Unpacking libubsan1:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package libquadmath0:amd64.\n",
      "Preparing to unpack .../24-libquadmath0_10.2.1-6_amd64.deb ...\n",
      "Unpacking libquadmath0:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package libgcc-10-dev:amd64.\n",
      "Preparing to unpack .../25-libgcc-10-dev_10.2.1-6_amd64.deb ...\n",
      "Unpacking libgcc-10-dev:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package gcc-10.\n",
      "Preparing to unpack .../26-gcc-10_10.2.1-6_amd64.deb ...\n",
      "Unpacking gcc-10 (10.2.1-6) ...\n",
      "Selecting previously unselected package gcc.\n",
      "Preparing to unpack .../27-gcc_4%3a10.2.1-1_amd64.deb ...\n",
      "Unpacking gcc (4:10.2.1-1) ...\n",
      "Selecting previously unselected package libbrotli1:amd64.\n",
      "Preparing to unpack .../28-libbrotli1_1.0.9-2+b2_amd64.deb ...\n",
      "Unpacking libbrotli1:amd64 (1.0.9-2+b2) ...\n",
      "Selecting previously unselected package libmd0:amd64.\n",
      "Preparing to unpack .../29-libmd0_1.0.3-3_amd64.deb ...\n",
      "Unpacking libmd0:amd64 (1.0.3-3) ...\n",
      "Selecting previously unselected package libbsd0:amd64.\n",
      "Preparing to unpack .../30-libbsd0_0.11.3-1_amd64.deb ...\n",
      "Unpacking libbsd0:amd64 (0.11.3-1) ...\n",
      "Selecting previously unselected package libc-dev-bin.\n",
      "Preparing to unpack .../31-libc-dev-bin_2.31-13+deb11u5_amd64.deb ...\n",
      "Unpacking libc-dev-bin (2.31-13+deb11u5) ...\n",
      "Selecting previously unselected package libpng16-16:amd64.\n",
      "Preparing to unpack .../32-libpng16-16_1.6.37-3_amd64.deb ...\n",
      "Unpacking libpng16-16:amd64 (1.6.37-3) ...\n",
      "Selecting previously unselected package libfreetype6:amd64.\n",
      "Preparing to unpack .../33-libfreetype6_2.10.4+dfsg-1+deb11u1_amd64.deb ...\n",
      "Unpacking libfreetype6:amd64 (2.10.4+dfsg-1+deb11u1) ...\n",
      "Selecting previously unselected package libfontconfig1:amd64.\n",
      "Preparing to unpack .../34-libfontconfig1_2.13.1-4.2_amd64.deb ...\n",
      "Unpacking libfontconfig1:amd64 (2.13.1-4.2) ...\n",
      "Selecting previously unselected package libjpeg62-turbo:amd64.\n",
      "Preparing to unpack .../35-libjpeg62-turbo_1%3a2.0.6-4_amd64.deb ...\n",
      "Unpacking libjpeg62-turbo:amd64 (1:2.0.6-4) ...\n",
      "Selecting previously unselected package libdeflate0:amd64.\n",
      "Preparing to unpack .../36-libdeflate0_1.7-1_amd64.deb ...\n",
      "Unpacking libdeflate0:amd64 (1.7-1) ...\n",
      "Selecting previously unselected package libjbig0:amd64.\n",
      "Preparing to unpack .../37-libjbig0_2.1-3.1+b2_amd64.deb ...\n",
      "Unpacking libjbig0:amd64 (2.1-3.1+b2) ...\n",
      "Selecting previously unselected package libwebp6:amd64.\n",
      "Preparing to unpack .../38-libwebp6_0.6.1-2.1_amd64.deb ...\n",
      "Unpacking libwebp6:amd64 (0.6.1-2.1) ...\n",
      "Selecting previously unselected package libtiff5:amd64.\n",
      "Preparing to unpack .../39-libtiff5_4.2.0-1+deb11u1_amd64.deb ...\n",
      "Unpacking libtiff5:amd64 (4.2.0-1+deb11u1) ...\n",
      "Selecting previously unselected package libxau6:amd64.\n",
      "Preparing to unpack .../40-libxau6_1%3a1.0.9-1_amd64.deb ...\n",
      "Unpacking libxau6:amd64 (1:1.0.9-1) ...\n",
      "Selecting previously unselected package libxdmcp6:amd64.\n",
      "Preparing to unpack .../41-libxdmcp6_1%3a1.1.2-3_amd64.deb ...\n",
      "Unpacking libxdmcp6:amd64 (1:1.1.2-3) ...\n",
      "Selecting previously unselected package libxcb1:amd64.\n",
      "Preparing to unpack .../42-libxcb1_1.14-3_amd64.deb ...\n",
      "Unpacking libxcb1:amd64 (1.14-3) ...\n",
      "Selecting previously unselected package libx11-data.\n",
      "Preparing to unpack .../43-libx11-data_2%3a1.7.2-1_all.deb ...\n",
      "Unpacking libx11-data (2:1.7.2-1) ...\n",
      "Selecting previously unselected package libx11-6:amd64.\n",
      "Preparing to unpack .../44-libx11-6_2%3a1.7.2-1_amd64.deb ...\n",
      "Unpacking libx11-6:amd64 (2:1.7.2-1) ...\n",
      "Selecting previously unselected package libxpm4:amd64.\n",
      "Preparing to unpack .../45-libxpm4_1%3a3.5.12-1_amd64.deb ...\n",
      "Unpacking libxpm4:amd64 (1:3.5.12-1) ...\n",
      "Selecting previously unselected package libgd3:amd64.\n",
      "Preparing to unpack .../46-libgd3_2.3.0-2_amd64.deb ...\n",
      "Unpacking libgd3:amd64 (2.3.0-2) ...\n",
      "Selecting previously unselected package libc-devtools.\n",
      "Preparing to unpack .../47-libc-devtools_2.31-13+deb11u5_amd64.deb ...\n",
      "Unpacking libc-devtools (2.31-13+deb11u5) ...\n",
      "Selecting previously unselected package linux-libc-dev:amd64.\n",
      "Preparing to unpack .../48-linux-libc-dev_5.10.158-2_amd64.deb ...\n",
      "Unpacking linux-libc-dev:amd64 (5.10.158-2) ...\n",
      "Selecting previously unselected package libcrypt-dev:amd64.\n",
      "Preparing to unpack .../49-libcrypt-dev_1%3a4.4.18-4_amd64.deb ...\n",
      "Unpacking libcrypt-dev:amd64 (1:4.4.18-4) ...\n",
      "Selecting previously unselected package libtirpc-dev:amd64.\n",
      "Preparing to unpack .../50-libtirpc-dev_1.3.1-1+deb11u1_amd64.deb ...\n",
      "Unpacking libtirpc-dev:amd64 (1.3.1-1+deb11u1) ...\n",
      "Selecting previously unselected package libnsl-dev:amd64.\n",
      "Preparing to unpack .../51-libnsl-dev_1.3.0-2_amd64.deb ...\n",
      "Unpacking libnsl-dev:amd64 (1.3.0-2) ...\n",
      "Selecting previously unselected package libc6-dev:amd64.\n",
      "Preparing to unpack .../52-libc6-dev_2.31-13+deb11u5_amd64.deb ...\n",
      "Unpacking libc6-dev:amd64 (2.31-13+deb11u5) ...\n",
      "Selecting previously unselected package libffi-dev:amd64.\n",
      "Preparing to unpack .../53-libffi-dev_3.3-6_amd64.deb ...\n",
      "Unpacking libffi-dev:amd64 (3.3-6) ...\n",
      "Selecting previously unselected package manpages-dev.\n",
      "Preparing to unpack .../54-manpages-dev_5.10-1_all.deb ...\n",
      "Unpacking manpages-dev (5.10-1) ...\n",
      "Setting up libxau6:amd64 (1:1.0.9-1) ...\n",
      "Setting up manpages (5.10-1) ...\n",
      "Setting up libbrotli1:amd64 (1.0.9-2+b2) ...\n",
      "Setting up binutils-common:amd64 (2.35.2-2) ...\n",
      "Setting up libdeflate0:amd64 (1.7-1) ...\n",
      "Setting up linux-libc-dev:amd64 (5.10.158-2) ...\n",
      "Setting up libctf-nobfd0:amd64 (2.35.2-2) ...\n",
      "Setting up libgomp1:amd64 (10.2.1-6) ...\n",
      "Setting up libffi-dev:amd64 (3.3-6) ...\n",
      "Setting up libjbig0:amd64 (2.1-3.1+b2) ...\n",
      "Setting up libasan6:amd64 (10.2.1-6) ...\n",
      "Setting up libtirpc-dev:amd64 (1.3.1-1+deb11u1) ...\n",
      "Setting up libjpeg62-turbo:amd64 (1:2.0.6-4) ...\n",
      "Setting up libx11-data (2:1.7.2-1) ...\n",
      "Setting up libmpfr6:amd64 (4.1.0-3) ...\n",
      "Setting up libquadmath0:amd64 (10.2.1-6) ...\n",
      "Setting up libpng16-16:amd64 (1.6.37-3) ...\n",
      "Setting up libmpc3:amd64 (1.2.0-1) ...\n",
      "Setting up libatomic1:amd64 (10.2.1-6) ...\n",
      "Setting up libwebp6:amd64 (0.6.1-2.1) ...\n",
      "Setting up fonts-dejavu-core (2.37-2) ...\n",
      "Setting up libubsan1:amd64 (10.2.1-6) ...\n",
      "Setting up libmd0:amd64 (1.0.3-3) ...\n",
      "Setting up libnsl-dev:amd64 (1.3.0-2) ...\n",
      "Setting up sensible-utils (0.0.14) ...\n",
      "Setting up libcrypt-dev:amd64 (1:4.4.18-4) ...\n",
      "Setting up libtiff5:amd64 (4.2.0-1+deb11u1) ...\n",
      "Setting up libbinutils:amd64 (2.35.2-2) ...\n",
      "Setting up libisl23:amd64 (0.23-1) ...\n",
      "Setting up libc-dev-bin (2.31-13+deb11u5) ...\n",
      "Setting up libbsd0:amd64 (0.11.3-1) ...\n",
      "Setting up libcc1-0:amd64 (10.2.1-6) ...\n",
      "Setting up liblsan0:amd64 (10.2.1-6) ...\n",
      "Setting up cpp-10 (10.2.1-6) ...\n",
      "Setting up libitm1:amd64 (10.2.1-6) ...\n",
      "Setting up libtsan0:amd64 (10.2.1-6) ...\n",
      "Setting up libctf0:amd64 (2.35.2-2) ...\n",
      "Setting up manpages-dev (5.10-1) ...\n",
      "Setting up libxdmcp6:amd64 (1:1.1.2-3) ...\n",
      "Setting up libxcb1:amd64 (1.14-3) ...\n",
      "Setting up libgcc-10-dev:amd64 (10.2.1-6) ...\n",
      "Setting up libfreetype6:amd64 (2.10.4+dfsg-1+deb11u1) ...\n",
      "Setting up ucf (3.0043) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.32.1 /usr/local/share/perl/5.32.1 /usr/lib/x86_64-linux-gnu/perl5/5.32 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl-base /usr/lib/x86_64-linux-gnu/perl/5.32 /usr/share/perl/5.32 /usr/local/lib/site_perl) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "Setting up cpp (4:10.2.1-1) ...\n",
      "Setting up libc6-dev:amd64 (2.31-13+deb11u5) ...\n",
      "Setting up libx11-6:amd64 (2:1.7.2-1) ...\n",
      "Setting up binutils-x86-64-linux-gnu (2.35.2-2) ...\n",
      "Setting up libxpm4:amd64 (1:3.5.12-1) ...\n",
      "Setting up fontconfig-config (2.13.1-4.2) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.32.1 /usr/local/share/perl/5.32.1 /usr/lib/x86_64-linux-gnu/perl5/5.32 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl-base /usr/lib/x86_64-linux-gnu/perl/5.32 /usr/share/perl/5.32 /usr/local/lib/site_perl) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "Setting up binutils (2.35.2-2) ...\n",
      "Setting up gcc-10 (10.2.1-6) ...\n",
      "Setting up libfontconfig1:amd64 (2.13.1-4.2) ...\n",
      "Setting up gcc (4:10.2.1-1) ...\n",
      "Setting up libgd3:amd64 (2.3.0-2) ...\n",
      "Setting up libc-devtools (2.31-13+deb11u5) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u3) ...\n",
      "Removing intermediate container 02da4f2c71dc\n",
      " ---> 05f861fa69e3\n",
      "Step 5/11 : COPY ./utils.py ./app/utils.py\n",
      " ---> 0c773eeb866d\n",
      "Step 6/11 : COPY ./diagnosis_utils.py ./app/diagnosis_utils.py\n",
      " ---> 462e8023c0fe\n",
      "Step 7/11 : COPY ./baseimage-requirements.txt ./app/baseimage-requirements.txt\n",
      " ---> 52835f0364db\n",
      "Step 8/11 : WORKDIR ./app\n",
      " ---> Running in 49efe369d8ea\n",
      "Removing intermediate container 49efe369d8ea\n",
      " ---> 9556ce4c2335\n",
      "Step 9/11 : RUN ls\n",
      " ---> Running in 6e2bda38f54e\n",
      "baseimage-requirements.txt\n",
      "diagnosis_utils.py\n",
      "utils.py\n",
      "Removing intermediate container 6e2bda38f54e\n",
      " ---> a13c75663fc3\n",
      "Step 10/11 : RUN pip install -r baseimage-requirements.txt\n",
      " ---> Running in d0248e5e766e\n",
      "Collecting attrs==21.2.0\n",
      "  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
      "      53.7/53.7 KB 955.1 kB/s eta 0:00:00\n",
      "Collecting numpy==1.18.1\n",
      "  Downloading numpy-1.18.1-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "      20.1/20.1 MB 42.6 MB/s eta 0:00:00\n",
      "Collecting pandas==1.1.4\n",
      "  Downloading pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "      9.5/9.5 MB 58.3 MB/s eta 0:00:00\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-1.30.0-py3-none-any.whl (17.0 MB)\n",
      "      17.0/17.0 MB 25.9 MB/s eta 0:00:00\n",
      "Collecting setuptools==45.2.0\n",
      "  Downloading setuptools-45.2.0-py3-none-any.whl (584 kB)\n",
      "      584.2/584.2 KB 43.3 MB/s eta 0:00:00\n",
      "Collecting h5py==2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "      2.9/2.9 MB 74.9 MB/s eta 0:00:00\n",
      "Collecting keras==2.3.1\n",
      "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "      377.8/377.8 KB 36.7 MB/s eta 0:00:00\n",
      "Collecting joblib==0.17.0\n",
      "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "      301.5/301.5 KB 32.3 MB/s eta 0:00:00\n",
      "Collecting scikit-learn==0.24.1\n",
      "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "      22.3/22.3 MB 37.5 MB/s eta 0:00:00\n",
      "Collecting tensorflow==1.15.4\n",
      "  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
      "      110.5/110.5 MB 9.5 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage==1.44.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "      106.8/106.8 KB 15.5 MB/s eta 0:00:00\n",
      "Collecting google-cloud-aiplatform==1.13.0\n",
      "  Downloading google_cloud_aiplatform-1.13.0-py2.py3-none-any.whl (1.8 MB)\n",
      "      1.8/1.8 MB 52.6 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.4.1-py2.py3-none-any.whl (215 kB)\n",
      "      215.1/215.1 KB 26.8 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "      139.5/139.5 KB 18.8 MB/s eta 0:00:00\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2022.11.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting db-dtypes\n",
      "  Downloading db_dtypes-1.0.5-py2.py3-none-any.whl (14 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "      247.7/247.7 KB 28.3 MB/s eta 0:00:00\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "      499.4/499.4 KB 31.3 MB/s eta 0:00:00\n",
      "Collecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting scipy>=0.14\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "      38.1/38.1 MB 27.3 MB/s eta 0:00:00\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "      50.7/50.7 KB 7.8 MB/s eta 0:00:00\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "      42.6/42.6 KB 6.3 MB/s eta 0:00:00\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "      596.3/596.3 KB 39.3 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "      57.5/57.5 KB 9.3 MB/s eta 0:00:00\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.51.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "      4.8/4.8 MB 63.6 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "      75.2/75.2 KB 10.6 MB/s eta 0:00:00\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "      3.8/3.8 MB 60.9 MB/s eta 0:00:00\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "      124.6/124.6 KB 17.3 MB/s eta 0:00:00\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15.4->-r baseimage-requirements.txt (line 10)) (0.37.1)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "      503.4/503.4 KB 39.4 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
      "      409.8/409.8 KB 22.2 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "      65.5/65.5 KB 10.5 MB/s eta 0:00:00\n",
      "Collecting google-api-core<3.0dev,>=1.29.0\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "      120.3/120.3 KB 17.3 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media<3.0dev,>=1.3.0\n",
      "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "      77.4/77.4 KB 10.6 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "      177.0/177.0 KB 15.7 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "      62.8/62.8 KB 8.9 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "      206.6/206.6 KB 24.1 MB/s eta 0:00:00\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.7.0-py2.py3-none-any.whl (235 kB)\n",
      "      235.3/235.3 KB 29.6 MB/s eta 0:00:00\n",
      "Collecting proto-plus>=1.15.0\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "      47.9/47.9 KB 7.1 MB/s eta 0:00:00\n",
      "Collecting packaging>=14.3\n",
      "  Downloading packaging-22.0-py3-none-any.whl (42 kB)\n",
      "      42.6/42.6 KB 5.4 MB/s eta 0:00:00\n",
      "Collecting prometheus-flask-exporter<1\n",
      "  Downloading prometheus_flask_exporter-0.21.0-py3-none-any.whl (18 kB)\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
      "      184.0/184.0 KB 21.5 MB/s eta 0:00:00\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "      96.6/96.6 KB 14.2 MB/s eta 0:00:00\n",
      "Collecting entrypoints<1\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting cloudpickle<3\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting gunicorn<21\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "      79.5/79.5 KB 11.6 MB/s eta 0:00:00\n",
      "Collecting docker<7,>=4.0.0\n",
      "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
      "      147.5/147.5 KB 19.2 MB/s eta 0:00:00\n",
      "Collecting querystring-parser<2\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
      "      42.8/42.8 KB 6.7 MB/s eta 0:00:00\n",
      "Collecting Flask<3\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "      101.5/101.5 KB 14.8 MB/s eta 0:00:00\n",
      "Collecting packaging>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "      40.8/40.8 KB 5.8 MB/s eta 0:00:00\n",
      "Collecting sqlalchemy<2,>=1.4.0\n",
      "  Downloading SQLAlchemy-1.4.45-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "      1.6/1.6 MB 67.8 MB/s eta 0:00:00\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Downloading databricks-cli-0.17.4.tar.gz (82 kB)\n",
      "      82.3/82.3 KB 11.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting importlib-metadata!=4.7.0,<6,>=3.7.0\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting alembic<2\n",
      "  Downloading alembic-1.9.1-py3-none-any.whl (210 kB)\n",
      "      210.4/210.4 KB 24.1 MB/s eta 0:00:00\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "      1.0/1.0 MB 60.8 MB/s eta 0:00:00\n",
      "Collecting decorator>4.1.2\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
      "      948.0/948.0 KB 55.9 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting pyarrow>=3.0.0\n",
      "  Downloading pyarrow-10.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
      "      36.0/36.0 MB 28.1 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.7.4\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "      231.4/231.4 KB 19.1 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "      148.0/148.0 KB 19.6 MB/s eta 0:00:00\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "      94.8/94.8 KB 13.6 MB/s eta 0:00:00\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "      78.7/78.7 KB 11.6 MB/s eta 0:00:00\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.10.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "      151.7/151.7 KB 21.0 MB/s eta 0:00:00\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
      "      55.3/55.3 KB 8.0 MB/s eta 0:00:00\n",
      "Collecting urllib3>=1.26.0\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "      140.6/140.6 KB 19.6 MB/s eta 0:00:00\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "      232.7/232.7 KB 28.8 MB/s eta 0:00:00\n",
      "Collecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "      133.1/133.1 KB 18.0 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "      62.7/62.7 KB 9.0 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.57.0-py2.py3-none-any.whl (217 kB)\n",
      "      218.0/218.0 KB 23.8 MB/s eta 0:00:00\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "      155.3/155.3 KB 20.2 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "      98.3/98.3 KB 13.2 MB/s eta 0:00:00\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.15.0-py3-none-any.whl (60 kB)\n",
      "      60.1/60.1 KB 7.0 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "      61.5/61.5 KB 8.9 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "      155.3/155.3 KB 19.7 MB/s eta 0:00:00\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (514 kB)\n",
      "      514.4/514.4 KB 39.9 MB/s eta 0:00:00\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "      93.3/93.3 KB 13.6 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "      77.1/77.1 KB 10.7 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: gast, databricks-cli\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=23c330f5378964f086aad39f196bcbbcbda72b002ee95f8723378e25440264e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.17.4-py3-none-any.whl size=142894 sha256=8540c9a62eedf0a212c1ce7207aafea7b7ed47cf4e675be7233d150cbda28ac2\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/2b/1f/3dd83a4d3a7ddde907597799c4a0cf1fe46e63091b13fedf6a\n",
      "Successfully built gast databricks-cli\n",
      "Installing collected packages: tensorflow-estimator, pytz, pyasn1, zipp, wrapt, websocket-client, urllib3, typing-extensions, threadpoolctl, termcolor, tabulate, sqlparse, smmap, six, setuptools, rsa, pyyaml, pyparsing, pyjwt, pyasn1-modules, protobuf, prometheus-client, oauthlib, numpy, multidict, MarkupSafe, joblib, itsdangerous, idna, grpcio, greenlet, google-crc32c, gast, fsspec, frozenlist, entrypoints, decorator, cloudpickle, charset-normalizer, certifi, cachetools, attrs, asynctest, astor, absl-py, yarl, Werkzeug, scipy, requests, querystring-parser, python-dateutil, pyarrow, proto-plus, packaging, opt-einsum, keras-preprocessing, Jinja2, importlib-resources, importlib-metadata, h5py, gunicorn, googleapis-common-protos, google-resumable-media, google-pasta, google-auth, gitdb, async-timeout, aiosignal, sqlalchemy, scikit-learn, requests-oauthlib, pandas, markdown, Mako, keras-applications, grpcio-status, google-api-core, gitpython, docker, click, aiohttp, tensorboard, keras, grpc-google-iam-v1, google-cloud-core, google-auth-oauthlib, Flask, db-dtypes, databricks-cli, alembic, tensorflow, prometheus-flask-exporter, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, mlflow, google-cloud-aiplatform, gcsfs\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 57.5.0\n",
      "    Uninstalling setuptools-57.5.0:\n",
      "      Successfully uninstalled setuptools-57.5.0\n",
      "Successfully installed Flask-2.2.2 Jinja2-3.1.2 Mako-1.2.4 MarkupSafe-2.1.1 Werkzeug-2.2.2 absl-py-1.3.0 aiohttp-3.8.3 aiosignal-1.3.1 alembic-1.9.1 astor-0.8.1 async-timeout-4.0.2 asynctest-0.13.0 attrs-21.2.0 cachetools-5.2.0 certifi-2022.12.7 charset-normalizer-2.1.1 click-8.1.3 cloudpickle-2.2.0 databricks-cli-0.17.4 db-dtypes-1.0.5 decorator-5.1.1 docker-6.0.1 entrypoints-0.4 frozenlist-1.3.3 fsspec-2022.11.0 gast-0.2.2 gcsfs-2022.11.0 gitdb-4.0.10 gitpython-3.1.30 google-api-core-2.11.0 google-auth-2.15.0 google-auth-oauthlib-0.8.0 google-cloud-aiplatform-1.13.0 google-cloud-bigquery-2.34.4 google-cloud-core-2.3.2 google-cloud-resource-manager-1.7.0 google-cloud-storage-1.44.0 google-crc32c-1.5.0 google-pasta-0.2.0 google-resumable-media-2.4.0 googleapis-common-protos-1.57.0 greenlet-2.0.1 grpc-google-iam-v1-0.12.4 grpcio-1.51.1 grpcio-status-1.48.2 gunicorn-20.1.0 h5py-2.10.0 idna-3.4 importlib-metadata-5.2.0 importlib-resources-5.10.2 itsdangerous-2.1.2 joblib-0.17.0 keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.4.1 mlflow-1.30.0 multidict-6.0.4 numpy-1.18.1 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-21.3 pandas-1.1.4 prometheus-client-0.15.0 prometheus-flask-exporter-0.21.0 proto-plus-1.22.1 protobuf-3.20.3 pyarrow-10.0.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyjwt-2.6.0 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.7 pyyaml-6.0 querystring-parser-1.2.4 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-0.24.1 scipy-1.7.3 setuptools-45.2.0 six-1.16.0 smmap-5.0.0 sqlalchemy-1.4.45 sqlparse-0.4.3 tabulate-0.9.0 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1 termcolor-2.1.1 threadpoolctl-3.1.0 typing-extensions-4.4.0 urllib3-1.26.13 websocket-client-1.4.2 wrapt-1.14.1 yarl-1.8.2 zipp-3.11.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container d0248e5e766e\n",
      " ---> afe7faa2ca4f\n",
      "Step 11/11 : RUN pip install pandas==1.1.4 fsspec gcsfs kfp==1.8.12\n",
      " ---> Running in 37a13e670dc5\n",
      "Requirement already satisfied: pandas==1.1.4 in /usr/local/lib/python3.7/site-packages (1.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/site-packages (2022.11.0)\n",
      "Requirement already satisfied: gcsfs in /usr/local/lib/python3.7/site-packages (2022.11.0)\n",
      "Collecting kfp==1.8.12\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "      301.2/301.2 KB 2.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.4) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.4) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.4) (1.18.1)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12) (1.3.0)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "      636.6/636.6 KB 10.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12) (2.11.0)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12) (1.44.0)\n",
      "Collecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "      1.6/1.6 MB 25.4 MB/s eta 0:00:00\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "      62.1/62.1 KB 9.4 MB/s eta 0:00:00\n",
      "Collecting google-auth<2,>=1.6.1\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "      152.9/152.9 KB 16.7 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "      54.5/54.5 KB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12) (2.2.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "      58.1/58.1 KB 9.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "      56.3/56.3 KB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12) (0.9.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12) (8.1.3)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.14\n",
      "  Downloading kfp_pipeline_spec-0.1.17-py3-none-any.whl (12 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "      88.3/88.3 KB 9.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12) (3.20.3)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.10.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "      3.1/3.1 MB 48.8 MB/s eta 0:00:00\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting typing-extensions<4,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.7/site-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from gcsfs) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/site-packages (from gcsfs) (3.8.3)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/site-packages (from gcsfs) (0.8.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (21.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp==1.8.12) (5.2.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.12) (1.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.12) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.12) (2.1.1)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "      115.6/115.6 KB 15.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (1.57.0)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "      96.8/96.8 KB 13.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (45.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (0.2.8)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.12) (2.4.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.12) (2.3.2)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "      57.5/57.5 KB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12) (1.26.13)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12) (2022.12.7)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12) (1.4.2)\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "      11.8/11.8 MB 48.0 MB/s eta 0:00:00\n",
      "  Downloading pydantic-1.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "      11.8/11.8 MB 47.3 MB/s eta 0:00:00\n",
      "  Downloading pydantic-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "      11.8/11.8 MB 18.4 MB/s eta 0:00:00\n",
      "  Downloading pydantic-1.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "      11.1/11.1 MB 14.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->gcsfs) (3.4)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.12) (0.37.1)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.7.1-py2.py3-none-any.whl (19 kB)\n",
      "  Downloading google_auth_oauthlib-0.7.0-py2.py3-none-any.whl (19 kB)\n",
      "  Downloading google_auth_oauthlib-0.5.3-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.12) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.8.12) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.12) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.12) (3.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata->click<9,>=7.1.2->kfp==1.8.12) (3.11.0)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419051 sha256=f4311db54d21544bff5864471f05cafbbd3cffffe89769dfc814952a91ec976e\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116935 sha256=9273d45e495343f5e53cdf32ef5dc5f6b34ab5deae2ba021d7a18e1473f79525\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99698 sha256=a7c7d581b4db22d32fc7b9db1f832700c5a9a30d087129df151d75954fa6ac60\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/0e/7b/ed385d69453b7b754834c01d83fa9f5708ba66b4f6ed5d6a35\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=072a6f39f77edd8c469202a0a6e810b0bb69be301101a3208d940539a62c9229\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: typing-extensions, uritemplate, strip-hints, PyYAML, pyrsistent, pydantic, kfp-pipeline-spec, httplib2, fire, docstring-parser, Deprecated, cachetools, requests-toolbelt, kfp-server-api, jsonschema, google-auth, typer, kubernetes, google-auth-oauthlib, google-auth-httplib2, google-api-core, google-api-python-client, kfp\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.2.0\n",
      "    Uninstalling cachetools-5.2.0:\n",
      "      Successfully uninstalled cachetools-5.2.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.15.0\n",
      "    Uninstalling google-auth-2.15.0:\n",
      "      Successfully uninstalled google-auth-2.15.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.8.0\n",
      "    Uninstalling google-auth-oauthlib-0.8.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.8.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.11.0\n",
      "    Uninstalling google-api-core-2.11.0:\n",
      "      Successfully uninstalled google-api-core-2.11.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-resource-manager 1.7.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 PyYAML-5.4.1 cachetools-4.2.4 docstring-parser-0.15 fire-0.5.0 google-api-core-2.10.2 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-auth-oauthlib-0.5.3 httplib2-0.21.0 jsonschema-3.2.0 kfp-1.8.12 kfp-pipeline-spec-0.1.17 kfp-server-api-1.8.5 kubernetes-18.20.0 pydantic-1.9.2 pyrsistent-0.19.3 requests-toolbelt-0.10.1 strip-hints-0.1.10 typer-0.7.0 typing-extensions-3.10.0.2 uritemplate-3.0.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 37a13e670dc5\n",
      " ---> 8ce21ba5986a\n",
      "Successfully built 8ce21ba5986a\n",
      "Successfully tagged gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {BASE_IMAGE} . --progress=plain --no-cache\n",
    "# NOTE: When it has error of \n",
    "# \n",
    "# docker image prune -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7d96a-ee3b-434c-900d-d2b88c9d401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev]\n",
      "\n",
      "\u001b[1Bc66ca61f: Preparing \n",
      "\u001b[1B7342315f: Preparing \n",
      "\u001b[1B5fa66e60: Preparing \n",
      "\u001b[1B70d11f6f: Preparing \n",
      "\u001b[1B4fa0cae2: Preparing \n",
      "\u001b[1Bc81d3c51: Preparing \n",
      "\u001b[1B9881c898: Preparing \n",
      "\u001b[1Ba219fa77: Preparing \n",
      "\u001b[1Bb0dce6b7: Preparing \n",
      "\u001b[1B8ce55a0d: Preparing \n",
      "\u001b[10B342315f: Pushed   1.393GB/1.372GB\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!docker push {BASE_IMAGE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da71360-4619-4876-bdf4-df80b2153433",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57fd8e0-9455-4ca2-a8e2-e305997752d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delete files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e343684-d477-420f-8b19-94c25be5fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./baseimage-requirements.txt ./Dockerfile ./utils.py ./diagnosis_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5235ab4-47df-47c9-9903-83d311a300a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3323fa-4653-4b3f-b5aa-241c2e36bce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559bf9f-67c8-447b-8932-d0d45cf9757a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfad4c9-1a41-481b-a2b2-fc7785128c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b760b-b52f-47bc-a38f-5d4a14f402a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_GCS_BUCKET = \"gs://oyi-ds-vertex-pipeline-bucket-output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bd963-d242-46d4-ae37-5e32930846a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gcs_bucket_if_not_exist(PROJECT_ID,NEW_GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e934b-99cc-4bb5-8cf1-1dffd7979995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcs_bucket_if_not_exist(project_id: str, gcs_bucket: str):\n",
    "    from google.cloud import storage\n",
    "    gcs = storage.Client()\n",
    "    buckets = !gsutil list -p {project_id}\n",
    "    if f'{gcs_bucket}/' not in buckets:\n",
    "        !gsutil mb -l us -c standard {gcs_bucket}\n",
    "        print(f'Bucket {gcs_bucket} is created!')\n",
    "    else: \n",
    "        print(f'Bucket {gcs_bucket} already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5a03a-7b71-45b9-973f-6b049209b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_gcs_bucket = PIPELINE_ROOT#.replace(\"-nonprod\", '')\n",
    "# print(existing_gcs_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce679c6-b061-456f-8791-816ceaef7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $existing_gcs_bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94a99a-c894-4c5f-85fb-c3f4e45ecce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
