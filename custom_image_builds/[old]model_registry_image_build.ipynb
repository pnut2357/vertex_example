{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77b4364-215a-454a-b2af-be6162fcc23d",
   "metadata": {},
   "source": [
    "# Create Model Registry Docker Image for The Containers of Element\n",
    "- nosales_model_registry.py: \n",
    "- utils.py: util functions for training model\n",
    "- diagnosis_utils.py: evaluation metric functions as diagnosis for training model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f103ca-5298-4b2b-bbc0-f0bb8ec18294",
   "metadata": {},
   "source": [
    "## Contigurations from Settings.yml for Element Model Registry Docker Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f63eca-ad1f-4a16-9a41-1746f79a171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/oyi-ds-vertex/custom_image_builds\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8778b3-13a7-48f6-81d9-3b153912b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from util.pipeline_utils import yaml_import\n",
    "import argparse\n",
    "\n",
    "file_path = \"/home/jupyter/oyi-ds-vertex/settings.yml\"\n",
    "PARAMS = yaml_import(file_path)\n",
    "# ENV = PARAMS['env_flag']\n",
    "try:\n",
    "    args = pipeline_utils.get_args()\n",
    "except:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--COMMIT_ID\", required=True, type=str)\n",
    "    parser.add_argument(\"--BRANCH\", required=True, type=str)\n",
    "    parser.add_argument(\"--is_prod\", required=False, type=str)\n",
    "    # parser.add_argument(\"--MODE\", required=True, type=str)\n",
    "    # parser.add_argument(\"--STAGE1_FLAG\", required=True, type=str)\n",
    "    # parser.add_argument(\"--ENSEMBLE_FLAG\", required=True, type=str)\n",
    "    # parser.add_argument(\"--RF_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--LOGISTIC_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--STAGE1_NN_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--GNB_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--STG1_FEATURE_SELECTOR_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--NOSALES_MODEL_PATH\", required=True, type=str)\n",
    "    sys.args = [\n",
    "        \"--COMMIT_ID\", \"1234\",\n",
    "        \"--BRANCH\", \"dev\",\n",
    "        \"--is_prod\", \"False\",\n",
    "        # \"--MODE\", \"test\",\n",
    "        # \"--STAGE1_FLAG\", \"train\",\n",
    "        # \"--ENSEMBLE_FLAG\", \"train\",\n",
    "        # \"--RF_CLF_MODEL_PATH\", \"\",\n",
    "        # \"--LOGISTIC_CLF_MODEL_PATH\", \"\",\n",
    "        # \"--STAGE1_NN_MODEL_PATH\", \"\",\n",
    "        # \"--GNB_MODEL_PATH\", \"\",\n",
    "        # \"--STG1_FEATURE_SELECTOR_MODEL_PATH\", \"\",\n",
    "        # \"--NOSALES_MODEL_PATH\", \"\",\n",
    "    ]\n",
    "    args = parser.parse_args(sys.args)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d14674c-da82-4da3-9bb1-1255031bac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH_ID = args.BRANCH\n",
    "is_prod = args.is_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42f929a-f712-4d8a-a8a2-13738faabcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV: dev, \n",
      "PROJECT_ID: wmt-mlp-p-oyi-ds-or-oyi-dsns, \n",
      "MLFLOW_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest, \n",
      "FOLDER_NANE: modelRegistryImage\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if BRANCH_ID == \"stage\" and is_prod == \"True\":\n",
    "    BRANCH_ID = \"prod\"\n",
    "    \n",
    "ENV = BRANCH_ID\n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "MLFLOW_IMAGE = PARAMS['envs'][ENV]['MLFLOW_IMAGE']\n",
    "REGISTRY_FOLDER_NANE = \"modelRegistryImage\"\n",
    "print(f\"ENV: {ENV}, \\nPROJECT_ID: {PROJECT_ID}, \\nMLFLOW_IMAGE: {MLFLOW_IMAGE}, \\nFOLDER_NANE: {REGISTRY_FOLDER_NANE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804341a4-3f6f-4c65-8027-e5c899031c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Project to push Image in Registry\n",
    "# PROJECT_ID = \"wmt-mlp-p-oyi-ds-or-oyi-dsns\"\n",
    "# TAG = \"latest\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6353f716-ca41-424d-addc-92e42381053a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- ### Build Python Scripts -->\n",
    "\n",
    "* ML Flow Model Registry Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d8253-b510-4631-88b4-483d7006312f",
   "metadata": {},
   "source": [
    "## Write nosales_model_registry.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3e91cdd-fb58-4b22-9a98-7a2a5b793ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modelRegistryImage'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REGISTRY_FOLDER_NANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7308c1-969e-476f-894a-9b4de40e977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelRegistryImage\n"
     ]
    }
   ],
   "source": [
    "!echo $REGISTRY_FOLDER_NANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7896b744-cbd8-4dc3-a839-b7cef9c9cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $REGISTRY_FOLDER_NANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68aaaa50-6cff-4efe-9969-6dc92f9abc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf scripts\n",
    "# !mkdir -p scripts\n",
    "\n",
    "!mkdir -p $REGISTRY_FOLDER_NANE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3dbd24-04dd-4e5f-8bcc-6ef5593dcc5c",
   "metadata": {},
   "source": [
    "### ML Flow Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12dc9a6c-36f8-4479-b407-5c75f515e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modelRegistryImage/nosales_model_registry.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $REGISTRY_FOLDER_NANE/nosales_model_registry.py\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import argparse\n",
    "import mlflow.pyfunc\n",
    "import utils\n",
    "import pickle\n",
    "from tempfile import TemporaryFile\n",
    "# from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "def get_args():\n",
    "    # Import arguments to local variables\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # # cmd line args\n",
    "    # parser.add_argument(\"--PROJECT_ID\", required=True, type=str)\n",
    "    # parser.add_argument(\"--SERVICE_ACCOUNT\", required=True, type=str)\n",
    "    parser.add_argument(\"--GCS_MODEL_PATH\", required=True, type=str) # \n",
    "    parser.add_argument(\"--MODEL_REGISTRY_NAME\", required=True, type=str)\n",
    "    parser.add_argument(\"--MLFLOW_EXP_NAME\", required=True, type=str)\n",
    "    parser.add_argument(\"--CURRENT_AUC_SCORE\", required=True, type=float)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "GCS_MODEL_PATH = args.GCS_MODEL_PATH # LATEST_NOSALES_MODEL_PATH= \n",
    "MODEL_NAME = args.MODEL_REGISTRY_NAME # MODEL_REGISTRY_NAME=\"oyi_nosales_model_stage\" \n",
    "EXPERIMENT_NAME = args.MLFLOW_EXP_NAME # MLFLOW_EXP_NAME=\"oyi_training_stage\"\n",
    "CURRENT_AUC_SCORE_STACK = args.CURRENT_AUC_SCORE\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    blob = storage.blob.Blob.from_string(GCS_MODEL_PATH, client=storage.Client())\n",
    "    with TemporaryFile() as temp_file:\n",
    "        blob.download_to_file(temp_file)\n",
    "        temp_file.seek(0)\n",
    "        model=pickle.load(temp_file)\n",
    "    \n",
    "    mlflow.pyfunc.log_model(python_model=utils.EnsembleClassifierWrapper(model=model,), \n",
    "                            artifact_path=\"model\",\n",
    "                            registered_model_name=MODEL_NAME)\n",
    "    \n",
    "    \n",
    "    # data = model.predict()\n",
    "    # metrics = model.eval()\n",
    "    # mlflow.log_param(param1)\n",
    "    # mlflow.log_param(param2)\n",
    "    mlflow.log_metric(CURRENT_AUC_SCORE_STACK)\n",
    "    # mlflow.log_metric(metrics2)\n",
    "    \n",
    "# artifact_repository = './mlflow-run'\n",
    "\n",
    "# # Parameters\n",
    "# version = '1'\n",
    "# params = {\"feature1\": \"1\", \"feature2\": \"2\"}\n",
    "# auc_score = 0.7\n",
    "# run_name = \"test_model\"\n",
    "# # Initialize client\n",
    "# c = MlflowClient()\n",
    "# # If experiment exist then grab the existing one\n",
    "# # else it will create a new experiment id and will use to to run the experiments\n",
    "# try:\n",
    "#     # Get the experiment id if it already exists\n",
    "#     experiment_id = c.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "# except:\n",
    "#     # Create experiment \n",
    "#     experiment_id = c.create_experiment(EXPERIMENT_NAME, artifact_location=artifact_repository)\n",
    "    \n",
    "\n",
    "# # Launching Multiple Runs in One Program.This is easy to do because the ActiveRun object returned by mlflow.start_run() is a \n",
    "# # Python context manager. You can “scope” each run to just one block of code as follows:\n",
    "# with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:\n",
    "#     # Get run id \n",
    "#     run_id = run.info.run_uuid\n",
    "\n",
    "#     # Set the notes for the run\n",
    "#     c.set_tag(run_id,\n",
    "#               \"mlflow.note.content\",\n",
    "#               \"This is experiment for testing\")\n",
    "\n",
    "#     # Define customer tag\n",
    "#     tags = {\"Application\": \"Order Your Inventory\",\n",
    "#             \"release.candidate\": \"PMP\",\n",
    "#             \"release.version\": f\"{version}\"}\n",
    "\n",
    "#     # Set Tag\n",
    "#     mlflow.set_tags(tags)\n",
    "\n",
    "#     # Log python re details\n",
    "#     # mlflow.log_artifact('requirements.txt')\n",
    "\n",
    "#     # logging params\n",
    "#     mlflow.log_params(params)\n",
    "\n",
    "#     # Perform model training\n",
    "#     blob = storage.blob.Blob.from_string(GCS_MODEL_PATH, client=storage.Client())\n",
    "#     with TemporaryFile() as temp_file:\n",
    "#         blob.download_to_file(temp_file)\n",
    "#         temp_file.seek(0)\n",
    "#         model=pickle.load(temp_file)\n",
    "#     # Log model artifacts\n",
    "#     mlflow.pyfunc.log_model(python_model=utils.EnsembleClassifierWrapper(model=model,), \n",
    "#                             artifact_path=\"model\",\n",
    "#                             registered_model_name=MODEL_NAME)\n",
    "\n",
    "#     # Perform model evaluation \n",
    "\n",
    "#     # log metrics\n",
    "#     mlflow.log_metrics({\"stack_auc\": CURRENT_AUC_SCORE_STACK})\n",
    "\n",
    "# #     # Plot and save feature importance details\n",
    "# #     ax = plot_importance(lgb_clf, height=0.4)\n",
    "# #     filename = './images/lgb_validation_feature_importance.png'\n",
    "# #     plt.savefig(filename)\n",
    "# #     # log model artifacts\n",
    "# #     mlflow.log_artifact(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638387e-4e9c-4f88-b276-6d11a971c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile scripts/cancelled_model_registry.py\n",
    "# from google.cloud import storage\n",
    "# import os\n",
    "# import argparse\n",
    "# import mlflow.sklearn\n",
    "# import utils\n",
    "# import joblib\n",
    "# from tempfile import TemporaryFile\n",
    "\n",
    "# def get_args():\n",
    "#     # Import arguments to local variables\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     # # cmd line args\n",
    "#     # parser.add_argument(\"--PROJECT_ID\", required=True, type=str)\n",
    "#     # parser.add_argument(\"--SERVICE_ACCOUNT\", required=True, type=str)\n",
    "#     parser.add_argument(\"--GCS_MODEL_PATH\", required=True, type=str)\n",
    "#     parser.add_argument(\"--MODEL_REGISTRY_NAME\", required=True, type=str)\n",
    "#     parser.add_argument(\"--MLFLOW_EXP_NAME\", required=True, type=str)\n",
    "#     args = parser.parse_args()\n",
    "#     return args\n",
    "\n",
    "\n",
    "# args = get_args()\n",
    "# gcs_model_path = args.GCS_MODEL_PATH\n",
    "# model_name = args.MODEL_REGISTRY_NAME\n",
    "# experiment_name = args.MLFLOW_EXP_NAME\n",
    "\n",
    "\n",
    "# mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# with mlflow.start_run():\n",
    "#     blob = storage.blob.Blob.from_string(gcs_model_path, client=storage.Client())\n",
    "#     with TemporaryFile() as temp_file:\n",
    "#         blob.download_to_file(temp_file)\n",
    "#         temp_file.seek(0)\n",
    "#         model=joblib.load(temp_file)\n",
    "#     mlflow.sklearn.log_model(model, artifact_path=\"model\", registered_model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9033a7b3-80c3-4147-a5c0-4cd533cbb797",
   "metadata": {},
   "source": [
    "## Write utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ec447a-458e-4d7e-baa0-34f217416b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modelRegistryImage/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $REGISTRY_FOLDER_NANE/utils.py\n",
    "# Databricks notebook source\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, FunctionTransformer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "from pytz import timezone, utc\n",
    "from datetime import timedelta, datetime\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# assert tf.__version__=='2.3.0'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "import mlflow.pyfunc\n",
    "# from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "eligible_clubs= (4738,4787,6309,6310,6311,6312,6321,6324,6436,6535,8149,8164,\n",
    "        8167,8183,8185, 4724,4776,4817,4836,4989,4990,6329,6361,6435,8102,8106,\n",
    "        8107,8119,8196, 8213,8236,8247,4969,4808,6449)\n",
    "\n",
    "eligible_cats= (19, 28, 55, 22, 23, 33, 34, 95, 8, 41, 43, 46, 49, 51, 40, 52, 1, 58, 78, 5,\n",
    "          6, 20, 29, 31, 64, 61, 70, 83, 10, 14, 15, 16, 7, 11, 12, 17, 18, 21,\n",
    "          60, 89, 3, 4, 13, 53, 94, 98, 66, 67, 68, 2, 27, 47, 54, 36, 9, 86)\n",
    "\n",
    "eligible_cats_PI= (19, 28, 55, 22, 23, 33, 34, 95, 8, 41, 43, 46, 49, 51, 40, 52, 1, 58, 78, 5,\n",
    "          6, 20, 29, 31, 64, 61, 70, 83, 10, 14, 15, 16, 7, 11, 12, 17, 18, 21,\n",
    "          60, 89, 3, 4, 13, 53, 94, 98, 66, 67, 68, 2, 27, 47, 54, 36, 9, 86,\n",
    "          37, 38, 39, 42, 44, 48, 56, 57, 72, 93, 96)\n",
    "\n",
    "CONFIG_DIR = \"/dbfs/OYI/prod_artifacts/\"\n",
    "\n",
    "\"\"\"\n",
    "The below classes override TransformerMixin to create transformers either equivalent to\n",
    "their scikit-learn preprocessing/pipeline counter parts but are dataframe aware\n",
    "And all these functions have no side-effects, ie. don't modify the original dataframe\n",
    "\"\"\"\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select columns from pandas dataframe by specifying a list of column names\n",
    "    From this source:\n",
    "    https://github.com/philipmgoddard/pipelines/blob/master/custom_transformers.py\n",
    "    '''\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.loc[:, self.attribute_names].copy()\n",
    "\n",
    "\n",
    "class LogFeaturizer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Log1p transforms inputs, filling NAs with zeroes\n",
    "    '''\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        X[X < 0] = 0\n",
    "        res= np.log1p(X.fillna(0)).values\n",
    "        return pd.DataFrame(res, columns= [i+'_log' for i in X.columns])\n",
    "\n",
    "class ClipFeaturizer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Clips input values below min_value to min_value, and/or\n",
    "    max_value to max value.\n",
    "    '''\n",
    "    def __init__(self, min_value=None, max_value=None):\n",
    "        self.min_value=min_value\n",
    "        self.max_value=max_value\n",
    "\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        X= X.copy()\n",
    "        if self.min_value is not None:\n",
    "            X[X< self.min_value]= self.min_value\n",
    "        if self.max_value is not None:\n",
    "            X[X>self.max_value]= self.max_value\n",
    "        return X\n",
    "\n",
    "# LocationExtractor is used in cancelled items ml model \n",
    "class LocationExtractor(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Extracts location from the Reserve and Sales_Floor_Location column\n",
    "    Currently does not enforce the columns to be strings\n",
    "    Also untested when providing just one column (might break if DataFrameSelector\n",
    "    on a single column returns a Series instead of a DataFrame)\n",
    "    '''\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        newdf= {}\n",
    "        for col in X.columns:\n",
    "            extract= X.loc[:,col].str.extract('(\\w+)-.*').iloc[:,0]\n",
    "            extract= extract.fillna(value='na_{0}'.format(col))\n",
    "            newdf[col+'_proc']= extract\n",
    "        return pd.DataFrame(newdf)\n",
    "\n",
    "class TimeExtractor(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    converts date time to weekday and adds it as a new column\n",
    "    '''\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        newdf= {}\n",
    "        for col in X.columns:\n",
    "            extract= X.loc[:,col].apply(lambda x: x.weekday())\n",
    "            newdf[col+'_proc']= extract\n",
    "        return pd.DataFrame(newdf)\n",
    "\n",
    "class CategoryFeaturizer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Returns Dummy variables of categorical inputs (assumes that they are categorical for now)\n",
    "    Accepts strings and integers\n",
    "    Important: Will work even if the testing dataset that the object is transforming has fewer\n",
    "    categories than the fitted dataset, and so will have the same number of columns as the latter\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.onehot_enc= OneHotEncoder(sparse=False,dtype='int', handle_unknown='ignore')\n",
    "\n",
    "    def fit(self, X,y=None):\n",
    "        self.onehot_enc.fit(X)\n",
    "        self.colnames=[]\n",
    "        for i,col in enumerate(X.columns):\n",
    "            for level in self.onehot_enc.categories_[i]:\n",
    "                self.colnames.append(col+'_'+str(level))\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        res= self.onehot_enc.transform(X)\n",
    "        return pd.DataFrame(res, columns= self.colnames)\n",
    "\n",
    "\n",
    "class ColumnMerge(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Like scikit-learn's FeatureUnion but dataframe aware\n",
    "    '''\n",
    "    def __init__(self,transformer_list, n_jobs=None, transformer_weights=None):\n",
    "        self.tf_list= transformer_list\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for tf_name,tf in self.tf_list:\n",
    "            tf.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        res=[]\n",
    "        for tf_name,tf in self.tf_list:\n",
    "            res.append(tf.transform(X).reset_index(drop=True))\n",
    "        res= pd.concat(res, axis=1)\n",
    "        return res\n",
    "\n",
    "class ModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        df =  pd.DataFrame(self.model.predict(X), columns=['result']).reset_index(drop=True)\n",
    "        df.index = list(df.index)\n",
    "        return df\n",
    "\n",
    "\n",
    "class Stage1_NeuralNetwork(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_shape=None, num_classes=5, batch_size=128, epochs=20, verbose=2):\n",
    "        self.num_classes=num_classes\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.verbose=verbose\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # prepare data for keras-NN\n",
    "        x_train= X.values\n",
    "        self.classes_ = np.unique(y)\n",
    "        y_train= pd.get_dummies(y).values\n",
    "\n",
    "        # define model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, activation='relu', input_shape=(x_train.shape[1],), kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(50, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(25, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(12, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(9, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(self.num_classes, activation='softmax', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(),  metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        self.model.fit(x_train, y_train, batch_size=self.batch_size, epochs=self.epochs, verbose=self.verbose)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        x_test= X.values\n",
    "        return self.model.predict(x_test)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "\n",
    "def stage2_nn(input_dimen=45):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=input_dimen,  activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(70, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class EnsembleClassifierWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, context, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, stage1_preprocessor, stage1_classifiers, stage2_preprocessor, stage2_classifier):\n",
    "        self.stage1_classifiers = stage1_classifiers\n",
    "        self.stage1_preprocessor = stage1_preprocessor\n",
    "        self.stage2_preprocessor = stage2_preprocessor\n",
    "        self.stage2_classifier = stage2_classifier\n",
    "        self.classes_ = None\n",
    "\n",
    "    def _prepare_stage2_data(self, X, y):\n",
    "        X_transformed = self.stage1_preprocessor.transform(X)\n",
    "        s2_x = []\n",
    "        for clf in self.stage1_classifiers:\n",
    "            s2_x.append(clf.predict_proba(X_transformed))\n",
    "        s2_x = np.hstack(s2_x)\n",
    "\n",
    "        X_transformed2 = self.stage2_preprocessor.fit_transform(X)\n",
    "        s2_x = np.hstack([s2_x, X_transformed2.values])\n",
    "        s2_y= y.values\n",
    "        return s2_x, s2_y\n",
    "\n",
    "    def fit(self, X, y, val_x=None, val_y=None):\n",
    "        self.classes_ = np.unique(y)\n",
    "        s2_train_x, s2_train_y = self._prepare_stage2_data(X, y)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        self.stage2_classifier.fit(s2_train_x, s2_train_y, validation_split=0.2, callbacks=[es, mc])\n",
    "        self.stage2_classifier = load_model('best_model.h5')\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Use self.s1_models to get s2 test-data\n",
    "        X_transformed = self.stage1_preprocessor.transform(X)\n",
    "        s2_x = []\n",
    "        for clf in self.stage1_classifiers:\n",
    "            s2_x.append(clf.predict_proba(X_transformed))\n",
    "        s2_test_x = np.hstack(s2_x)\n",
    "        X_transformed2 = self.stage2_preprocessor.transform(X)\n",
    "        s2_test_x = np.hstack([s2_test_x, X_transformed2.values])\n",
    "        return self.stage2_classifier.predict_proba(s2_test_x)\n",
    "\n",
    "class MinMaxScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.min_max_scalar= MinMaxScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.min_max_scalar.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        arr = self.min_max_scalar.transform(X)\n",
    "        return pd.DataFrame(arr, columns=list(X.columns))\n",
    "\n",
    "\n",
    "# Functions for tpr-calculations\n",
    "\n",
    "# input: pandas.core.series.Series Eg: ( [2,4,6,7,8], [2,5,1,8,9], [1,15,3,5,2], [2,7,10,9,1] )\n",
    "# output: [7, 31, 20, 29, 20]\n",
    "def _sum_value_counts(pd_series):\n",
    "    return np.sum(list(pd_series), axis=0).tolist()\n",
    "\n",
    "\n",
    "# input: pandas.core.series.Series Eg: ['No Action Taken, already out for sale', 'Add to picklist', 'Add to picklist', 'Location updated for the item',....]\n",
    "# output: [c1, c2, c3, c4, c5] where (c1-c5) are the value_counts of target-values in the series\n",
    "def _value_counts(pd_series):\n",
    "    arr = []\n",
    "    target_values = ['No Action Taken, already out for sale', 'Add to picklist', 'Updated the on hands quantity for the item', 'Location updated for the item',\n",
    "                     'New price print sign has been printed']\n",
    "    c = Counter(pd_series)\n",
    "    for v in target_values:\n",
    "        arr.append(c[v])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def calculate_daily_tpr(df, level, path='', save=False):\n",
    "    level_name = '_'.join(_ for _ in level)\n",
    "    groupbycol = level.copy()\n",
    "    groupbycol.append('run_date')\n",
    "\n",
    "    grouped = df.groupby(groupbycol).agg({'log_id':'count','event_note': lambda x:_value_counts(x) }).\\\n",
    "            reset_index().rename(columns={'log_id':'total_action_cnt','event_note':'action_distr'})\n",
    "\n",
    "    joined = grouped.merge(grouped,on=level,how='left',suffixes=('','_1'))\n",
    "\n",
    "    joined = joined.loc[joined['run_date']>joined['run_date_1']]\n",
    "\n",
    "    joined = joined.groupby(groupbycol).agg({'total_action_cnt_1':'sum','action_distr_1': lambda x:_sum_value_counts(x) }).\\\n",
    "        reset_index().rename(columns={'total_action_cnt_1':'total_action_cnt','action_distr_1':'action_distr'})\n",
    "\n",
    "    joined[level_name + '_tpr'] = joined.apply(lambda row: [round(x/row['total_action_cnt'], 3) for x in row['action_distr']], axis=1)\n",
    "    joined.drop(columns=['action_distr', 'total_action_cnt'], inplace=True)\n",
    "\n",
    "    target_values = ['no_action_taken', 'add_to_picklist', 'update_ohq', 'update_loc', 'new_price_sign']\n",
    "    target_cols = [level_name + \"_\" + x + \"_tpr\" for x in target_values]\n",
    "    joined[target_cols] = pd.DataFrame(joined[level_name + '_tpr'].values.tolist(), index= joined.index)\n",
    "\n",
    "    select_col = groupbycol.copy()\n",
    "    select_col += target_cols\n",
    "\n",
    "    if save:\n",
    "        to_dump = joined.loc[joined['run_date']==joined.run_date.max()]\n",
    "        to_dump = to_dump[select_col]\n",
    "        local_path = level_name + '_tpr.joblib'\n",
    "        dump(to_dump, local_path)\n",
    "        storage_path = os.path.join(path, local_path)\n",
    "        blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "        blob.upload_from_filename(local_path)\n",
    "        \n",
    "\n",
    "    df = df.merge(joined[select_col], on=groupbycol, how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_all_level_tpr(df, path='', save=False):\n",
    "    levels = [ ['mkt'], ['reg'], ['club_nbr'], ['cat'], ['item_nbr'], ['club_nbr','cat'], ['state','cat'], ['mkt','cat'], ['reg','cat'] ]\n",
    "    for level in levels:\n",
    "        df = calculate_daily_tpr(df, level, path, save)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_tpr_features(nosales_test, path, config=None):\n",
    "    df = nosales_test.copy()\n",
    "    levels = [ ['mkt'], ['reg'], ['club_nbr'], ['cat'], ['item_nbr'],\n",
    "              ['club_nbr','cat'], ['state','cat'], ['mkt','cat'], ['reg','cat'] ]\n",
    "\n",
    "    for level in levels:\n",
    "        level_name = '_'.join(_ for _ in level)\n",
    "        file_name = level_name + '_tpr.joblib'\n",
    "\n",
    "        if config is None:\n",
    "            tmp = load(os.path.join(path, file_name)) #not used in production\n",
    "        else:\n",
    "            tmp = load(\"{config['model_path']}/{0}\".format(file_name))\n",
    "\n",
    "        tmp = tmp.drop('run_date', axis=1)\n",
    "        df = df.merge(tmp, on=level, how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_raw_score_thresholds(train):\n",
    "    club_thresh = {}\n",
    "\n",
    "    mins, maxs= {},{}\n",
    "\n",
    "    for club in train.club_nbr.unique():\n",
    "        train_club = train[train.club_nbr==club]\n",
    "        thresholds = np.sort(list(set(np.round(train_club.raw_score.unique(), 4))))\n",
    "\n",
    "        f1_arr = []\n",
    "        prec_arr = []\n",
    "        recall_arr= []\n",
    "        for th in thresholds:\n",
    "            y_pred = list(train_club.raw_score >= th)\n",
    "            y_true = list(train_club.action == True)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1_arr.append(f1)\n",
    "            prec_arr.append(prec)\n",
    "            recall_arr.append(recall)\n",
    "\n",
    "        club_thresh[club] = thresholds[np.argmax(f1_arr)]\n",
    "\n",
    "    return club_thresh\n",
    "\n",
    "\n",
    "def gen_thresholds(df, predictions, classes):\n",
    "    cutoff = (df.run_date.max() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    df = df.reset_index(drop=True)\n",
    "    scores = pd.DataFrame(predictions, columns=classes)\n",
    "\n",
    "    action_cols= ['Add to picklist', 'Location updated for the item',\n",
    "                  'New price print sign has been printed', 'Updated the on hands quantity for the item']\n",
    "    scores['total_score'] = scores.loc[:,action_cols].sum(axis=1)\n",
    "\n",
    "    # 'Location updated for the item', 'New price print sign has been printed'\n",
    "    df['act_bool']= df.event_note.isin(action_cols)*1\n",
    "    df['raw_score'] = scores['total_score']\n",
    "    df['action']= ~(df.event_note.isin(['No Action Taken, already out for sale','No Action Taken, already OFS']))\n",
    "\n",
    "    cols = [\"central_dt\", \"club_nbr\", \"item_nbr\", \"event_note\", \"action\", \"act_bool\", \"run_date\", \"old_nbr\", \"raw_score\"]\n",
    "    df = df[cols]\n",
    "    df_subset = df[pd.to_datetime(df.central_dt) >= cutoff]\n",
    "    np.sort(df_subset.central_dt.unique())\n",
    "    thresh = get_raw_score_thresholds(df_subset)\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def get_config(mode=1):\n",
    "\n",
    "    config_file = os.path.join(CONFIG_DIR,'config.json')\n",
    "\n",
    "    with open(config_file,'r') as f:\n",
    "        config= json.load(f)\n",
    "    return config\n",
    "\n",
    "class CustomizedGaussianNB(GaussianNB):\n",
    "    \"\"\"Cast dtype to 128 float to avoid numerical underflow\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return log-probability estimates for the test vector X.\n",
    "        Overriding function. Check value in jll and set small value to -inf.\n",
    "        Cause np.exp(very small value) will cause numerical underflow.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like, shape = [n_samples, n_classes]\n",
    "            Returns the log-probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # normalize by P(x) = P(f_1, ..., f_n)\n",
    "\n",
    "        # Replace all value smaller than -10000 to -np.inf, when the shape have\n",
    "        # more than 1 column\n",
    "        if len(jll.shape) > 1 and jll.shape[1] > 1:\n",
    "            jll[jll<=-10000] = -np.inf\n",
    "        jll = jll.astype('float128')\n",
    "        log_prob_x = logsumexp(jll, axis=1)\n",
    "        jll = jll.astype('float128')\n",
    "        log_prob_x = log_prob_x.astype('float128')\n",
    "        return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like of shape (n_samples, n_classes)\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.exp(self.predict_log_proba(X).astype('float128'))\n",
    "\n",
    "\n",
    "# class MLFlowFunc: \n",
    "\n",
    "#     def __init__(self, client):\n",
    "#         self.mlf_client = client\n",
    "    \n",
    "#     def find_latest_version(self, model_name):\n",
    "#         return self.mlf_client.get_latest_versions(model_name, stages=[\"None\"])[0].version\n",
    "    \n",
    "#     def find_version_status(self, model_name, version):\n",
    "#         return self.mlf_client.get_model_version(name=model_name, version=version)\n",
    "    \n",
    "#     def model_version_registry(self, model_name, version, stage):\n",
    "#         status = self.find_version_status(model_name, version)\n",
    "#         self.mlf_client.transition_model_version_stage(\n",
    "#             name=model_name,\n",
    "#             version=version,\n",
    "#             stage=stage\n",
    "#         )\n",
    "#         print(f\"{model_name}/{version}: {status} -> {stage}\")\n",
    "    \n",
    "#     def latest_to_registry(self, model_name, stage):\n",
    "#         lastest_version = self.find_latest_version(model_name)\n",
    "#         self.mlf_client.transition_model_version_stage(\n",
    "#             name=model_name,\n",
    "#             version=lastest_version,\n",
    "#             stage=stage\n",
    "#         )\n",
    "        \n",
    "# class MLFlowFunc: \n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.mlf_client = MlflowClient()\n",
    " \n",
    "#     def find_latest_version(self, model_name):\n",
    "#         for properties in self.mlf_client.search_registered_models():\n",
    "#             check_propertries = dict(properties)\n",
    "#             if check_propertries[\"name\"] == model_name:\n",
    "#                 for lv in check_propertries[\"latest_versions\"]:\n",
    "#                     dict_lv = dict(lv)\n",
    "#                     if dict_lv[\"current_stage\"] == \"None\":\n",
    "#                         return dict_lv[\"version\"]\n",
    "    \n",
    "#     def find_version_status(self, model_name, version):\n",
    "#         for properties in self.mlf_client.search_registered_models():\n",
    "#             check_propertries = dict(properties)\n",
    "#             if check_propertries[\"name\"] == model_name:\n",
    "#                 for lv in check_propertries[\"latest_versions\"]:\n",
    "#                     dict_lv = dict(lv)\n",
    "#                     if dict_lv[\"version\"] == version:\n",
    "#                         return dict_lv[\"current_stage\"]\n",
    "    \n",
    "#     def model_version_registry(self, model_name, version, stage):\n",
    "#         status = self.find_version_status(model_name, version)\n",
    "#         self.mlf_client.transition_model_version_stage(\n",
    "#             name=model_name,\n",
    "#             version=version,\n",
    "#             stage=stage\n",
    "#         )\n",
    "#         print(f\"{model_name}/{version}: {status} -> {stage}\")\n",
    "    \n",
    "#     def latest_to_registry(self, model_name, stage):\n",
    "#         latest_version = self.find_latest_version(model_name)\n",
    "#         self.mlf_client.transition_model_version_stage(\n",
    "#             name=model_name,\n",
    "#             version=latest_version,\n",
    "#             stage=stage\n",
    "#         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed59625-6ef6-40e1-bb95-7ec06fedfadf",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Create MLFlow Docker Image of **Dockerfile**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ae8513e-efc3-49f1-810a-6067ca33724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modelRegistryImage/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $REGISTRY_FOLDER_NANE/Dockerfile\n",
    "\n",
    "FROM hub.docker.prod.walmart.com/google/cloud-sdk:slim\n",
    "WORKDIR /root\n",
    "\n",
    "# EOF\n",
    "RUN echo 'deb http://satellite-capsule.wal-mart.com/debian/deb.debian.org/debian/ buster main' >> /etc/apt/sources.list\n",
    "RUN echo 'deb http://satellite-capsule.wal-mart.com/debian/security.debian.org/debian-security/ buster/updates main' >> /etc/apt/sources.list\n",
    "RUN echo 'deb http://satellite-capsule.wal-mart.com/debian/deb.debian.org/debian/ buster-updates main' >> /etc/apt/sources.list\n",
    "\n",
    "# added new \n",
    "ENV PIP_ROOT_USER_ACTION=ignore\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends apt-utils\n",
    "\n",
    "# install system dependencies\n",
    "RUN pip3 install --upgrade pip\n",
    "RUN apt-get clean && apt-get update -y\n",
    "RUN apt-get install -y cmake\n",
    "RUN apt-get install -y locales locales-all\n",
    "RUN apt-get install net-tools\n",
    "\n",
    "# pip Installs\n",
    "RUN pip install google_cloud_pipeline_components==1.0.7\n",
    "RUN pip install kfp_pipeline_spec==0.1.16\n",
    "# RUN pip install kfp_pipeline_spec==0.1.13\n",
    "RUN pip install pytest==6.2.3\n",
    "RUN pip install kfp==1.8.14\n",
    "RUN pip install xgboost==1.5.2\n",
    "RUN pip install protobuf==3.20.3\n",
    "# RUN pip install protobuf==3.19.4\n",
    "RUN pip install fsspec\n",
    "RUN pip install gcsfs==2022.8.2\n",
    "RUN python3 -m pip install ipykernel\n",
    "RUN python3 -m ipykernel install --user\n",
    "\n",
    "# Copy contents\n",
    "# ENV FOLDER_NANE=modelRegistryImage\n",
    "# COPY ./$FOLDER_NANE/* .\n",
    "# COPY ./* .\n",
    "COPY ./nosales_model_registry.py nosales_model_registry.py \n",
    "# ./modelRegistryImage/nosales_model_registry.py nosales_model_registry.py\n",
    "COPY ./utils.py utils.py \n",
    "# ./modelRegistryImage/utils.py utils.py\n",
    "\n",
    "# Env Variables\n",
    "RUN pip install https://repository.walmart.com/content/repositories/pangaea_releases/com/walmart/analytics/platform/library/element-mlflow-plugins-release/0.0.497/element-mlflow-plugins-release-0.0.497.tar.gz\n",
    "RUN pip install attrs==21.2.0\n",
    "RUN pip install numpy==1.18.1\n",
    "RUN pip install pandas==1.1.4\n",
    "RUN pip install setuptools==45.2.0\n",
    "RUN pip install joblib==1.1.0\n",
    "# RUN pip install joblib==0.17.0\n",
    "RUN pip install h5py==2.10.0\n",
    "RUN pip install keras==2.3.1\n",
    "RUN pip install scikit-learn==0.24.1\n",
    "RUN pip install tensorflow==1.15.4\n",
    "\n",
    "# RUN pip install mlflow\n",
    "\n",
    "# ENV PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "\n",
    "ENV DATASET_SERVICE_HOST=\"ml.prod.walmart.com\" \n",
    "ENV DATASET_SERVICE_PORT=\"31500\" \n",
    "ENV PROJECT_TOKEN=\"ab57d8bba855c2a1103e2ee6900c518572e42c8539ce3edd46afa32994382bda\"  \n",
    "ENV PROJECT_ID=\"11112\"  \n",
    "ENV ELEMENT_TOKEN=\"ab57d8bba855c2a1103e2ee6900c518572e42c8539ce3edd46afa32994382bda\"\n",
    "ENV ELEMENT_DECRYPTION_KEY=\"9YRrxbzmPm5vy4BODlWvE1Mqc\"\n",
    "\n",
    "ENV ENV=\"DEV\"  \n",
    "# DEV, STAGE\n",
    "\n",
    "ENV NOTEBOOK_ID=\"37764\"\n",
    "ENV MLFLOW_TRACKING_URI=\"element://\"\n",
    "ENV CLUSTER_NAME=\"NA\"\n",
    "ENV CLUSTER_USER=\"NA\"\n",
    "\n",
    "# ENTRYPOINT [\"python3\", \"nosales_model_registry.py\"]\n",
    "# ENTRYPOINT [\"python3\", \"cancelled_model_registry.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bfc888-8a39-4d12-b924-ed0c0e5e7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# itefile $REGISTRY_FOLDER_NANE/Dockerfile\n",
    "\n",
    "# FROM hub.docker.prod.walmart.com/google/cloud-sdk:slim\n",
    "# WORKDIR /root\n",
    "\n",
    "# # EOF\n",
    "# RUN echo 'deb http://satellite-capsule.wal-mart.com/debian/deb.debian.org/debian/ buster main' >> /etc/apt/sources.list\n",
    "# RUN echo 'deb http://satellite-capsule.wal-mart.com/debian/security.debian.org/debian-security/ buster/updates main' >> /etc/apt/sources.list\n",
    "# RUN echo 'deb http://satellite-capsule.wal-mart.com/debian/deb.debian.org/debian/ buster-updates main' >> /etc/apt/sources.list\n",
    "\n",
    "# # install system dependencies\n",
    "# RUN pip3 install --upgrade pip\n",
    "# RUN apt-get clean && apt-get update -y\n",
    "# RUN apt-get install -y cmake\n",
    "# RUN apt-get install -y locales locales-all\n",
    "# RUN apt-get install net-tools\n",
    "\n",
    "# # pip Installs\n",
    "# RUN pip install google_cloud_pipeline_components==1.0.7\n",
    "# RUN pip install kfp_pipeline_spec==0.1.16\n",
    "# # RUN pip install kfp_pipeline_spec==0.1.13\n",
    "# RUN pip install pytest==6.2.3\n",
    "# RUN pip install kfp==1.8.14\n",
    "# RUN pip install xgboost==1.5.2\n",
    "# RUN pip install protobuf==3.19.4\n",
    "# RUN pip install fsspec\n",
    "# RUN pip install gcsfs==2022.8.2\n",
    "# RUN python3 -m pip install ipykernel\n",
    "# RUN python3 -m ipykernel install --user\n",
    "\n",
    "# # Copy contents\n",
    "# # ENV FOLDER_NANE=modelRegistryImage\n",
    "# # COPY ./$FOLDER_NANE/* .\n",
    "# # COPY ./* .\n",
    "# COPY ./nosales_model_registry.py nosales_model_registry.py \n",
    "# # ./modelRegistryImage/nosales_model_registry.py nosales_model_registry.py\n",
    "# COPY ./utils.py utils.py \n",
    "# # ./modelRegistryImage/utils.py utils.py\n",
    "\n",
    "# # Env Variables\n",
    "# RUN pip install https://repository.walmart.com/content/repositories/pangaea_releases/com/walmart/analytics/platform/library/element-mlflow-plugins-release/0.0.497/element-mlflow-plugins-release-0.0.497.tar.gz\n",
    "# RUN pip install attrs==21.2.0\n",
    "# RUN pip install numpy==1.18.1\n",
    "# RUN pip install pandas==1.1.4\n",
    "# RUN pip install setuptools==45.2.0\n",
    "# RUN pip install joblib==1.1.0\n",
    "# # RUN pip install joblib==0.17.0\n",
    "# RUN pip install h5py==2.10.0\n",
    "# RUN pip install keras==2.3.1\n",
    "# RUN pip install scikit-learn==0.24.1\n",
    "# RUN pip install tensorflow==1.15.4\n",
    "\n",
    "# ENV DATASET_SERVICE_HOST=\"ml.prod.walmart.com\" \n",
    "# ENV DATASET_SERVICE_PORT=\"31500\" \n",
    "# ENV PROJECT_TOKEN=\"ab57d8bba855c2a1103e2ee6900c518572e42c8539ce3edd46afa32994382bda\"  \n",
    "# ENV PROJECT_ID=\"11112\"  \n",
    "# ENV ELEMENT_TOKEN=\"ab57d8bba855c2a1103e2ee6900c518572e42c8539ce3edd46afa32994382bda\"\n",
    "# ENV ELEMENT_DECRYPTION_KEY=\"9YRrxbzmPm5vy4BODlWvE1Mqc\"\n",
    "\n",
    "# ENV ENV=\"DEV\"  \n",
    "# # DEV, STAGE\n",
    "\n",
    "# ENV NOTEBOOK_ID=\"37764\"\n",
    "# ENV MLFLOW_TRACKING_URI=\"element://\"\n",
    "# ENV CLUSTER_NAME=\"NA\"\n",
    "# ENV CLUSTER_USER=\"NA\"\n",
    "\n",
    "# # ENTRYPOINT [\"python3\", \"nosales_model_registry.py\"]\n",
    "# # ENTRYPOINT [\"python3\", \"cancelled_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b9cd50-0911-47f0-bd80-0b5665c74cb3",
   "metadata": {},
   "source": [
    "## Build MLFlow Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f076140-4c5d-40cd-a930-3225344956e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLFLOW_IMAGE\n",
    "# docker build -t gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest . --progress=plain --no-cache\n",
    "# python3 nosales_model_registry.py --GCS_MODEL_PATH \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_nosales_model_output_dev\" --MLFLOW_EXP_NAME \"oyi_training_dev\" --MODEL_REGISTRY_NAME \"oyi_nosales_model_dev\" --CURRENT_AUC_SCORE 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4abf8073-08d7-46d1-a754-f8aa289636f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modelRegistryImage'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REGISTRY_FOLDER_NANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdc877-9146-4a65-b27e-9d5ab85e95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05358fc-fc54-432a-85f8-4d5c7ce03f06",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!docker build -t {MLFLOW_IMAGE} ./ -f ./$FOLDER_NANE/Dockerfile\n",
    "# docker build -t gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest .\n",
    "# docker build -t gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-stage:latest ./ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646a5c1-3d75-4cc9-bab7-5ab006cce2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_URI=f\"gcr.io/{PROJECT_ID}/mlflow-image:{TAG}\"\n",
    "# !docker build ./ -t {IMAGE_URI}\n",
    "# !docker push {IMAGE_URI}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0efe4e-43a5-49c6-8140-80dab117c0d1",
   "metadata": {},
   "source": [
    "## Push The MLFlow Docker Image to Google Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae6a4b-ab64-48f6-bf4f-d8da5ac6322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push {MLFLOW_IMAGE}\n",
    "# docker push gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6babe-51d5-4c83-9dc7-74aea704d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf scripts\n",
    "# !rm Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb317db-caec-48ad-81b4-8cf23a5e905e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63a9ab-fe2a-4fd1-8ed4-cd1782b0ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"display_name\": \"mlflow-model-registry\", \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\": {\"machine_type\": \"n1-standard-4\", \"accelerator_count\": 0}, \"container_spec\": {\"image_uri\": \"gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest\", \"command\": [\"python3\", \"nosales_model_registry.py\"], \"args\": [\"--GCS_MODEL_PATH\", \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_nosales_model_output_dev\", \"--MLFLOW_EXP_NAME\", \"oyi_training_dev\", \"--MODEL_REGISTRY_NAME\", \"oyi_nosales_model_dev\", \"--CURRENT_AUC_SCORE\", 0.0]}}], \"scheduling\": {\"timeout\": \"604800s\", \"restart_job_on_worker_restart\": \"false\"}, \"service_account\": \"svc-oyi-ds-or-oyi-dsns-admin@wmt-mlp-p-oyi-ds-or-oyi-dsns.iam.gserviceaccount.com\", \"tensorboard\": \"\", \"enable_web_access\": \"false\", \"network\": \"projects/12856960411/global/networks/vpcnet-private-svc-access-usc1\", \"reserved_ip_ranges\": [], \"base_output_directory\": {\"output_uri_prefix\": \"\"}}, \"labels\": {}, \"encryption_spec\": {\"kms_key_name\":\"\"}};\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bcfb85-ca19-423c-b2b8-a308f426fa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
