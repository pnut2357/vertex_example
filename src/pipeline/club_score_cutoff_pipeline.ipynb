{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0c465-2725-4054-b753-0aac20dc53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pytz\n",
    "# import joblib \n",
    "# # Imports for vertex pipeline\n",
    "# from google.cloud import aiplatform\n",
    "# import google_cloud_pipeline_components\n",
    "# from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "# from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "# from kfp.v2 import compiler\n",
    "# import datetime\n",
    "# from kfp.v2.dsl import (\n",
    "#     Artifact,\n",
    "#     component,\n",
    "#     pipeline,\n",
    "#     Input,\n",
    "#     Output,\n",
    "#     Model,\n",
    "#     Dataset,\n",
    "#     InputPath,\n",
    "#     OutputPath,\n",
    "# )\n",
    "# import kfp.components as comp\n",
    "# import kfp.dsl as dsl\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sys.path.append(str(Path(\".\").absolute().parent))\n",
    "# sys.path.append(str(Path(\".\").absolute().parent) + \"/utils\")\n",
    "# sys.path.append(str(Path(\".\").absolute().parent.parent))\n",
    "# sys.path.append(str(Path(\".\").absolute().parent.parent.parent))\n",
    "\n",
    "# from utils import *\n",
    "# import pipeline_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287d1f40-357d-4c6e-9059-0b101bd74d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Imports for vertex pipeline\n",
    "from google.cloud import aiplatform\n",
    "import google_cloud_pipeline_components\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    component,\n",
    "    Condition,\n",
    "    pipeline,\n",
    "    Input,\n",
    "    Output,\n",
    "    Model,\n",
    "    Dataset,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    ")\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from typing import NamedTuple\n",
    "\n",
    "# import c_utils\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "sys.path.append(str(Path(\".\").absolute().parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent) + \"/utils\")\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent.parent))\n",
    "\n",
    "import pipeline_utils\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2634771-cfa6-4998-b289-0639834a769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --COMMIT_ID COMMIT_ID --BRANCH BRANCH\n",
      "                             --is_prod IS_PROD\n",
      "ipykernel_launcher.py: error: the following arguments are required: --COMMIT_ID, --BRANCH, --is_prod\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    args = pipeline_utils.get_args()\n",
    "except:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--COMMIT_ID\", required=True, type=str)\n",
    "    parser.add_argument(\"--BRANCH\", required=True, type=str)\n",
    "    parser.add_argument(\"--is_prod\", required=False, type=str)\n",
    "    sys.args = [\n",
    "        \"--COMMIT_ID\", \"1234\",\n",
    "        \"--BRANCH\", \"dev\",\n",
    "        \"--is_prod\", \"False\",\n",
    "    ]\n",
    "    args = parser.parse_args(sys.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87d8a46-7c97-4194-94cd-3174c6c29b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def str2bool(value):\n",
    "#     return value.lower() in (\"True\", \"true\", \"TRUE\")\n",
    "\n",
    "BRANCH_ID = args.BRANCH\n",
    "is_prod = args.is_prod\n",
    "\n",
    "if BRANCH_ID == \"stage\" and is_prod == \"True\":\n",
    "    BRANCH_ID = \"prod\"\n",
    "    \n",
    "ENV = BRANCH_ID\n",
    "ENV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095354f-3675-4532-944e-d77df703ba12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305cb723-e38e-4e57-9093-1bd5a39960fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = ( datetime.datetime.now()  - datetime.timedelta(days=7*8) ).strftime(\"%Y-%m-%d\")\n",
    "to_date   = ( datetime.datetime.now()  - datetime.timedelta(days=1) ).strftime(\"%Y-%m-%d\")\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "PARAMS = pipeline_utils.yaml_import('settings.yml')\n",
    "\n",
    "# NETWORK = 'projects/12856960411/global/networks/vpcnet-private-svc-access-usc1'\n",
    "\n",
    "ENV = PARAMS['env_flag']\n",
    "\n",
    "NETWORK = PARAMS['envs'][ENV]['VPC_NETWORK']\n",
    "# params = pipeline_utils.yaml_import()\n",
    "BASE_IMAGE = PARAMS['envs'][ENV]['BASE_IMAGE']\n",
    "SERVICE_ACCOUNT = PARAMS['envs'][ENV]['SERVICE_ACCOUNT']\n",
    "CLUB_THRESH_PATH = PARAMS['envs'][ENV]['CLUB_THRESH_PATH']\n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "PIPELINE_ROOT = PARAMS['envs'][ENV]['CLUB_THRESH_PIPELINE_ROOT']\n",
    "PIPELINE_NAME = PARAMS['envs'][ENV]['CLUB_THRESH_PIPELINE_NAME']\n",
    "PIPELINE_JSON = PARAMS['envs'][ENV]['CLUB_THRESH_PIPELINE_JSON']\n",
    "TMP_PIPELINE_JSON = os.path.join(\"/tmp\", PIPELINE_JSON)\n",
    "LATEST_PIPELINE_PATH = PARAMS['envs'][ENV]['CLUB_THRESH_LATEST_PIPELINE_PATH']\n",
    "print(f\"ENV: {ENV}, \\nPROJECT_ID: {PROJECT_ID}, \\nBASE_IMAGE: {BASE_IMAGE}, \\nPIPELINE_NAME: {PIPELINE_NAME}, \\nPIPELINE_JSON: {PIPELINE_JSON}, \\nNETWORK: {NETWORK}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf3f49-d35f-4d67-86b3-c5aac0ebb8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEST_PIPELINE_PATH, TMP_PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850094d9-1b0c-41df-9c68-3a32fbface28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1aade6-442e-4d4a-979b-34fca5b6f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def get_logger(\n",
    "    from_date: str,\n",
    "    to_date: str,\n",
    "    project_id: str,\n",
    "    df_subset_output: Output[Dataset]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    sql = \"\"\"(select * from oyi.rm_report_logger where event_ts>= '{from_date}' and event_ts<= '{to_date}' AND\n",
    "TRIM(LOWER(event_user)) LIKE '%.%' AND TRIM(LOWER(event_txt)) = 'root_cause')\"\"\".format(from_date=from_date,to_date=to_date)\n",
    "    df = client.query(sql).to_dataframe()\n",
    "    action_thresh = 5\n",
    "    df.event_ts= pd.to_datetime(df.event_ts)\n",
    "    df['central_ts']= df.event_ts.dt.tz_convert('US/Central')\n",
    "    df_subset= df[(df.event_txt=='root_cause') & (df.event_user.str.match('\\w+\\.\\w+'))].copy()\n",
    "    df_subset['central_dt']=df_subset.central_ts.dt.date\n",
    "    df_subset= df_subset.sort_values(['central_dt','club_nbr','item_nbr','central_ts'], ascending=False)\n",
    "    #df_subset= df_subset[~df_subset.duplicated(['central_dt','club_nbr','item_nbr'],keep= 'first')]\n",
    "    df_subset= df_subset[~df_subset.duplicated(['ds_uuid','club_nbr','item_nbr'],keep= 'first')]\n",
    "    df_subset= df_subset.sort_values(['club_nbr','event_user','central_ts'],ascending=True)\n",
    "    gp= df_subset.groupby(['club_nbr','event_user'])\n",
    "    df_subset['ts_shifted']=gp.central_ts.transform(lambda x:x.shift(1))\n",
    "    #df_subset.ts_shifted= df_subset.ts_shifted.dt.tz_localize('GMT').dt.tz_convert('US/Central')\n",
    "    df_subset.ts_shifted= df_subset.ts_shifted.dt.tz_convert('US/Central')\n",
    "    df_subset['ts_diff']=  df_subset.central_ts- df_subset.ts_shifted\n",
    "    df_subset['spurious']= ~(df_subset.ts_diff.isna()) & (df_subset.ts_diff.dt.seconds <= action_thresh)\n",
    "    df_subset= df_subset[~df_subset.spurious].copy()\n",
    "\n",
    "    df_subset.to_csv(df_subset_output.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c799984-60f9-48e0-9e30-85ae7d8e8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def get_inv(\n",
    "    from_date: str,\n",
    "    to_date: str,\n",
    "    project_id: str,\n",
    "    invdash_output: Output[Dataset]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    \n",
    "    from_rundate= (pd.to_datetime(from_date)- pd.Timedelta('1 days')).date().strftime('%Y-%m-%d')\n",
    "    to_rundate= (pd.to_datetime(to_date)- pd.Timedelta('1 days')).date().strftime('%Y-%m-%d')\n",
    "    \n",
    "    \n",
    "    client = bigquery.Client(project=project_id)\n",
    "    sql = f\"\"\"select club_nbr,item_nbr,old_nbr,run_date,raw_score,special_item, report_type, uuid from oyi_prod.inventory_dashboard_history \n",
    "        where run_date>= '{from_rundate}' and run_date <= '{to_rundate}'\n",
    "        and display_ind='Display'\n",
    "        \"\"\".format(from_rundate=from_rundate,to_rundate=to_rundate)\n",
    "    invdash = client.query(sql).to_dataframe()\n",
    "    print(invdash.columns)\n",
    "    invdash.run_date = pd.to_datetime(invdash.run_date)\n",
    "    invdash['actual_date']= invdash.run_date+ pd.Timedelta('1 day')\n",
    "    invdash.actual_date= invdash.actual_date.dt.date\n",
    "    invdash= invdash[~invdash.duplicated(['actual_date','club_nbr','old_nbr'],keep= 'first')].copy()\n",
    "    invdash.actual_date = invdash.actual_date.astype(str)\n",
    "    #Removing special items which are always added to the list\n",
    "    invdash = invdash[~(invdash.special_item==1)]\n",
    "    \n",
    "    invdash.to_csv(invdash_output.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc3e76-3100-4f4d-8d67-0293f6df3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def dataprep(\n",
    "    logger_input: Input[Dataset],\n",
    "    inv_input: Input[Dataset],\n",
    "    match_nosales_output: Output[Dataset],\n",
    "    match_cancelled_output: Output[Dataset]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    logger = pd.read_csv(logger_input.path)\n",
    "    inv = pd.read_csv(inv_input.path)\n",
    "  \n",
    "    match= pd.merge(left= logger,\n",
    "                    right= inv, \n",
    "                    left_on = ['ds_uuid','club_nbr','item_nbr'],\n",
    "                    right_on= ['uuid','club_nbr','old_nbr'],\n",
    "                    how= 'inner', indicator=True, validate='one_to_one')\n",
    "    match['run_date'] = match['run_date'].astype(str)\n",
    "    match['action']= ~(match.event_note.isin(['No Action Taken, already out for sale','No Action Taken, already OFS']))\n",
    "    match_nosales= match[~ (match.report_type=='C')]\n",
    "    match_cancelled= match[(match.report_type=='C')]\n",
    "  \n",
    "    match_nosales.to_csv(match_nosales_output.path, index=False)\n",
    "    match_cancelled.to_csv(match_cancelled_output.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca60c05-ea2a-409e-b930-909b5ec6a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def get_raw_score_thresholds(\n",
    "    train_input: Input[Dataset],\n",
    "    club_thresh_output: Output[Dataset]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "    \n",
    "    club_thresh = {}\n",
    "    club_prec = {}\n",
    "    club_recall = {}\n",
    "    \n",
    "    mins, maxs= {},{}\n",
    "    \n",
    "    train = pd.read_csv(train_input.path)\n",
    "    for club in train.club_nbr.unique():\n",
    "        train_club = train[train.club_nbr==club]\n",
    "        thresholds = np.sort(list(set(np.round(train_club.raw_score.unique(), 4))))\n",
    "\n",
    "        f1_arr = []\n",
    "        prec_arr = []\n",
    "        recall_arr= []\n",
    "        for th in thresholds:\n",
    "            y_pred = list(train_club.raw_score >= th)\n",
    "            y_true = list(train_club.action == True)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1_arr.append(f1)\n",
    "            prec_arr.append(prec)\n",
    "            recall_arr.append(recall)\n",
    "        \n",
    "        club_thresh[club] = thresholds[np.argmax(f1_arr)]\n",
    "#         get the precision and recall associated with the max F1 socre\n",
    "        club_prec[club] = np.round(prec_arr[np.argmax(f1_arr)], 4)\n",
    "        club_recall[club] = np.round(recall_arr[np.argmax(f1_arr)], 4)\n",
    "   \n",
    "    df_club_prec = pd.DataFrame(club_prec.items(),columns = ['club_nbr','precision'])\n",
    "    df_club_recall = pd.DataFrame(club_recall.items(),columns = ['club_nbr','recall'])\n",
    "    df_club_thresh = pd.DataFrame(club_thresh.items(),columns = ['club_nbr','club_thresh'])\n",
    "    df_thresholds = df_club_thresh.merge(df_club_prec, how = 'left', on = 'club_nbr')\\\n",
    "                                      .merge(df_club_recall, how = 'left', on = 'club_nbr')\n",
    "    df_thresholds.to_csv(club_thresh_output.path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa86bb-efe6-4263-928c-a1132d152348",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def combine_results(\n",
    "    nosales_club_thresholds_input: Input[Dataset],\n",
    "    cancelled_club_thresholds_input: Input[Dataset],\n",
    "    club_thresh_chain_path_input: str,\n",
    "    regularized_club_thresh_chain_output: Output[Dataset],\n",
    "    unregularized_club_thresh_chain_output: Output[Dataset]\n",
    "     \n",
    "):\n",
    "    import pandas as pd\n",
    "    \n",
    "    nosales_club_thresholds = pd.read_csv(nosales_club_thresholds_input.path)\n",
    "    nosales_club_thresholds = nosales_club_thresholds.rename(columns = {'club_thresh': 'nosales_club_thresh',\n",
    "                                                                        'precision': 'nosales_precision',\n",
    "                                                                        'recall': 'nosales_recall'})\n",
    "    cancelled_club_thresholds = pd.read_csv(cancelled_club_thresholds_input.path)\n",
    "    cancelled_club_thresholds = cancelled_club_thresholds.rename(columns = {'club_thresh': 'cancelled_club_thresh',\n",
    "                                                                             'precision': 'cancelled_precision',\n",
    "                                                                             'recall': 'cancelled_recall'})\n",
    "   \n",
    "    # merge the DFs\n",
    "    df_thresholds = nosales_club_thresholds.merge(cancelled_club_thresholds, how = 'left', on = 'club_nbr')\n",
    "    # Regularize the chosen values by averaging the results with the group mean.\n",
    "    df_thresholds['nosales_club_thresh'] = ((df_thresholds['nosales_club_thresh'] + df_thresholds['nosales_club_thresh'].mean()) / 2).round(4)\n",
    "    df_thresholds['cancelled_club_thresh'] = ((df_thresholds['cancelled_club_thresh'] + df_thresholds['cancelled_club_thresh'].mean()) / 2).round(4)\n",
    "\n",
    "\n",
    "    current_time = pd.datetime.now()\n",
    "    df_thresholds['update_ts'] = current_time\n",
    "    df_thresholds.to_csv(regularized_club_thresh_chain_output.path, index=False)\n",
    "    unregularized_club_thresh_chain_output.path = f'{club_thresh_chain_path_input}/club_thresh_chain.csv'\n",
    "    \n",
    "    df_thresholds_unregularized = nosales_club_thresholds[['club_nbr', 'nosales_club_thresh']].merge(cancelled_club_thresholds[['club_nbr', 'cancelled_club_thresh']],\n",
    "                                                                      how = 'left', on = 'club_nbr')\n",
    "    df_thresholds_unregularized.to_csv(f'{club_thresh_chain_path_input}/club_thresh_chain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18916d58-f7e5-4003-9397-128816174ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"gs://oyi-ds-club-score-cutoff-pipeline-bucket-nonprod/335163835346/oyi-ds-club-score-cutoff-pipeline-bucket-dev-20221228003046/get-raw-score-thresholds_4936985604475846656/club_thresh_output\"\n",
    "pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ec94c-d388-4cda-90bd-6fe178e72aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=PIPELINE_NAME)\n",
    "def pipeline():\n",
    "    logger = get_logger(from_date=from_date,\n",
    "                           to_date=to_date,\n",
    "                           project_id = PROJECT_ID)\n",
    "    \n",
    "    invdash = get_inv(from_date=from_date,\n",
    "                      to_date=to_date,\n",
    "                      project_id = PROJECT_ID)\n",
    "    \n",
    "    match_data = dataprep(logger_input=logger.outputs['df_subset_output'],\n",
    "                       inv_input=invdash.outputs['invdash_output'])\n",
    "    \n",
    "    nosales_result = get_raw_score_thresholds(train_input=match_data.outputs['match_nosales_output'])\n",
    "    cancelled_result = get_raw_score_thresholds(train_input=match_data.outputs['match_cancelled_output'])\n",
    "    \n",
    "    club_thresh_chain = combine_results(nosales_club_thresholds_input=nosales_result.outputs['club_thresh_output'],\n",
    "                                        cancelled_club_thresholds_input=cancelled_result.outputs['club_thresh_output'],\n",
    "                                        club_thresh_chain_path_input=CLUB_THRESH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a422211-f55d-46ee-a83e-795c0b42de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=TMP_PIPELINE_JSON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdfcd89-418c-4250-9f35-184214339c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "    template_path=TMP_PIPELINE_JSON,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={},\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429adef-e337-4d55-9161-669a06dfc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_utils.store_pipeline(\n",
    "#     storage_path=LATEST_PIPELINE_PATH, \n",
    "#     filename=TMP_PIPELINE_JSON\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1310c-d705-4ac0-9935-e9b1ab91d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job.submit(service_account=SERVICE_ACCOUNT, network=NETWORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182a93a-8f3e-45db-82b4-c3489b75f9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6875f-e35c-49e4-a577-45aa6f4a5f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea7e5c4-3451-45ba-bb61-a8db575efc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deaba8c-3343-4f84-b665-4686dbebbf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "club_thresh_chain_path_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a4f4e-d425-4430-af3e-5e591cf511a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
