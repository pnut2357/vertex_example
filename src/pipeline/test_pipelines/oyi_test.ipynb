{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire Pipeline\n",
    "data-preprocessing >> [train-test-split, train-eval-model] >> element-mlflow-model-registry, update-thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d75fbeca-a613-4cd4-8ea2-4c804747240c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Imports for vertex pipeline\n",
    "from google.cloud import aiplatform\n",
    "import google_cloud_pipeline_components\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    component,\n",
    "    pipeline,\n",
    "    Input,\n",
    "    Output,\n",
    "    Model,\n",
    "    Metrics,\n",
    "    Dataset,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    ")\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "# import c_utils\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "sys.path.append(str(Path(\".\").absolute().parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent) + \"/utils\")\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent.parent))\n",
    "\n",
    "import pipeline_utils\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --COMMIT_ID COMMIT_ID --BRANCH BRANCH\n",
      "                             --is_prod IS_PROD\n",
      "ipykernel_launcher.py: error: the following arguments are required: --COMMIT_ID, --BRANCH, --is_prod\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    args = pipeline_utils.get_args()\n",
    "except:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--COMMIT_ID\", required=True, type=str)\n",
    "    parser.add_argument(\"--BRANCH\", required=True, type=str)\n",
    "    parser.add_argument(\"--is_prod\", required=False, type=str)\n",
    "    # parser.add_argument(\"--MODE\", required=True, type=str)\n",
    "    # parser.add_argument(\"--STAGE1_FLAG\", required=True, type=str)\n",
    "    # parser.add_argument(\"--ENSEMBLE_FLAG\", required=True, type=str)\n",
    "    # parser.add_argument(\"--RF_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--LOGISTIC_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--STAGE1_NN_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--GNB_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--STG1_FEATURE_SELECTOR_MODEL_PATH\", required=True, type=str)\n",
    "    # parser.add_argument(\"--NOSALES_MODEL_PATH\", required=True, type=str)\n",
    "    sys.args = [\n",
    "        \"--COMMIT_ID\", \"1234\",\n",
    "        \"--BRANCH\", \"dev\",\n",
    "        \"--is_prod\", \"False\",\n",
    "        # \"--MODE\", \"test\",\n",
    "        # \"--STAGE1_FLAG\", \"train\",\n",
    "        # \"--ENSEMBLE_FLAG\", \"train\",\n",
    "        # \"--RF_CLF_MODEL_PATH\", \"\",\n",
    "        # \"--LOGISTIC_CLF_MODEL_PATH\", \"\",\n",
    "        # \"--STAGE1_NN_MODEL_PATH\", \"\",\n",
    "        # \"--GNB_MODEL_PATH\", \"\",\n",
    "        # \"--STG1_FEATURE_SELECTOR_MODEL_PATH\", \"\",\n",
    "        # \"--NOSALES_MODEL_PATH\", \"\",\n",
    "    ]\n",
    "    args = parser.parse_args(sys.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH_ID = args.BRANCH\n",
    "is_prod = args.is_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if BRANCH_ID == \"stage\" and is_prod == \"True\":\n",
    "    BRANCH_ID = \"prod\"\n",
    "    \n",
    "ENV = BRANCH_ID\n",
    "ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE: test \n",
      "STAGE1_FLAG: train \n",
      "ENSEMBLE_FLAG: train \n",
      "RF_CLF_MODEL_PATH:  \n",
      "LOGISTIC_CLF_MODEL_PATH:  \n",
      "STAGE1_NN_MODEL_PATH:  \n",
      "GNB_MODEL_PATH:  \n",
      "STG1_FEATURE_SELECTOR_MODEL_PATH:  \n",
      "NOSALES_MODEL_PATH: \n"
     ]
    }
   ],
   "source": [
    "PARAMS = pipeline_utils.yaml_import(\"settings.yml\")\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "# MODE \"test\" --STAGE1_FLAG \"train\" --ENSEMBLE_FLAG \"train\" --RF_CLF_MODEL_PATH \"\" --LOGISTIC_CLF_MODEL_PATH \"\" --STAGE1_NN_MODEL_PATH \"\" --GNB_MODEL_PATH \"\" --STG1_FEATURE_SELECTOR_MODEL_PATH \"\" --NOSALES_MODEL_PATH \"\"\n",
    "MODE = PARAMS[\"envs\"][ENV][\"MODE\"] # \"test\" \n",
    "STAGE1_FLAG = PARAMS[\"envs\"][ENV][\"STAGE1_FLAG\"] # \"train\" \n",
    "ENSEMBLE_FLAG = PARAMS[\"envs\"][ENV][\"ENSEMBLE_FLAG\"] # \"train\" \n",
    "RF_CLF_MODEL_PATH = PARAMS[\"envs\"][ENV][\"RF_CLF_MODEL_PATH\"] # \"\" \n",
    "LOGISTIC_CLF_MODEL_PATH = PARAMS[\"envs\"][ENV][\"LOGISTIC_CLF_MODEL_PATH\"] # \"\" \n",
    "STAGE1_NN_MODEL_PATH = PARAMS[\"envs\"][ENV][\"STAGE1_NN_MODEL_PATH\"] # \"\" \n",
    "GNB_MODEL_PATH = PARAMS[\"envs\"][ENV][\"GNB_MODEL_PATH\"] # \"\" \n",
    "STG1_FEATURE_SELECTOR_MODEL_PATH = PARAMS[\"envs\"][ENV][\"STG1_FEATURE_SELECTOR_MODEL_PATH\"] # \"\"\n",
    "NOSALES_MODEL_PATH = PARAMS[\"envs\"][ENV][\"NOSALES_MODEL_PATH\"] # \"\"\n",
    "\n",
    "\n",
    "print(\"MODE:\", MODE,\"\\nSTAGE1_FLAG:\", STAGE1_FLAG,\"\\nENSEMBLE_FLAG:\",ENSEMBLE_FLAG,\"\\nRF_CLF_MODEL_PATH:\",RF_CLF_MODEL_PATH, \"\\nLOGISTIC_CLF_MODEL_PATH:\",LOGISTIC_CLF_MODEL_PATH,\"\\nSTAGE1_NN_MODEL_PATH:\", STAGE1_NN_MODEL_PATH, \"\\nGNB_MODEL_PATH:\",\n",
    "GNB_MODEL_PATH,\"\\nSTG1_FEATURE_SELECTOR_MODEL_PATH:\", STG1_FEATURE_SELECTOR_MODEL_PATH,\"\\nNOSALES_MODEL_PATH:\", NOSALES_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV: dev, \n",
      "PROJECT_ID: wmt-mlp-p-oyi-ds-or-oyi-dsns, \n",
      "BASE_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest, \n",
      "MLFLOW_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest \n",
      "TRANSITION_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/version-transition-dev:latest\n",
      "\n",
      "PIPELINE_NAME: oyi-nosales-model-pipeline-dev, \n",
      "PIPELINE_JSON: oyi-nosales-model-pipeline-dev.json, \n",
      "MODEL_PREFIX: oyi_nosales_model\n"
     ]
    }
   ],
   "source": [
    "# PARAMS = pipeline_utils.yaml_import('settings.yml')\n",
    "\n",
    "# ENV = PARAMS['env_flag']\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "REGION = PARAMS['envs'][ENV]['REGION']\n",
    "BASE_IMAGE = PARAMS['envs'][ENV]['BASE_IMAGE']\n",
    "\n",
    "PIPELINE_ROOT = PARAMS['envs'][ENV]['PIPELINE_ROOT']\n",
    "PIPELINE_NAME = PARAMS['envs'][ENV]['PIPELINE_NAME']\n",
    "PIPELINE_JSON = PARAMS['envs'][ENV]['PIPELINE_JSON']\n",
    "TMP_PIPELINE_JSON = os.path.join(\"/tmp\", PIPELINE_JSON)\n",
    "\n",
    "\n",
    "TRAINING_TABLE_NAME = PARAMS['envs'][ENV]['TRAINING_TABLE_NAME']\n",
    "TRAINING_DATA_BQ_QUERY = f'select * from {TRAINING_TABLE_NAME} LIMIT 10000' #f'select * from {TRAINING_TABLE_NAME}'  \n",
    "\n",
    "MLFLOW_IMAGE = PARAMS['envs'][ENV]['MLFLOW_IMAGE']\n",
    "MLFLOW_EXP_NAME = PARAMS['envs'][ENV]['MLFLOW_EXP_NAME']\n",
    "MODEL_REGISTRY_NAME = PARAMS['envs'][ENV]['MODEL_REGISTRY_NAME']\n",
    "\n",
    "TRANSITION_IMAGE = PARAMS['envs'][ENV]['TRANSITION_IMAGE']\n",
    "MODEL_PREFIX = PARAMS['envs'][ENV]['MODEL_PREFIX']\n",
    "\n",
    "SERVICE_ACCOUNT = PARAMS['envs'][ENV]['SERVICE_ACCOUNT']\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    " \n",
    "# Matches on non-word, non-regular-punctuation characters.\n",
    "MATCHER = r\"\"\"[^a-zA-Z0-9'\"!@#$%\\^&*()\\[\\]{}:;<>?,.-=_+ ]+\"\"\" \n",
    "\n",
    "CLUB_THRESH_PATH = PARAMS['envs'][ENV]['CLUB_THRESH_PATH']\n",
    "LATEST_NOSALES_MODEL_PATH = PARAMS['envs'][ENV]['LATEST_NOSALES_MODEL_PATH']\n",
    "LATEST_PIPELINE_PATH = PARAMS['envs'][ENV]['LATEST_PIPELINE_PATH']\n",
    "RUN_PIPELINE = PARAMS['envs'][ENV]['RUN_PIPELINE']\n",
    "print(f\"ENV: {ENV}, \\nPROJECT_ID: {PROJECT_ID}, \\nBASE_IMAGE: {BASE_IMAGE}, \\nMLFLOW_IMAGE: {MLFLOW_IMAGE} \\nTRANSITION_IMAGE: {TRANSITION_IMAGE}\")\n",
    "print(f\"\\nPIPELINE_NAME: {PIPELINE_NAME}, \\nPIPELINE_JSON: {PIPELINE_JSON}, \\nMODEL_PREFIX: {MODEL_PREFIX}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/prework_test:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oyi_prod.oyi_train_no_testscan'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_TABLE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(packages_to_install=[\"attrs==21.2.0\", \"numpy==1.18.1\", \"pandas==1.1.4\", \"mlflow==1.11.0\", \"setuptools==45.2.0\", \"h5py==2.10.0\", \"keras==2.6.0\", \"joblib==0.17.0\", \"scikit-learn==0.24.1\", \"tensorflow==1.15.4\", \"google-cloud-storage==1.44.0\", \"google-cloud-aiplatform==1.13.0\", \"google-cloud-bigquery\",  \"fsspec\", \"gcsfs\", \"db-dtypes\"])\n",
    "@component(base_image=BASE_IMAGE)\n",
    "def data_preprocessing(\n",
    "    training_data_bq_query_input: str,\n",
    "    matcher: str,\n",
    "    project_id: str,\n",
    "    env: str,\n",
    "    pipeline_root: str,\n",
    "    training_data_output: Output[Dataset]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    import utils\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    data = client.query(training_data_bq_query_input).to_dataframe()\n",
    "    nosales_data = data[\n",
    "      (data.report_type!='C') &\n",
    "      (data.display_ind == \"Display\") &\n",
    "      (data.oh_qty>=0)]\n",
    "    nosales_data[\"item_desc\"] = nosales_data['item_desc'].str.replace(matcher, \"\", regex=True)\n",
    "    nosales_data['run_date'] = pd.to_datetime(nosales_data['run_date'])\n",
    "    max_date = nosales_data['run_date'].max()\n",
    "    cutoff_date = (max_date - timedelta(days=182)).strftime('%Y-%m-%d')\n",
    "    nosales_data = nosales_data[nosales_data.run_date > cutoff_date]\n",
    "    \n",
    "    nosales_data.replace(\"No Action Taken, already OFS\", \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.replace('Updated the NOSALES type with scrubber event', \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.sort_values(by = ['run_date','club_nbr','item_nbr','event_ts'],inplace = True)\n",
    "    nosales_data.drop_duplicates(['old_nbr','club_nbr','run_date'], keep='first',inplace = True)\n",
    "    \n",
    "    nosales_ext = utils.calculate_all_level_tpr(df=nosales_data, env=env, pipeline_root=pipeline_root, path='', save=True) #calculate_all_level_tpr(nosales_data, env, pipeline_root, save=True) \n",
    "    nosales_ext.fillna(0, inplace=True)\n",
    "    nosales_ext.to_csv(training_data_output.path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def train_test_split(\n",
    "    nosales_ext_input: Input[Dataset],\n",
    "    nosales_train_ext_output: Output[Dataset],\n",
    "    nosales_test_ext_output: Output[Dataset],\n",
    "    nosales_train_usampled_output: Output[Dataset]\n",
    "    \n",
    "):\n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    nosales_ext = pd.read_csv(nosales_ext_input.path)\n",
    "    nosales_ext['run_date'] = pd.to_datetime(nosales_ext['run_date'])\n",
    "    split_date = (nosales_ext.run_date.max() - timedelta(days=50)).strftime('%Y-%m-%d')\n",
    "    nosales_train_ext = nosales_ext[pd.to_datetime(nosales_ext.run_date) < split_date].copy() \n",
    "    nosales_test_ext  = nosales_ext[pd.to_datetime(nosales_ext.run_date) >= split_date].copy() \n",
    "\n",
    "    x=nosales_train_ext.shape[0]\n",
    "    y=nosales_test_ext.shape[0]\n",
    "    print(f\"split_date is {split_date}.\")\n",
    "    print(\"Train/Test ratio:\", x*100/(x+y))\n",
    "    seed = 2019\n",
    "    frac = 11\n",
    "    grouped = nosales_train_ext[nosales_train_ext.event_note == \"No Action Taken, already out for sale\"].groupby('club_nbr')\n",
    "    u1 = grouped.apply(lambda x: x.sample(n=int(x.shape[0]/frac),  random_state=seed)).reset_index(drop=True)\n",
    "\n",
    "    u2 = nosales_train_ext[nosales_train_ext.event_note != \"No Action Taken, already out for sale\"]\n",
    "\n",
    "    nosales_train_usampled = pd.concat([u1, u2])\n",
    "    nosales_train_usampled = nosales_train_usampled.sample(frac=1)\n",
    "    print(nosales_train_usampled.shape)\n",
    "    nosales_train_usampled.event_note.value_counts()\n",
    "    \n",
    "    nosales_train_ext.to_csv(nosales_train_ext_output.path, index=False)\n",
    "    nosales_test_ext.to_csv(nosales_test_ext_output.path, index=False)\n",
    "    nosales_train_usampled.to_csv(nosales_train_usampled_output.path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be3b59b6-65c2-465f-827d-9edca7b83947",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def train_eval_model(\n",
    "    nosales_ext_input: Input[Dataset],\n",
    "    nosales_train_ext_input: Input[Dataset],\n",
    "    nosales_test_ext_input: Input[Dataset],\n",
    "    nosales_train_usampled_input: Input[Dataset],\n",
    "    mode: str,\n",
    "    stage1_flag: str,\n",
    "    ensemble_flag: str,\n",
    "    rf_clf_model_path_input: str,\n",
    "    logistic_clf_model_path_input: str,\n",
    "    stage1_nn_model_path_input: str,\n",
    "    gnb_model_path_input: str,\n",
    "    stg1_feature_selector_model_path_input: str,\n",
    "    nosales_model_path_input: str,\n",
    "    latest_nosales_model_path_input: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    timestamp: str,\n",
    "    rf_clf_model_output: Output[Model],\n",
    "    logistic_clf_model_output: Output[Model],\n",
    "    stage1_nn_model_output: Output[Model],\n",
    "    gnb_model_output: Output[Model],\n",
    "    stg1_feature_selector_model_output: Output[Model],\n",
    "    nosales_model_output: Output[Model],\n",
    "    nosales_test_ext_output: Output[Dataset]\n",
    ") -> float:\n",
    "    import os \n",
    "    import pandas as pd\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    import utils\n",
    "    import diagnosis_utils\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.cluster import KMeans\n",
    "    import pickle\n",
    "    from google.cloud import storage, aiplatform\n",
    "    \n",
    "    nosales_ext = pd.read_csv(nosales_ext_input.path)\n",
    "    nosales_train_ext = pd.read_csv(nosales_train_ext_input.path)\n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "    nosales_train_usampled = pd.read_csv(nosales_train_usampled_input.path)\n",
    "    \n",
    "    nosales_ext['run_date'] = pd.to_datetime(nosales_ext['run_date'])\n",
    "    nosales_train_ext['run_date'] = pd.to_datetime(nosales_train_ext['run_date'])\n",
    "    nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "    nosales_train_usampled['run_date'] = pd.to_datetime(nosales_train_usampled['run_date'])\n",
    "    \n",
    "    tpr_features = [col for col in nosales_train_ext.columns if '_tpr' in col]  # len(tpr_features) : 45\n",
    "\n",
    "    numerical_features= ['gap_days','exp_scn_in_nosale_period', 'unit_retail','oh_qty','avg_sales_interval']\n",
    "    numerical_features.extend(tpr_features)\n",
    "    categorical_features = ['club_nbr','state','cat']\n",
    "\n",
    "    all_features = numerical_features + categorical_features\n",
    "    target = ['event_note']\n",
    "\n",
    "    top_features = list(['oh_qty_log',  'club_nbr_cat_update_loc_tpr_log',  'club_nbr_cat_new_price_sign_tpr_log',  'club_nbr_update_loc_tpr_log',\n",
    "    'club_nbr_new_price_sign_tpr_log',  'club_nbr_cat_add_to_picklist_tpr_log',  'item_nbr_update_ohq_tpr_log',\n",
    "    'item_nbr_add_to_picklist_tpr_log',  'club_nbr_add_to_picklist_tpr_log',  'avg_sales_interval_log', \n",
    "    'club_nbr_cat_no_action_taken_tpr_log',  'club_nbr_no_action_taken_tpr_log',  'item_nbr_no_action_taken_tpr_log',\n",
    "    'cat_add_to_picklist_tpr_log',  'unit_retail_log',  'exp_scn_in_nosale_period_log',  'club_nbr_cat_update_ohq_tpr_log', \n",
    "    'cat_update_ohq_tpr_log',  'club_nbr_update_ohq_tpr_log',  'state_cat_add_to_picklist_tpr_log',  'reg_cat_update_ohq_tpr_log',\n",
    "    'state_cat_new_price_sign_tpr_log',  'mkt_cat_new_price_sign_tpr_log',  'mkt_cat_update_ohq_tpr_log', \n",
    "    'reg_cat_add_to_picklist_tpr_log',  'state_cat_update_ohq_tpr_log',  'cat_new_price_sign_tpr_log', \n",
    "    'mkt_cat_update_loc_tpr_log',  'mkt_update_loc_tpr_log',  'mkt_new_price_sign_tpr_log', \n",
    "    'mkt_cat_add_to_picklist_tpr_log',  'mkt_no_action_taken_tpr_log',  'reg_no_action_taken_tpr_log', \n",
    "    'cat_no_action_taken_tpr_log',  'mkt_cat_no_action_taken_tpr_log',  'state_cat_update_loc_tpr_log', \n",
    "    'gap_days_log',  'reg_new_price_sign_tpr_log',  'mkt_update_ohq_tpr_log',  'state_cat_no_action_taken_tpr_log'])\n",
    "\n",
    "    if mode == 'test':\n",
    "        verbose_flag = True\n",
    "    else:\n",
    "        verbose_flag = False\n",
    "\n",
    "\n",
    "    feature_flags = {'kmeans_clustering': False}\n",
    "\n",
    "    class_weights = dict(nosales_train_usampled.event_note.value_counts()[0]/nosales_train_usampled.event_note.value_counts()[:])\n",
    "\n",
    "\n",
    "    # pipeline: location-feat\n",
    "    location_features_tf= Pipeline([\n",
    "        ('select_loc', utils.DataFrameSelector(['sales_floor_location']))\n",
    "    ])\n",
    "\n",
    "    # pipeline: time-feat\n",
    "    time_features_tf= Pipeline([\n",
    "        ('select_rundate', utils.DataFrameSelector(['run_date'])),\n",
    "        ('time_featurize', utils.TimeExtractor())\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline: other-catg-feat\n",
    "    add_cat_tf= Pipeline([\n",
    "        ('select_other_cat', utils.DataFrameSelector(['club_nbr','cat','state']))\n",
    "    ])  \n",
    "\n",
    "\n",
    "    # pipeline: K-means clustering\n",
    "    kmeans_tf = make_pipeline(\n",
    "        utils.DataFrameSelector(numerical_features),\n",
    "        utils.MinMaxScalerTransformer(),\n",
    "        utils.ModelTransformer(KMeans(2))\n",
    "    )\n",
    "\n",
    "    ######################################## Assembling 'Catg' n 'Numeric' Features  #####################################\n",
    "\n",
    "    # list(catg pipelines)\n",
    "    list_of_pipelines_for_catg_feat = [\n",
    "        ('loc_features',location_features_tf),\n",
    "        ('time_features',time_features_tf),\n",
    "        ('other_cat_features', add_cat_tf)\n",
    "    ]\n",
    "    if feature_flags['kmeans_clustering']:\n",
    "        list_of_pipelines_for_catg_feat.append(('clusters', kmeans_tf))\n",
    "\n",
    "\n",
    "    # pipeline: encoding the catg features.\n",
    "    cat_tf = Pipeline([\n",
    "        ('combine_cats', utils.ColumnMerge(transformer_list=list_of_pipelines_for_catg_feat)),\n",
    "        ('cat_featurize', utils.CategoryFeaturizer())\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline: numeric_features + log-transformation   \n",
    "    num_features_tf= Pipeline([\n",
    "        ('select_num', utils.DataFrameSelector(numerical_features)),\n",
    "        ('log', utils.LogFeaturizer()),\n",
    "        ('select_top_features', utils.DataFrameSelector(top_features))\n",
    "    ])\n",
    "\n",
    "    stage2_init_feature_num = 20\n",
    "    num_features_tf2= Pipeline([\n",
    "        ('select_num', utils.DataFrameSelector(numerical_features)),\n",
    "        ('log', utils.LogFeaturizer()),\n",
    "        ('select_top_features', utils.DataFrameSelector(top_features[:stage2_init_feature_num]))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # all_feat => catg_feat + numerical_feat\n",
    "    add_all_tf= utils.ColumnMerge([\n",
    "        ('num_features',num_features_tf),\n",
    "        ('cat_features',cat_tf)\n",
    "    ])\n",
    "\n",
    "    ############################################################## Final pipelines ######################################################################\n",
    "\n",
    "    # Lone classifier-pipelines and pre-processors\n",
    "\n",
    "    #1\n",
    "    rf_clf = RandomForestClassifier(n_jobs=-1, criterion='gini',n_estimators=50, max_depth=7,max_features='sqrt',\n",
    "                                    class_weight = class_weights )\n",
    "\n",
    "    #2\n",
    "    logistic_clf = LogisticRegression(n_jobs=-1, multi_class='multinomial', solver='lbfgs', max_iter=1000, penalty='l2', class_weight=class_weights)\n",
    "    \n",
    "\n",
    "    #3\n",
    "    gnb = utils.CustomizedGaussianNB()\n",
    "\n",
    "    #4\n",
    "    stage1_nn = utils.Stage1_NeuralNetwork(num_classes=5, batch_size=128, epochs=25, verbose=verbose_flag)\n",
    "\n",
    "\n",
    "    stage1_classifiers = {'rf_clf':rf_clf, 'logistic_clf':logistic_clf, 'stage1_nn':stage1_nn, 'gnb':gnb}\n",
    "\n",
    "    stage2_nn_input_dimen = stage2_init_feature_num + len(stage1_classifiers)*5\n",
    "    stage2_estimator = KerasClassifier(build_fn=utils.stage2_nn, input_dimen=stage2_nn_input_dimen, epochs=5, batch_size=128, verbose=verbose_flag)\n",
    "    \n",
    "    ##set flags when in mode: 'test'#####\n",
    "    # True: if you want to save stage1 models during test. Will automatically set to False when in prod\n",
    "    s1_save_flag = True\n",
    "\n",
    "    # Stage 1 models\n",
    "    ####################################\n",
    "    # Force flag to be 'train' during prod\n",
    "    if mode == 'prod':\n",
    "        s1_save_flag = False\n",
    "        stage1_flag = 'train'\n",
    "\n",
    "    stg1_feature_selector = num_features_tf\n",
    "\n",
    "\n",
    "\n",
    "    if stage1_flag == 'train': \n",
    "        print(\"Training and saving models...\")\n",
    "        X_train = stg1_feature_selector.fit_transform(nosales_train_usampled, nosales_train_usampled.event_note)\n",
    "        X_train = X_train.astype('float128')\n",
    "        y_train = nosales_train_usampled.event_note\n",
    "        if s1_save_flag:\n",
    "            with open(stg1_feature_selector_model_output.path, 'wb') as file:  \n",
    "                pickle.dump(stg1_feature_selector, file)\n",
    "    \n",
    "        X_test= stg1_feature_selector.transform(nosales_test_ext)\n",
    "        stage1_model_output_paths = {'rf_clf':rf_clf_model_output.path, 'logistic_clf':logistic_clf_model_output.path,\n",
    "                               'stage1_nn':stage1_nn_model_output.path, 'gnb':gnb_model_output.path}\n",
    "        for clf in stage1_classifiers:\n",
    "            print(clf)\n",
    "\n",
    "            model = stage1_classifiers[clf]\n",
    "            # filename = clf + \".model\"\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            print(\"\\n\")\n",
    "            if s1_save_flag:\n",
    "                save_path = stage1_model_output_paths[clf]\n",
    "                with open(save_path, 'wb') as file:  \n",
    "                    pickle.dump(model, file)\n",
    "\n",
    "    else:\n",
    "        print(\"Loading models...\")\n",
    "        \n",
    "        with open(rf_clf_model_path_input, \"rb\") as handler:\n",
    "            rf_clf = pickle.load(handler)\n",
    "       \n",
    "        with open(logistic_clf_model_path_input, \"rb\") as handler:\n",
    "            logistic_clf = pickle.load(handler)\n",
    "        \n",
    "        with open(stage1_nn_model_path_input, \"rb\") as handler:\n",
    "            stage1_nn = pickle.load(handler)\n",
    "        \n",
    "        with open(gnb_model_path_input, \"rb\") as handler:\n",
    "            gnb = pickle.load(handler)\n",
    "       \n",
    "        stage1_classifiers = {'rf_clf':rf_clf, 'logistic_clf':logistic_clf, 'stage1_nn':stage1_nn, 'gnb':gnb}\n",
    "        \n",
    "        with open(stg1_feature_selector_model_path_input, \"rb\") as handler:\n",
    "            stg1_feature_selector = pickle.load(handler)\n",
    "        X_test= stg1_feature_selector.transform(nosales_test_ext)\n",
    "        for clf in stage1_classifiers:\n",
    "            print(clf)\n",
    "            model = stage1_classifiers[clf]\n",
    "            nosales_test_ext, current_auc_score_stage1 = diagnosis_utils.model_diag(nosales_test_ext, model.predict_proba(X_test), model.classes_)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        rf_clf_model_output.path = rf_clf_model_path_input\n",
    "        logistic_clf_model_output.path = logistic_clf_model_path_input\n",
    "        stage1_nn_model_output.path = stage1_nn_model_path_input\n",
    "        gnb_model_output.path = gnb_model_path_input\n",
    "        stg1_feature_selector_model_output.path = stg1_feature_selector_model_path_input\n",
    "        \n",
    "\n",
    "\n",
    "    # ensemble model\n",
    "    #################################################################### \n",
    "    if mode == 'test':\n",
    "        train_x = nosales_train_ext\n",
    "        train_y = nosales_train_ext.event_note\n",
    "\n",
    "    if mode == 'prod':\n",
    "        ensemble_flag = 'train'\n",
    "        train_x = nosales_ext\n",
    "        train_y = nosales_ext.event_note\n",
    "\n",
    "\n",
    "    print(mode, ensemble_flag, train_x.shape[0])  \n",
    "\n",
    "    stg2_feture_selector = num_features_tf2\n",
    "\n",
    "    if ensemble_flag == 'train': \n",
    "        print(\"Training and saving ensemble...\")\n",
    "        stack_pipeline = Pipeline([\n",
    "            ('ensemble_classifier', utils.EnsembleClassifier(stg1_feature_selector, list(stage1_classifiers.values()),\n",
    "                                                     stg2_feture_selector, stage2_estimator)) ])\n",
    "        stack_pipeline.fit(train_x, train_y)\n",
    "        with open(nosales_model_output.path, 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file)\n",
    "        \n",
    "        with open('latest_nosales_model_output', 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file) \n",
    "        blob = storage.blob.Blob.from_string(latest_nosales_model_path_input, client=storage.Client())\n",
    "        blob.upload_from_filename('latest_nosales_model_output')\n",
    "        print(\"Saved the final model\")\n",
    "        \n",
    "        if mode == 'test':\n",
    "            nosales_test_ext, current_auc_score_stack = diagnosis_utils.model_diag(nosales_test_ext, stack_pipeline.predict_proba(nosales_test_ext), stack_pipeline.classes_)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"Loading ensemble...\")\n",
    "        with open(nosales_model_path_input, \"rb\") as handler:\n",
    "            stack_pipeline = pickle.load(handler)\n",
    "        nosales_test_ext, current_auc_score_stack = diagnosis_utils.model_diag(nosales_test_ext, stack_pipeline.predict_proba(nosales_test_ext), stack_pipeline.classes_)\n",
    "        \n",
    "        nosales_model_output.path = nosales_model_path_input\n",
    "        with open('latest_nosales_model_output', 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file) \n",
    "        blob = storage.blob.Blob.from_string(latest_nosales_model_path_input, client=storage.Client())\n",
    "        blob.upload_from_filename('latest_nosales_model_output')\n",
    "       \n",
    "        \n",
    "    nosales_test_ext.to_csv(nosales_test_ext_output.path, index = False)\n",
    "    \n",
    "    return current_auc_score_stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(base_image=BASE_IMAGE)\n",
    "# def metrics(\n",
    "#     project_id: str,\n",
    "#     nosales_test_ext_input: Input[Dataset],\n",
    "#     nosales_model_input: Input[Model],\n",
    "#     env: str,\n",
    "#     table_name: str\n",
    "# ):\n",
    "    \n",
    "#     import utils\n",
    "#     import pandas as pd\n",
    "#     import pickle\n",
    "#     import os\n",
    "#     from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "#     from google.cloud import storage\n",
    "#     from tempfile import TemporaryFile\n",
    "#     from google.cloud import bigquery\n",
    "    \n",
    "    \n",
    "#     nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "#     nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "   \n",
    "#     with open(nosales_model_input.path, \"rb\") as handler:\n",
    "#         stack_pipeline = pickle.load(handler)\n",
    "        \n",
    "    \n",
    "#     prob_result = pd.DataFrame(stack_pipeline.predict_proba(X=nosales_test_ext), columns=stack_pipeline.classes_)\n",
    "#     prob_result[\"maxResult\"] = prob_result.apply(lambda x: prob_result.columns[x.argmax()], axis=1)\n",
    "#     report = classification_report(nosales_test_ext.event_note, prob_result.maxResult, output_dict=True)\n",
    "#     df_report = pd.DataFrame(report).T\n",
    "#     df_report.reset_index(drop=False, inplace=True)\n",
    "#     df_report.rename(columns={\"f1-score\": \"f1_score\"}, inplace=True)\n",
    "    \n",
    "#     # Construct a BigQuery client object.\n",
    "#     # client = bigquery.Client(project=project_id)\n",
    "#     # data = client.query(training_data_bq_query_input).to_dataframe()\n",
    "#     client = bigquery.Client(project=project_id)\n",
    "\n",
    "#     # TODO(developer): Set table_id to the ID of the destination table.\n",
    "#     table_id = f\"wmt-mlp-p-oyi-ds-or-oyi-dsns.oyi_{env}.{table_name}\"\n",
    "    \n",
    "#     job_config = bigquery.LoadJobConfig(\n",
    "#         # Specify a (partial) schema. All columns are always written to the\n",
    "#         # table. The schema is used to assist in data type definitions.\n",
    "#         schema=[\n",
    "#             # Specify the type of columns whose type cannot be auto-detected. For\n",
    "#             # example the \"title\" column uses pandas dtype \"object\", so its\n",
    "#             # data type is ambiguous.\n",
    "#             # precision    recall  f1-score   support\n",
    "#             bigquery.SchemaField(\"index\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#             # Indexes are written if included in the schema by name.\n",
    "#             bigquery.SchemaField(\"precision\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "#             bigquery.SchemaField(\"recall\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "#             bigquery.SchemaField(\"f1_score\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "#             bigquery.SchemaField(\"support\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "#         ],\n",
    "#         # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "#         # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "#         # disposition it replaces the table with the loaded data.\n",
    "#         write_disposition=\"WRITE_TRUNCATE\",\n",
    "#     )\n",
    "\n",
    "#     job = client.load_table_from_dataframe(\n",
    "#         df_report, table_id, job_config=job_config\n",
    "#     )  # Make an API request.\n",
    "#     job.result()  # Wait for the job to complete.\n",
    "\n",
    "#     table = client.get_table(table_id)  # Make an API request.\n",
    "#     print(\n",
    "#         \"Loaded {} rows and {} columns to {}\".format(\n",
    "#             table.num_rows, len(table.schema), table_id\n",
    "#         )\n",
    "#     )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def update_thresholds(\n",
    "    nosales_test_ext_input: Input[Dataset],\n",
    "    club_thresh_path_input: str,\n",
    "    nosales_model_input: Input[Model],\n",
    "    club_threshold_output: Output[Dataset]\n",
    "):\n",
    "    \n",
    "    import utils\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    from google.cloud import storage\n",
    "    from tempfile import TemporaryFile\n",
    "    \n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "    nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "   \n",
    "    with open(nosales_model_input.path, \"rb\") as handler:\n",
    "        stack_pipeline = pickle.load(handler)\n",
    "    \n",
    "    nosales_thresh = utils.gen_thresholds(df = nosales_test_ext,  predictions = stack_pipeline.predict_proba(X=nosales_test_ext), classes = stack_pipeline.classes_)\n",
    "    df_nosales_thresh = pd.DataFrame(nosales_thresh.items(), columns = ['club_nbr','nosales_club_thresh'])\n",
    "    \n",
    "    club_threshold_file_path = os.path.join(club_thresh_path_input, \"club_thresh_chain.csv\")\n",
    "    df_cancelled_thresh = pd.read_csv(club_threshold_file_path).drop(columns = 'nosales_club_thresh')\n",
    "    all_thresh = df_cancelled_thresh.merge(df_nosales_thresh, how = 'left', on = 'club_nbr')\n",
    "    club_threshold_output.path = club_threshold_file_path\n",
    "    all_thresh.to_csv(club_threshold_file_path, index = False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=PIPELINE_NAME)\n",
    "def pipeline():    \n",
    "    data = data_preprocessing(training_data_bq_query_input=TRAINING_DATA_BQ_QUERY,\n",
    "                              matcher=MATCHER,\n",
    "                              project_id=PROJECT_ID, \n",
    "                              env=ENV, \n",
    "                              pipeline_root=PIPELINE_ROOT)\n",
    "    \n",
    "    train_test_data = train_test_split(nosales_ext_input=data.outputs['training_data_output'])\n",
    "    \n",
    "    train_eval_data = train_eval_model(nosales_ext_input=data.outputs['training_data_output'],\n",
    "                                       nosales_train_ext_input=train_test_data.outputs['nosales_train_ext_output'],\n",
    "                                       nosales_test_ext_input=train_test_data.outputs['nosales_test_ext_output'],\n",
    "                                       nosales_train_usampled_input=train_test_data.outputs['nosales_train_usampled_output'],\n",
    "                                       mode=MODE,\n",
    "                                       stage1_flag=STAGE1_FLAG,\n",
    "                                       ensemble_flag=ENSEMBLE_FLAG,\n",
    "                                       rf_clf_model_path_input=RF_CLF_MODEL_PATH,\n",
    "                                       logistic_clf_model_path_input=LOGISTIC_CLF_MODEL_PATH,\n",
    "                                       stage1_nn_model_path_input=STAGE1_NN_MODEL_PATH,\n",
    "                                       gnb_model_path_input=GNB_MODEL_PATH,\n",
    "                                       stg1_feature_selector_model_path_input=STG1_FEATURE_SELECTOR_MODEL_PATH,\n",
    "                                       nosales_model_path_input=NOSALES_MODEL_PATH,\n",
    "                                       latest_nosales_model_path_input=LATEST_NOSALES_MODEL_PATH,\n",
    "                                       project_id=PROJECT_ID,\n",
    "                                       region=REGION,\n",
    "                                       timestamp=TIMESTAMP)\n",
    "    \n",
    "    \n",
    "    # metrics_data = metrics(project_id=PROJECT_ID, \n",
    "    #                        nosales_test_ext_input=train_eval_data.outputs['nosales_test_ext_output'], \n",
    "    #                        nosales_model_input=train_eval_data.outputs['nosales_model_output'], \n",
    "    #                        env=ENV, \n",
    "    #                        table_name=\"metrics\")\n",
    "   \n",
    "    updated_thresholds = update_thresholds(nosales_test_ext_input=train_eval_data.outputs['nosales_test_ext_output'],  \n",
    "                                           club_thresh_path_input=CLUB_THRESH_PATH,\n",
    "                                           nosales_model_input=train_eval_data.outputs['nosales_model_output'])\n",
    "    \n",
    "    # CURRENT_AUC_SCORE = train_eval_data.outputs[\"current_auc_score_stack\"]\n",
    "    # CURRENT_AUC_SCORE = str([train_eval_data.outputs[\"current_auc_score_stack\"]][:])\n",
    "    # CURRENT_AUC_SCORE = json.dumps(train_eval_data.outputs[\"current_auc_score_stack\"].__dict__)\n",
    "    # print(f\"check: {CURRENT_AUC_SCORE}\")\n",
    "    \n",
    "    CURRENT_AUC_SCORE = train_eval_data.outputs[\"Output\"]\n",
    "    CURRENT_AUC_SCORE = 0.0 ##CURRENT_AUC_SCORE.value,\n",
    "    \n",
    "    element_model_registry = CustomTrainingJobOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        network=\"projects/12856960411/global/networks/vpcnet-private-svc-access-usc1\",\n",
    "        # # reserved_ip_ranges=[\"vpcnet-shared-prod-01-datafusion-01\"],\n",
    "        # network=\"projects/12856960411/global/networks/vpcnet-shared-prod-01\",\n",
    "        # reserved_ip_ranges=[\"vpcnet-shared-prod-01-datafusion-01\"],\n",
    "\n",
    "        display_name=\"mlflow-model-registry\",\n",
    "\n",
    "        worker_pool_specs=[{\n",
    "            \"replica_count\": 1,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_count\": 0,\n",
    "            },\n",
    "            # The below dictionary specifies:\n",
    "            #   1. The URI of the custom image to run this CustomTrainingJobOp against\n",
    "            #      - this image is built from ../../custom_image_builds/model_registry_image_build.ipynb\n",
    "            #   2. The command to run against that image\n",
    "            #   3. The arguments to supply to that custom image \n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": MLFLOW_IMAGE,\n",
    "                \"command\": [\n",
    "                    \"python3\", \"nosales_model_registry.py\"\n",
    "                ],\n",
    "                \"args\": [\n",
    "                    \"--GCS_MODEL_PATH\", LATEST_NOSALES_MODEL_PATH,\n",
    "                    \"--MLFLOW_EXP_NAME\", MLFLOW_EXP_NAME,\n",
    "                    \"--MODEL_REGISTRY_NAME\", MODEL_REGISTRY_NAME,\n",
    "                    \"--CURRENT_AUC_SCORE\", CURRENT_AUC_SCORE, \n",
    "                ],\n",
    "            },\n",
    "        }],\n",
    "\n",
    "    ).set_display_name(\"element-mlflow-model-registry\")\n",
    "    element_model_registry.after(train_eval_data)\n",
    "    \n",
    "    version_transition = CustomTrainingJobOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        network=\"projects/12856960411/global/networks/vpcnet-private-svc-access-usc1\",\n",
    "\n",
    "        display_name=\"mlflow-version-transition\",\n",
    "\n",
    "        worker_pool_specs=[{\n",
    "            \"replica_count\": 1,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_count\": 0,\n",
    "            },\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": TRANSITION_IMAGE,\n",
    "                \"command\": [\n",
    "                    \"python3\", \"version_transition.py\"\n",
    "                ],\n",
    "                \"args\": [\n",
    "                    \"--ENV\", ENV,\n",
    "                    \"--MODEL_PREFIX\", MODEL_PREFIX,\n",
    "                    \"--CURRENT_AUC_SCORE\", CURRENT_AUC_SCORE,\n",
    "                ],\n",
    "            },\n",
    "        }],\n",
    "        \n",
    "    ).set_display_name(\"element-mlflow-version-transition\")\n",
    "    version_transition.after(element_model_registry)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oyi-nosales-model-pipeline-dev.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/oyi-nosales-model-pipeline-dev.json'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TMP_PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=TMP_PIPELINE_JSON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "    template_path=TMP_PIPELINE_JSON,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={},\n",
    "    enable_caching=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/oyi-nosales-model-pipeline-dev.json\n",
      "contents /tmp/oyi-nosales-model-pipeline-dev.json uploaded to gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_training_pipeline_dev.json.\n"
     ]
    }
   ],
   "source": [
    "pipeline_utils.store_pipeline(\n",
    "    storage_path=LATEST_PIPELINE_PATH, \n",
    "    filename=TMP_PIPELINE_JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/335163835346/locations/us-central1/pipelineJobs/oyi-nosales-model-pipeline-dev-20221223172839\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/335163835346/locations/us-central1/pipelineJobs/oyi-nosales-model-pipeline-dev-20221223172839')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/oyi-nosales-model-pipeline-dev-20221223172839?project=335163835346\n"
     ]
    }
   ],
   "source": [
    "# 'projects/12856960411/global/networks/vpcnet-private-svc-access-usc1' # 'projects/12856960411/global/networks/vpcnet-private-svc-access-use4'\n",
    "pipeline_job.submit(service_account=SERVICE_ACCOUNT,network='projects/12856960411/global/networks/vpcnet-private-svc-access-usc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_nosales_model_output_dev*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "train_model",
   "notebookOrigID": 2093267122234119,
   "widgets": {}
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
