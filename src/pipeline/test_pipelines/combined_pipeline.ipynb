{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire Pipeline\n",
    "data-preprocessing >> [train-test-split, train-eval-model] >> element-mlflow-model-registry, update-thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d75fbeca-a613-4cd4-8ea2-4c804747240c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "# Imports for vertex pipeline\n",
    "from google.cloud import aiplatform\n",
    "import google_cloud_pipeline_components\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    component,\n",
    "    pipeline,\n",
    "    Input,\n",
    "    Output,\n",
    "    Model,\n",
    "    Dataset,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    ")\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(str(Path(\".\").absolute().parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent) + \"/utils\")\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent.parent))\n",
    "import pipeline_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --MODE MODE --STAGE1_FLAG STAGE1_FLAG\n",
      "                             --ENSEMBLE_FLAG ENSEMBLE_FLAG --RF_CLF_MODEL_PATH\n",
      "                             RF_CLF_MODEL_PATH --LOGISTIC_CLF_MODEL_PATH\n",
      "                             LOGISTIC_CLF_MODEL_PATH --STAGE1_NN_MODEL_PATH\n",
      "                             STAGE1_NN_MODEL_PATH --GNB_MODEL_PATH\n",
      "                             GNB_MODEL_PATH --STG1_FEATURE_SELECTOR_MODEL_PATH\n",
      "                             STG1_FEATURE_SELECTOR_MODEL_PATH\n",
      "                             --NOSALES_MODEL_PATH NOSALES_MODEL_PATH\n",
      "ipykernel_launcher.py: error: the following arguments are required: --MODE, --STAGE1_FLAG, --ENSEMBLE_FLAG, --RF_CLF_MODEL_PATH, --LOGISTIC_CLF_MODEL_PATH, --STAGE1_NN_MODEL_PATH, --GNB_MODEL_PATH, --STG1_FEATURE_SELECTOR_MODEL_PATH, --NOSALES_MODEL_PATH\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    args = pipeline_utils.get_args()\n",
    "except:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--MODE\", required=True, type=str)\n",
    "    parser.add_argument(\"--STAGE1_FLAG\", required=True, type=str)\n",
    "    parser.add_argument(\"--ENSEMBLE_FLAG\", required=True, type=str)\n",
    "    parser.add_argument(\"--RF_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--LOGISTIC_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--STAGE1_NN_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--GNB_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--STG1_FEATURE_SELECTOR_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--NOSALES_MODEL_PATH\", required=True, type=str)\n",
    "    sys.args = [\n",
    "        \"--MODE\", \"test\",\n",
    "        \"--STAGE1_FLAG\", \"train\",\n",
    "        \"--ENSEMBLE_FLAG\", \"train\",\n",
    "        \"--RF_CLF_MODEL_PATH\", \"\",\n",
    "        \"--LOGISTIC_CLF_MODEL_PATH\", \"\",\n",
    "        \"--STAGE1_NN_MODEL_PATH\", \"\",\n",
    "        \"--GNB_MODEL_PATH\", \"\",\n",
    "        \"--STG1_FEATURE_SELECTOR_MODEL_PATH\", \"\",\n",
    "        \"--NOSALES_MODEL_PATH\", \"\",\n",
    "    ]\n",
    "    args = parser.parse_args(sys.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV: dev, \n",
      "PROJECT_ID: wmt-mlp-p-oyi-ds-or-oyi-dsns, \n",
      "BASE_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest, \n",
      "MLFLOW_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest, \n",
      "\n",
      "NOSALE_PIPELINE_NAME: oyi-nosales-model-pipeline-dev, \n",
      "LATEST_NOSALES_PATH: gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_nosales_model_output_dev\n",
      "\n",
      "CLUB_THRESH_PIPELINE_NAME: oyi-ds-club-score-cutoff-pipeline-bucket-dev, \n",
      "LATEST_CLUB_THRESH_PATH: gs://oyi-ds-club-score-cutoff-pipeline-bucket-nonprod/latest_club_thresh_chain_dev\n"
     ]
    }
   ],
   "source": [
    "# Time-stamps\n",
    "from_date = ( datetime.datetime.now()  - datetime.timedelta(days=7*8) ).strftime(\"%Y-%m-%d\")\n",
    "to_date   = ( datetime.datetime.now()  - datetime.timedelta(days=1) ).strftime(\"%Y-%m-%d\")\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "################################\n",
    "# Parameters from settings.yml\n",
    "################################\n",
    "PARAMS = pipeline_utils.yaml_import('settings.yml')\n",
    "\n",
    "\n",
    "# Env Flag for which env to run and Service Account\n",
    "ENV = PARAMS['env_flag']\n",
    "SERVICE_ACCOUNT = PARAMS['envs'][ENV]['SERVICE_ACCOUNT']\n",
    "\n",
    "# GCP Project ID, Region, Base Image and MLFlow Image\n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "REGION = PARAMS['envs'][ENV]['REGION']\n",
    "\n",
    "BASE_IMAGE = PARAMS['envs'][ENV]['BASE_IMAGE']\n",
    "MLFLOW_IMAGE = PARAMS['envs'][ENV]['MLFLOW_IMAGE']\n",
    "\n",
    "# Pipeline Run Flag\n",
    "RUN_PIPELINE = PARAMS['envs'][ENV]['RUN_PIPELINE']\n",
    "\n",
    "# Training Pipeline\n",
    "NOSALE_PIPELINE_ROOT = PARAMS['envs'][ENV]['PIPELINE_ROOT'] # GCS bucket\n",
    "NOSALE_PIPELINE_NAME = PARAMS['envs'][ENV]['PIPELINE_NAME'] # \"oyi-nosales-model-pipeline-dev\"\n",
    "NOSALE_PIPELINE_JSON = NOSALE_PIPELINE_NAME + \".json\" \n",
    "LATEST_NOSALES_PATH = PARAMS['envs'][ENV]['LATEST_NOSALES_MODEL_PATH'] \n",
    "LATEST_NOSALES_PATH_JSON = LATEST_NOSALES_PATH + \".json\" #PARAMS['envs'][ENV]['LATEST_PIPELINE_PATH'] # 'gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_training_pipeline_dev.json'\n",
    "TMP_NOSALE_JSON = os.path.join(\"/tmp\", NOSALE_PIPELINE_JSON)\n",
    "\n",
    "# Club Threshold Pipeline\n",
    "CLUB_THRESH_PIPELINE_ROOT = PARAMS['envs'][ENV]['CLUB_THRESH_PIPELINE_ROOT'] # GCS bucket\n",
    "CLUB_THRESH_PIPELINE_NAME = PARAMS['envs'][ENV]['CLUB_THRESH_PIPELINE_NAME'] # \"oyi-ds-club-score-cutoff-pipeline-bucket-dev\"\n",
    "CLUB_THRESH_PIPELINE_JSON = CLUB_THRESH_PIPELINE_NAME + \".json\" #PARAMS['envs'][ENV]['CLUB_THRESH_PIPELINE_JSON']\n",
    "LATEST_CLUB_THRESH_PATH = PARAMS['envs'][ENV]['CLUB_THRESH_PATH']\n",
    "LATEST_CLUB_THRESH_PATH_JSON = LATEST_CLUB_THRESH_PATH + \".json\" #'gs://oyi-ds-club-score-cutoff-pipeline-bucket-nonprod/latest_pipeline_dev.json'\n",
    "TMP_CLUB_THRESH_JSON = os.path.join(\"/tmp\", CLUB_THRESH_PIPELINE_JSON)\n",
    "\n",
    "\n",
    "TRAINING_TABLE_NAME = PARAMS['envs'][ENV]['TRAINING_TABLE_NAME']\n",
    "TRAINING_DATA_BQ_QUERY = f'select * from {TRAINING_TABLE_NAME}'\n",
    "\n",
    "MLFLOW_EXP_NAME = PARAMS['envs'][ENV]['MLFLOW_EXP_NAME']\n",
    "MODEL_REGISTRY_NAME = PARAMS['envs'][ENV]['MODEL_REGISTRY_NAME']\n",
    " \n",
    "# Matches on non-word, non-regular-punctuation characters.\n",
    "MATCHER = r\"\"\"[^a-zA-Z0-9'\"!@#$%\\^&*()\\[\\]{}:;<>?,.-=_+ ]+\"\"\" \n",
    "\n",
    "print(f\"ENV: {ENV}, \\nPROJECT_ID: {PROJECT_ID}, \\nBASE_IMAGE: {BASE_IMAGE}, \\nMLFLOW_IMAGE: {MLFLOW_IMAGE}, \\n\\nNOSALE_PIPELINE_NAME: {NOSALE_PIPELINE_NAME}, \\nLATEST_NOSALES_PATH: {LATEST_NOSALES_PATH}\")\n",
    "print(f\"\\nCLUB_THRESH_PIPELINE_NAME: {CLUB_THRESH_PIPELINE_NAME}, \\nLATEST_CLUB_THRESH_PATH: {LATEST_CLUB_THRESH_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `@dsl.component` decorator transforms a Python function into a component, that can be used within a pipeline. \n",
    "# You can specify the type annotations on the parameters and return values. The type annotations and return value enable the KFP compiler \n",
    "# to type check any data that is passed. \n",
    "                \n",
    "@component(base_image=BASE_IMAGE)\n",
    "def data_preprocessing(\n",
    "    training_data_bq_query_input: str,\n",
    "    matcher: str,\n",
    "    project_id: str,\n",
    "    env: str,\n",
    "    pipeline_root: str,\n",
    "    training_data_output: Output[Dataset]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    import utils\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    data = client.query(training_data_bq_query_input).to_dataframe()\n",
    "    nosales_data = data[\n",
    "      (data.report_type!='C') &\n",
    "      (data.display_ind == \"Display\") &\n",
    "      (data.oh_qty>=0)]\n",
    "    nosales_data[\"item_desc\"] = nosales_data['item_desc'].str.replace(matcher, \"\", regex=True)\n",
    "    nosales_data['run_date'] = pd.to_datetime(nosales_data['run_date'])\n",
    "    max_date = nosales_data['run_date'].max()\n",
    "    cutoff_date = (max_date - timedelta(days=182)).strftime('%Y-%m-%d')\n",
    "    nosales_data = nosales_data[nosales_data.run_date > cutoff_date]\n",
    "    \n",
    "    nosales_data.replace(\"No Action Taken, already OFS\", \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.replace('Updated the NOSALES type with scrubber event', \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.sort_values(by = ['run_date','club_nbr','item_nbr','event_ts'],inplace = True)\n",
    "    nosales_data.drop_duplicates(['old_nbr','club_nbr','run_date'], keep='first',inplace = True)\n",
    "    \n",
    "    nosales_ext = utils.calculate_all_level_tpr(df=nosales_data, env=env, pipeline_root=pipeline_root,save=True)\n",
    "    nosales_ext.fillna(0, inplace=True)\n",
    "    nosales_ext.to_csv(training_data_output.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def train_test_split(\n",
    "    nosales_ext_input: Input[Dataset],\n",
    "    nosales_train_ext_output: Output[Dataset],\n",
    "    nosales_test_ext_output: Output[Dataset],\n",
    "    nosales_train_usampled_output: Output[Dataset]\n",
    "    \n",
    "):\n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    nosales_ext = pd.read_csv(nosales_ext_input.path)\n",
    "    nosales_ext['run_date'] = pd.to_datetime(nosales_ext['run_date'])\n",
    "    split_date = (nosales_ext.run_date.max() - timedelta(days=50)).strftime('%Y-%m-%d')\n",
    "    nosales_train_ext = nosales_ext[pd.to_datetime(nosales_ext.run_date) < split_date].copy() \n",
    "    nosales_test_ext  = nosales_ext[pd.to_datetime(nosales_ext.run_date) >= split_date].copy() \n",
    "\n",
    "    x=nosales_train_ext.shape[0]\n",
    "    y=nosales_test_ext.shape[0]\n",
    "    print(f\"split_date is {split_date}.\")\n",
    "    print(\"Train/Test ratio:\", x*100/(x+y))\n",
    "    seed = 2019\n",
    "    frac = 11\n",
    "    grouped = nosales_train_ext[nosales_train_ext.event_note == \"No Action Taken, already out for sale\"].groupby('club_nbr')\n",
    "    u1 = grouped.apply(lambda x: x.sample(n=int(x.shape[0]/frac),  random_state=seed)).reset_index(drop=True)\n",
    "\n",
    "    u2 = nosales_train_ext[nosales_train_ext.event_note != \"No Action Taken, already out for sale\"]\n",
    "\n",
    "    nosales_train_usampled = pd.concat([u1, u2])\n",
    "    nosales_train_usampled = nosales_train_usampled.sample(frac=1)\n",
    "    print(nosales_train_usampled.shape)\n",
    "    nosales_train_usampled.event_note.value_counts()\n",
    "    \n",
    "    nosales_train_ext.to_csv(nosales_train_ext_output.path, index=False)\n",
    "    nosales_test_ext.to_csv(nosales_test_ext_output.path, index=False)\n",
    "    nosales_train_usampled.to_csv(nosales_train_usampled_output.path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be3b59b6-65c2-465f-827d-9edca7b83947",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def train_eval_model(\n",
    "    nosales_ext_input: Input[Dataset],\n",
    "    nosales_train_ext_input: Input[Dataset],\n",
    "    nosales_test_ext_input: Input[Dataset],\n",
    "    nosales_train_usampled_input: Input[Dataset],\n",
    "    mode: str,\n",
    "    stage1_flag: str,\n",
    "    ensemble_flag: str,\n",
    "    rf_clf_model_path_input: str,\n",
    "    logistic_clf_model_path_input: str,\n",
    "    stage1_nn_model_path_input: str,\n",
    "    gnb_model_path_input: str,\n",
    "    stg1_feature_selector_model_path_input: str,\n",
    "    nosales_model_path_input: str,\n",
    "    latest_nosales_model_path_input: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    timestamp: str,\n",
    "    rf_clf_model_output: Output[Model],\n",
    "    logistic_clf_model_output: Output[Model],\n",
    "    stage1_nn_model_output: Output[Model],\n",
    "    gnb_model_output: Output[Model],\n",
    "    stg1_feature_selector_model_output: Output[Model],\n",
    "    nosales_model_output: Output[Model],\n",
    "    nosales_test_ext_output: Output[Dataset]\n",
    "):\n",
    "    import os \n",
    "    import pandas as pd\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    import utils\n",
    "    import diagnosis_utils\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.cluster import KMeans\n",
    "    import pickle\n",
    "    from google.cloud import storage, aiplatform\n",
    "    \n",
    "    nosales_ext = pd.read_csv(nosales_ext_input.path)\n",
    "    nosales_train_ext = pd.read_csv(nosales_train_ext_input.path)\n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "    nosales_train_usampled = pd.read_csv(nosales_train_usampled_input.path)\n",
    "    \n",
    "    nosales_ext['run_date'] = pd.to_datetime(nosales_ext['run_date'])\n",
    "    nosales_train_ext['run_date'] = pd.to_datetime(nosales_train_ext['run_date'])\n",
    "    nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "    nosales_train_usampled['run_date'] = pd.to_datetime(nosales_train_usampled['run_date'])\n",
    "    \n",
    "    tpr_features = [col for col in nosales_train_ext.columns if '_tpr' in col]  # len(tpr_features) : 45\n",
    "\n",
    "    numerical_features= ['gap_days','exp_scn_in_nosale_period', 'unit_retail','oh_qty','avg_sales_interval']\n",
    "    numerical_features.extend(tpr_features)\n",
    "    categorical_features = ['club_nbr','state','cat']\n",
    "\n",
    "    all_features = numerical_features + categorical_features\n",
    "    target = ['event_note']\n",
    "\n",
    "    top_features = list(['oh_qty_log',  'club_nbr_cat_update_loc_tpr_log',  'club_nbr_cat_new_price_sign_tpr_log',  'club_nbr_update_loc_tpr_log',\n",
    "    'club_nbr_new_price_sign_tpr_log',  'club_nbr_cat_add_to_picklist_tpr_log',  'item_nbr_update_ohq_tpr_log',\n",
    "    'item_nbr_add_to_picklist_tpr_log',  'club_nbr_add_to_picklist_tpr_log',  'avg_sales_interval_log', \n",
    "    'club_nbr_cat_no_action_taken_tpr_log',  'club_nbr_no_action_taken_tpr_log',  'item_nbr_no_action_taken_tpr_log',\n",
    "    'cat_add_to_picklist_tpr_log',  'unit_retail_log',  'exp_scn_in_nosale_period_log',  'club_nbr_cat_update_ohq_tpr_log', \n",
    "    'cat_update_ohq_tpr_log',  'club_nbr_update_ohq_tpr_log',  'state_cat_add_to_picklist_tpr_log',  'reg_cat_update_ohq_tpr_log',\n",
    "    'state_cat_new_price_sign_tpr_log',  'mkt_cat_new_price_sign_tpr_log',  'mkt_cat_update_ohq_tpr_log', \n",
    "    'reg_cat_add_to_picklist_tpr_log',  'state_cat_update_ohq_tpr_log',  'cat_new_price_sign_tpr_log', \n",
    "    'mkt_cat_update_loc_tpr_log',  'mkt_update_loc_tpr_log',  'mkt_new_price_sign_tpr_log', \n",
    "    'mkt_cat_add_to_picklist_tpr_log',  'mkt_no_action_taken_tpr_log',  'reg_no_action_taken_tpr_log', \n",
    "    'cat_no_action_taken_tpr_log',  'mkt_cat_no_action_taken_tpr_log',  'state_cat_update_loc_tpr_log', \n",
    "    'gap_days_log',  'reg_new_price_sign_tpr_log',  'mkt_update_ohq_tpr_log',  'state_cat_no_action_taken_tpr_log'])\n",
    "\n",
    "    if mode == 'test':\n",
    "        verbose_flag = True\n",
    "    else:\n",
    "        verbose_flag = False\n",
    "\n",
    "\n",
    "    feature_flags = {'kmeans_clustering': False}\n",
    "\n",
    "    class_weights = dict(nosales_train_usampled.event_note.value_counts()[0]/nosales_train_usampled.event_note.value_counts()[:])\n",
    "\n",
    "\n",
    "    # pipeline: location-feat\n",
    "    location_features_tf= Pipeline([\n",
    "        ('select_loc', utils.DataFrameSelector(['sales_floor_location']))\n",
    "    ])\n",
    "\n",
    "    # pipeline: time-feat\n",
    "    time_features_tf= Pipeline([\n",
    "        ('select_rundate', utils.DataFrameSelector(['run_date'])),\n",
    "        ('time_featurize', utils.TimeExtractor())\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline: other-catg-feat\n",
    "    add_cat_tf= Pipeline([\n",
    "        ('select_other_cat', utils.DataFrameSelector(['club_nbr','cat','state']))\n",
    "    ])  \n",
    "\n",
    "\n",
    "    # pipeline: K-means clustering\n",
    "    kmeans_tf = make_pipeline(\n",
    "        utils.DataFrameSelector(numerical_features),\n",
    "        utils.MinMaxScalerTransformer(),\n",
    "        utils.ModelTransformer(KMeans(2))\n",
    "    )\n",
    "\n",
    "    ######################################## Assembling 'Catg' n 'Numeric' Features  #####################################\n",
    "\n",
    "    # list(catg pipelines)\n",
    "    list_of_pipelines_for_catg_feat = [\n",
    "        ('loc_features',location_features_tf),\n",
    "        ('time_features',time_features_tf),\n",
    "        ('other_cat_features', add_cat_tf)\n",
    "    ]\n",
    "    if feature_flags['kmeans_clustering']:\n",
    "        list_of_pipelines_for_catg_feat.append(('clusters', kmeans_tf))\n",
    "\n",
    "\n",
    "    # pipeline: encoding the catg features.\n",
    "    cat_tf = Pipeline([\n",
    "        ('combine_cats', utils.ColumnMerge(transformer_list=list_of_pipelines_for_catg_feat)),\n",
    "        ('cat_featurize', utils.CategoryFeaturizer())\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline: numeric_features + log-transformation   \n",
    "    num_features_tf= Pipeline([\n",
    "        ('select_num', utils.DataFrameSelector(numerical_features)),\n",
    "        ('log', utils.LogFeaturizer()),\n",
    "        ('select_top_features', utils.DataFrameSelector(top_features))\n",
    "    ])\n",
    "\n",
    "    stage2_init_feature_num = 20\n",
    "    num_features_tf2= Pipeline([\n",
    "        ('select_num', utils.DataFrameSelector(numerical_features)),\n",
    "        ('log', utils.LogFeaturizer()),\n",
    "        ('select_top_features', utils.DataFrameSelector(top_features[:stage2_init_feature_num]))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # all_feat => catg_feat + numerical_feat\n",
    "    add_all_tf= utils.ColumnMerge([\n",
    "        ('num_features',num_features_tf),\n",
    "        ('cat_features',cat_tf)\n",
    "    ])\n",
    "\n",
    "    ############################################################## Final pipelines ######################################################################\n",
    "\n",
    "    # Lone classifier-pipelines and pre-processors\n",
    "\n",
    "    #1\n",
    "    rf_clf = RandomForestClassifier(n_jobs=-1, criterion='gini',n_estimators=50, max_depth=7,max_features='sqrt',\n",
    "                                    class_weight = class_weights )\n",
    "\n",
    "    #2\n",
    "    logistic_clf = LogisticRegression(n_jobs=-1, multi_class='multinomial', solver='lbfgs', max_iter=1000, penalty='l2', class_weight=class_weights)\n",
    "    \n",
    "\n",
    "    #3\n",
    "    gnb = utils.CustomizedGaussianNB()\n",
    "\n",
    "    #4\n",
    "    stage1_nn = utils.Stage1_NeuralNetwork(num_classes=5, batch_size=128, epochs=25, verbose=verbose_flag)\n",
    "\n",
    "\n",
    "    stage1_classifiers = {'rf_clf':rf_clf, 'logistic_clf':logistic_clf, 'stage1_nn':stage1_nn, 'gnb':gnb}\n",
    "\n",
    "    stage2_nn_input_dimen = stage2_init_feature_num + len(stage1_classifiers)*5\n",
    "    stage2_estimator = KerasClassifier(build_fn=utils.stage2_nn, input_dimen=stage2_nn_input_dimen, epochs=5, batch_size=128, verbose=verbose_flag)\n",
    "    \n",
    "    ##set flags when in mode: 'test'#####\n",
    "    # True: if you want to save stage1 models during test. Will automatically set to False when in prod\n",
    "    s1_save_flag = True\n",
    "\n",
    "    # Stage 1 models\n",
    "    ####################################\n",
    "    # Force flag to be 'train' during prod\n",
    "    if mode == 'prod':\n",
    "        s1_save_flag = False\n",
    "        stage1_flag = 'train'\n",
    "\n",
    "    stg1_feature_selector = num_features_tf\n",
    "\n",
    "\n",
    "\n",
    "    if stage1_flag == 'train': \n",
    "        print(\"Training and saving models...\")\n",
    "        X_train = stg1_feature_selector.fit_transform(nosales_train_usampled, nosales_train_usampled.event_note)\n",
    "        X_train = X_train.astype('float128')\n",
    "        y_train = nosales_train_usampled.event_note\n",
    "        if s1_save_flag:\n",
    "            with open(stg1_feature_selector_model_output.path, 'wb') as file:  \n",
    "                pickle.dump(stg1_feature_selector, file)\n",
    "    \n",
    "        X_test= stg1_feature_selector.transform(nosales_test_ext)\n",
    "        stage1_model_output_paths = {'rf_clf':rf_clf_model_output.path, 'logistic_clf':logistic_clf_model_output.path,\n",
    "                               'stage1_nn':stage1_nn_model_output.path, 'gnb':gnb_model_output.path}\n",
    "        for clf in stage1_classifiers:\n",
    "            print(clf)\n",
    "\n",
    "            model = stage1_classifiers[clf]\n",
    "            # filename = clf + \".model\"\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            print(\"\\n\")\n",
    "            if s1_save_flag:\n",
    "                save_path = stage1_model_output_paths[clf]\n",
    "                with open(save_path, 'wb') as file:  \n",
    "                    pickle.dump(model, file)\n",
    "\n",
    "    else:\n",
    "        print(\"Loading models...\")\n",
    "        \n",
    "        with open(rf_clf_model_path_input, \"rb\") as handler:\n",
    "            rf_clf = pickle.load(handler)\n",
    "       \n",
    "        with open(logistic_clf_model_path_input, \"rb\") as handler:\n",
    "            logistic_clf = pickle.load(handler)\n",
    "        \n",
    "        with open(stage1_nn_model_path_input, \"rb\") as handler:\n",
    "            stage1_nn = pickle.load(handler)\n",
    "        \n",
    "        with open(gnb_model_path_input, \"rb\") as handler:\n",
    "            gnb = pickle.load(handler)\n",
    "       \n",
    "        stage1_classifiers = {'rf_clf':rf_clf, 'logistic_clf':logistic_clf, 'stage1_nn':stage1_nn, 'gnb':gnb}\n",
    "        \n",
    "        with open(stg1_feature_selector_model_path_input, \"rb\") as handler:\n",
    "            stg1_feature_selector = pickle.load(handler)\n",
    "        X_test= stg1_feature_selector.transform(nosales_test_ext)\n",
    "        for clf in stage1_classifiers:\n",
    "            print(clf)\n",
    "            model = stage1_classifiers[clf]\n",
    "            nosales_test_ext = diagnosis_utils.model_diag(nosales_test_ext, model.predict_proba(X_test), model.classes_)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        rf_clf_model_output.path = rf_clf_model_path_input\n",
    "        logistic_clf_model_output.path = logistic_clf_model_path_input\n",
    "        stage1_nn_model_output.path = stage1_nn_model_path_input\n",
    "        gnb_model_output.path = gnb_model_path_input\n",
    "        stg1_feature_selector_model_output.path = stg1_feature_selector_model_path_input\n",
    "        \n",
    "\n",
    "\n",
    "    # ensemble model\n",
    "    #################################################################### \n",
    "    if mode == 'test':\n",
    "        train_x = nosales_train_ext\n",
    "        train_y = nosales_train_ext.event_note\n",
    "\n",
    "    if mode == 'prod':\n",
    "        ensemble_flag = 'train'\n",
    "        train_x = nosales_ext\n",
    "        train_y = nosales_ext.event_note\n",
    "\n",
    "\n",
    "    print(mode, ensemble_flag, train_x.shape[0])  \n",
    "\n",
    "    stg2_feture_selector = num_features_tf2\n",
    "\n",
    "    if ensemble_flag == 'train': \n",
    "        print(\"Training and saving ensemble...\")\n",
    "        stack_pipeline = Pipeline([\n",
    "            ('ensemble_classifier', utils.EnsembleClassifier(stg1_feature_selector, list(stage1_classifiers.values()),\n",
    "                                                     stg2_feture_selector, stage2_estimator)) ])\n",
    "        stack_pipeline.fit(train_x, train_y)\n",
    "        with open(nosales_model_output.path, 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file)\n",
    "        \n",
    "        with open('latest_nosales_model_output', 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file) \n",
    "        blob = storage.blob.Blob.from_string(latest_nosales_model_path_input, client=storage.Client())\n",
    "        blob.upload_from_filename('latest_nosales_model_output')\n",
    "        print(\"Saved the final model\")\n",
    "        \n",
    "        if mode == 'test':\n",
    "            nosales_test_ext = diagnosis_utils.model_diag(nosales_test_ext, stack_pipeline.predict_proba(nosales_test_ext), stack_pipeline.classes_)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"Loading ensemble...\")\n",
    "        with open(nosales_model_path_input, \"rb\") as handler:\n",
    "            stack_pipeline = pickle.load(handler)\n",
    "        nosales_test_ext = diagnosis_utils.model_diag(nosales_test_ext, stack_pipeline.predict_proba(nosales_test_ext), stack_pipeline.classes_)\n",
    "        \n",
    "        nosales_model_output.path = nosales_model_path_input\n",
    "        with open('latest_nosales_model_output', 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file) \n",
    "        blob = storage.blob.Blob.from_string(latest_nosales_model_path_input, client=storage.Client())\n",
    "        blob.upload_from_filename('latest_nosales_model_output')\n",
    "       \n",
    "        \n",
    "    nosales_test_ext.to_csv(nosales_test_ext_output.path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def update_thresholds(\n",
    "    nosales_test_ext_input: Input[Dataset],\n",
    "    club_thresh_ext_input: Input[Dataset],\n",
    "    club_thresh_path_input: str,\n",
    "    nosales_model_input: Input[Model],\n",
    "    club_threshold_output: Output[Dataset]\n",
    "):\n",
    "    \n",
    "    import utils\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    from google.cloud import storage\n",
    "    from tempfile import TemporaryFile\n",
    "    \n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "    nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "    \n",
    "    with open(nosales_model_input.path, \"rb\") as handler:\n",
    "        stack_pipeline = pickle.load(handler)\n",
    "    \n",
    "    nosales_thresh = utils.gen_thresholds(df = nosales_test_ext,  predictions = stack_pipeline.predict_proba(X=nosales_test_ext), classes = stack_pipeline.classes_)\n",
    "    df_nosales_thresh = pd.DataFrame(nosales_thresh.items(), columns = ['club_nbr','nosales_club_thresh'])\n",
    "    \n",
    "    # Old\n",
    "    club_threshold_file_path = os.path.join(club_thresh_path_input, \"club_thresh_chain.csv\")\n",
    "    # df_cancelled_thresh = pd.read_csv(club_thresh_path_input.path).drop(columns = 'nosales_club_thresh')\n",
    "    # New \n",
    "    df_cancelled_thresh = pd.read_csv(club_thresh_ext_input.path).drop(columns = 'nosales_club_thresh')\n",
    "    \n",
    "    all_thresh = df_cancelled_thresh.merge(df_nosales_thresh, how = 'left', on = 'club_nbr')\n",
    "    club_threshold_output.path = club_threshold_file_path # ????\n",
    "    all_thresh.to_csv(club_threshold_file_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Club Score Cutoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def get_logger(\n",
    "        from_date: str,\n",
    "        to_date: str,\n",
    "        project_id: str,\n",
    "        df_subset_output: Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    sql = \"\"\"(select * from oyi.rm_report_logger where event_ts>= '{from_date}' and event_ts<= '{to_date}' AND\n",
    "TRIM(LOWER(event_user)) LIKE '%.%' AND TRIM(LOWER(event_txt)) = 'root_cause')\"\"\".format(from_date=from_date,to_date=to_date)\n",
    "    df = client.query(sql).to_dataframe()\n",
    "    action_thresh = 5\n",
    "    df.event_ts= pd.to_datetime(df.event_ts)\n",
    "    df['central_ts']= df.event_ts.dt.tz_convert('US/Central')\n",
    "    df_subset= df[(df.event_txt=='root_cause') & (df.event_user.str.match('\\w+\\.\\w+'))].copy()\n",
    "    df_subset['central_dt']=df_subset.central_ts.dt.date\n",
    "    df_subset= df_subset.sort_values(['central_dt','club_nbr','item_nbr','central_ts'], ascending=False)\n",
    "    #df_subset= df_subset[~df_subset.duplicated(['central_dt','club_nbr','item_nbr'],keep= 'first')]\n",
    "    df_subset= df_subset[~df_subset.duplicated(['ds_uuid','club_nbr','item_nbr'],keep= 'first')]\n",
    "    df_subset= df_subset.sort_values(['club_nbr','event_user','central_ts'],ascending=True)\n",
    "    gp= df_subset.groupby(['club_nbr','event_user'])\n",
    "    df_subset['ts_shifted']=gp.central_ts.transform(lambda x:x.shift(1))\n",
    "    #df_subset.ts_shifted= df_subset.ts_shifted.dt.tz_localize('GMT').dt.tz_convert('US/Central')\n",
    "    df_subset.ts_shifted= df_subset.ts_shifted.dt.tz_convert('US/Central')\n",
    "    df_subset['ts_diff']=  df_subset.central_ts- df_subset.ts_shifted\n",
    "    df_subset['spurious']= ~(df_subset.ts_diff.isna()) & (df_subset.ts_diff.dt.seconds <= action_thresh)\n",
    "    df_subset= df_subset[~df_subset.spurious].copy()\n",
    "\n",
    "    df_subset.to_csv(df_subset_output.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def get_inv(\n",
    "        from_date: str,\n",
    "        to_date: str,\n",
    "        project_id: str,\n",
    "        invdash_output: Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    \n",
    "    from_rundate= (pd.to_datetime(from_date)- pd.Timedelta('1 days')).date().strftime('%Y-%m-%d')\n",
    "    to_rundate= (pd.to_datetime(to_date)- pd.Timedelta('1 days')).date().strftime('%Y-%m-%d')\n",
    "    \n",
    "    \n",
    "    client = bigquery.Client(project=project_id)\n",
    "    sql = f\"\"\"select club_nbr,item_nbr,old_nbr,run_date,raw_score,special_item, report_type, uuid from oyi_prod.inventory_dashboard_history \n",
    "        where run_date>= '{from_rundate}' and run_date <= '{to_rundate}'\n",
    "        and display_ind='Display'\n",
    "        \"\"\".format(from_rundate=from_rundate,to_rundate=to_rundate)\n",
    "    invdash = client.query(sql).to_dataframe()\n",
    "    print(invdash.columns)\n",
    "    invdash.run_date = pd.to_datetime(invdash.run_date)\n",
    "    invdash['actual_date']= invdash.run_date+ pd.Timedelta('1 day')\n",
    "    invdash.actual_date= invdash.actual_date.dt.date\n",
    "    invdash= invdash[~invdash.duplicated(['actual_date','club_nbr','old_nbr'],keep= 'first')].copy()\n",
    "    invdash.actual_date = invdash.actual_date.astype(str)\n",
    "    #Removing special items which are always added to the list\n",
    "    invdash = invdash[~(invdash.special_item==1)]\n",
    "    \n",
    "    invdash.to_csv(invdash_output.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(base_image=BASE_IMAGE)\n",
    "def dataprep(\n",
    "        logger_input: Input[Dataset],\n",
    "        inv_input: Input[Dataset],\n",
    "        match_nosales_output: Output[Dataset],\n",
    "        match_cancelled_output: Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    logger = pd.read_csv(logger_input.path)\n",
    "    inv = pd.read_csv(inv_input.path)\n",
    "  \n",
    "    match= pd.merge(left= logger,\n",
    "                    right= inv, \n",
    "                    left_on = ['ds_uuid','club_nbr','item_nbr'],\n",
    "                    right_on= ['uuid','club_nbr','old_nbr'],\n",
    "                    how= 'inner', indicator=True, validate='one_to_one')\n",
    "    match['run_date'] = match['run_date'].astype(str)\n",
    "    match['action']= ~(match.event_note.isin(['No Action Taken, already out for sale','No Action Taken, already OFS']))\n",
    "    match_nosales= match[~ (match.report_type=='C')]\n",
    "    match_cancelled= match[(match.report_type=='C')]\n",
    "  \n",
    "    match_nosales.to_csv(match_nosales_output.path, index=False)\n",
    "    match_cancelled.to_csv(match_cancelled_output.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(base_image=BASE_IMAGE)\n",
    "def get_raw_score_thresholds(\n",
    "        train_input: Input[Dataset],\n",
    "        club_thresh_output: Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "    \n",
    "    club_thresh = {}\n",
    "    club_prec = {}\n",
    "    club_recall = {}\n",
    "    \n",
    "    mins, maxs= {},{}\n",
    "    \n",
    "    train = pd.read_csv(train_input.path)\n",
    "    for club in train.club_nbr.unique():\n",
    "        train_club = train[train.club_nbr==club]\n",
    "        thresholds = np.sort(list(set(np.round(train_club.raw_score.unique(), 4))))\n",
    "\n",
    "        f1_arr = []\n",
    "        prec_arr = []\n",
    "        recall_arr= []\n",
    "        for th in thresholds:\n",
    "            y_pred = list(train_club.raw_score >= th)\n",
    "            y_true = list(train_club.action == True)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1_arr.append(f1)\n",
    "            prec_arr.append(prec)\n",
    "            recall_arr.append(recall)\n",
    "        \n",
    "        club_thresh[club] = thresholds[np.argmax(f1_arr)]\n",
    "#         get the precision and recall associated with the max F1 socre\n",
    "        club_prec[club] = np.round(prec_arr[np.argmax(f1_arr)], 4)\n",
    "        club_recall[club] = np.round(recall_arr[np.argmax(f1_arr)], 4)\n",
    "   \n",
    "    df_club_prec = pd.DataFrame(club_prec.items(),columns = ['club_nbr','precision'])\n",
    "    df_club_recall = pd.DataFrame(club_recall.items(),columns = ['club_nbr','recall'])\n",
    "    df_club_thresh = pd.DataFrame(club_thresh.items(),columns = ['club_nbr','club_thresh'])\n",
    "    df_thresholds = df_club_thresh.merge(df_club_prec, how = 'left', on = 'club_nbr')\\\n",
    "                                      .merge(df_club_recall, how = 'left', on = 'club_nbr')\n",
    "    df_thresholds.to_csv(club_thresh_output.path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def combine_results(\n",
    "        nosales_club_thresholds_input: Input[Dataset],\n",
    "        cancelled_club_thresholds_input: Input[Dataset],\n",
    "        club_thresh_chain_path_input: str,\n",
    "        regularized_club_thresh_chain_output: Output[Dataset],\n",
    "        unregularized_club_thresh_chain_output: Output[Dataset]\n",
    "    ):\n",
    "    import pandas as pd\n",
    "    \n",
    "    nosales_club_thresholds = pd.read_csv(nosales_club_thresholds_input.path)\n",
    "    nosales_club_thresholds = nosales_club_thresholds.rename(columns = {'club_thresh': 'nosales_club_thresh',\n",
    "                                                                        'precision': 'nosales_precision',\n",
    "                                                                        'recall': 'nosales_recall'})\n",
    "    cancelled_club_thresholds = pd.read_csv(cancelled_club_thresholds_input.path)\n",
    "    cancelled_club_thresholds = cancelled_club_thresholds.rename(columns = {'club_thresh': 'cancelled_club_thresh',\n",
    "                                                                        'precision': 'cancelled_precision',\n",
    "                                                                        'recall': 'cancelled_recall'})\n",
    "   \n",
    "    # merge the DFs\n",
    "    df_thresholds = nosales_club_thresholds.merge(cancelled_club_thresholds, how = 'left', on = 'club_nbr')\n",
    "    # Regularize the chosen values by averaging the results with the group mean.\n",
    "    df_thresholds['nosales_club_thresh'] = ((df_thresholds['nosales_club_thresh'] + df_thresholds['nosales_club_thresh'].mean()) / 2).round(4)\n",
    "    df_thresholds['cancelled_club_thresh'] = ((df_thresholds['cancelled_club_thresh'] + df_thresholds['cancelled_club_thresh'].mean()) / 2).round(4)\n",
    "\n",
    "\n",
    "    current_time = pd.datetime.now()\n",
    "    df_thresholds['update_ts'] = current_time\n",
    "    df_thresholds.to_csv(regularized_club_thresh_chain_output.path, index=False)\n",
    "    \n",
    "    df_thresholds_unregularized = nosales_club_thresholds[['club_nbr', 'nosales_club_thresh']].merge(cancelled_club_thresholds[['club_nbr', 'cancelled_club_thresh']],\n",
    "                                                                      how = 'left', on = 'club_nbr')\n",
    "    # old LATEST_CLUB_THRESH_PATH\n",
    "    # unregularized_club_thresh_chain_output.path = f'{LATEST_CLUB_THRESH_PATH}/club_thresh_chain.csv'\n",
    "    # df_thresholds_unregularized.to_csv(f'{LATEST_CLUB_THRESH_PATH}/club_thresh_chain.csv', index=False)\n",
    "    # new\n",
    "    df_thresholds_unregularized.to_csv(unregularized_club_thresh_chain_output.path, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Club-score cutoff\n",
    "# logger = get_logger(from_date=from_date,\n",
    "#                        to_date=to_date,\n",
    "#                        project_id = PROJECT_ID)\n",
    "\n",
    "# invdash = get_inv(from_date=from_date,\n",
    "#                   to_date=to_date,\n",
    "#                   project_id = PROJECT_ID)\n",
    "\n",
    "# match_data = dataprep(logger_input=logger.outputs['df_subset_output'],\n",
    "#                    inv_input=invdash.outputs['invdash_output'])\n",
    "\n",
    "# nosales_result = get_raw_score_thresholds(train_input=match_data.outputs['match_nosales_output'])\n",
    "# cancelled_result = get_raw_score_thresholds(train_input=match_data.outputs['match_cancelled_output'])\n",
    "\n",
    "# club_thresh_chain = combine_results(nosales_club_thresholds_input=nosales_result.outputs['club_thresh_output'],\n",
    "#                                         cancelled_club_thresholds_input=cancelled_result.outputs['club_thresh_output'],\n",
    "#                                         club_thresh_chain_path_input=LATEST_CLUB_THRESH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(logger.outputs[\"df_subset_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(base_image=BASE_IMAGE)\n",
    "# def out(\n",
    "#         logger_input: Input[Dataset],\n",
    "#         inv_input: Input[Dataset],\n",
    "#         out0: Output[Dataset],\n",
    "#         out1: Output[Dataset],\n",
    "#         out2: Output[Dataset]\n",
    "#     ):\n",
    "    \n",
    "#     import pandas as pd\n",
    "#     out0 = logger_input.path\n",
    "#     # print(f\"out0: {out0}\")\n",
    "#     out1 = pd.read_csv(logger_input.path)\n",
    "#     # print(f\"out1: {out1}\")\n",
    "#     out1.head()\n",
    "#     out2 = pd.read_csv(inv_input.path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out(logger_input=logger.outputs['df_subset_output'],inv_input=invdash.outputs['invdash_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.outputs[\"df_subset_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out(logger_input=logger.outputs['df_subset_output'],inv_input=invdash.outputs['invdash_output']).outputs[\"out0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.read_csv(logger.outputs['df_subset_output'])\n",
    "# a\n",
    "# # club_thresh_chain.outputs[\"unregularized_club_thresh_chain_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(packages_to_install=['pandas==1.3.5'])\n",
    "# def create_dataset(iris_dataset: Output[Dataset]):\n",
    "#     import pandas as pd\n",
    "\n",
    "#     csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "#     col_names = [\n",
    "#         'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "#     ]\n",
    "#     df = pd.read_csv(csv_url, names=col_names)\n",
    "\n",
    "#     with open(iris_dataset.path, 'w') as f:\n",
    "#         df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_dataset().outputs[\"iris_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = get_logger(from_date=from_date,\n",
    "#                            to_date=to_date,\n",
    "#                            project_id = PROJECT_ID)\n",
    "# # import pprint\n",
    "# # pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# # pp.pprint(logger.outputs['df_subset_output'])\n",
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `@dsl.pipeline` decorator transforms a Python function into a pipeline that can be executed by KFP back-end. \n",
    "@dsl.pipeline(pipeline_root=NOSALE_PIPELINE_ROOT, name=NOSALE_PIPELINE_NAME)\n",
    "def pipeline():\n",
    "    # Club-score cutoff\n",
    "    logger = get_logger(from_date=from_date,\n",
    "                           to_date=to_date,\n",
    "                           project_id = PROJECT_ID)\n",
    "    \n",
    "    invdash = get_inv(from_date=from_date,\n",
    "                      to_date=to_date,\n",
    "                      project_id = PROJECT_ID)\n",
    "    \n",
    "    match_data = dataprep(logger_input=logger.outputs['df_subset_output'],\n",
    "                       inv_input=invdash.outputs['invdash_output'])\n",
    "    \n",
    "    nosales_result = get_raw_score_thresholds(train_input=match_data.outputs['match_nosales_output'])\n",
    "    cancelled_result = get_raw_score_thresholds(train_input=match_data.outputs['match_cancelled_output'])\n",
    "    \n",
    "    club_thresh_chain = combine_results(nosales_club_thresholds_input=nosales_result.outputs['club_thresh_output'],\n",
    "                                        cancelled_club_thresholds_input=cancelled_result.outputs['club_thresh_output'],\n",
    "                                        club_thresh_chain_path_input=LATEST_CLUB_THRESH_PATH)\n",
    "    \n",
    "    # Training \n",
    "    data = data_preprocessing(training_data_bq_query_input=TRAINING_DATA_BQ_QUERY,\n",
    "                              matcher=MATCHER,\n",
    "                              project_id=PROJECT_ID,\n",
    "                              env=ENV,\n",
    "                              pipeline_root=NOSALE_PIPELINE_ROOT)\n",
    "    \n",
    "    train_test_data = train_test_split(nosales_ext_input=data.outputs['training_data_output'])\n",
    "    \n",
    "    train_eval_data = train_eval_model(nosales_ext_input=data.outputs['training_data_output'],\n",
    "                                       nosales_train_ext_input=train_test_data.outputs['nosales_train_ext_output'],\n",
    "                                       nosales_test_ext_input=train_test_data.outputs['nosales_test_ext_output'],\n",
    "                                       nosales_train_usampled_input=train_test_data.outputs['nosales_train_usampled_output'],\n",
    "                                       mode=args.MODE,\n",
    "                                       stage1_flag=args.STAGE1_FLAG,\n",
    "                                       ensemble_flag=args.ENSEMBLE_FLAG,\n",
    "                                       rf_clf_model_path_input=args.RF_CLF_MODEL_PATH,\n",
    "                                       logistic_clf_model_path_input=args.LOGISTIC_CLF_MODEL_PATH,\n",
    "                                       stage1_nn_model_path_input=args.STAGE1_NN_MODEL_PATH,\n",
    "                                       gnb_model_path_input=args.GNB_MODEL_PATH,\n",
    "                                       stg1_feature_selector_model_path_input=args.STG1_FEATURE_SELECTOR_MODEL_PATH,\n",
    "                                       nosales_model_path_input=args.NOSALES_MODEL_PATH,\n",
    "                                       latest_nosales_model_path_input=LATEST_NOSALES_PATH,\n",
    "                                       project_id=PROJECT_ID,\n",
    "                                       region=REGION,\n",
    "                                       timestamp=TIMESTAMP)\n",
    "   \n",
    "    updated_thresholds = update_thresholds(nosales_test_ext_input=train_eval_data.outputs['nosales_test_ext_output'],  \n",
    "                                           club_thresh_ext_input=club_thresh_chain.outputs[\"unregularized_club_thresh_chain_output\"], \n",
    "                                           club_thresh_path_input=LATEST_CLUB_THRESH_PATH,\n",
    "                                           nosales_model_input=train_eval_data.outputs['nosales_model_output'])\n",
    "\n",
    "    element_model_registry = CustomTrainingJobOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        network=\"projects/12856960411/global/networks/vpcnet-shared-prod-01\",\n",
    "        reserved_ip_ranges=[\"vpcnet-shared-prod-01-datafusion-01\"],\n",
    "\n",
    "        display_name=\"mlflow-model-registry\",\n",
    "\n",
    "        worker_pool_specs=[{\n",
    "            \"replica_count\": 1,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_count\": 0,\n",
    "            },\n",
    "            # The below dictionary specifies:\n",
    "            #   1. The URI of the custom image to run this CustomTrainingJobOp against\n",
    "            #      - this image is built from ../../custom_image_builds/model_registry_image_build.ipynb\n",
    "            #   2. The command to run against that image\n",
    "            #   3. The arguments to supply to that custom image \n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": MLFLOW_IMAGE,\n",
    "                \"command\": [\n",
    "                    \"python3\", \"nosales_model_registry.py\"\n",
    "                ],\n",
    "                \"args\": [\n",
    "                    \"--GCS_MODEL_PATH\", LATEST_NOSALES_PATH,\n",
    "                    \"--MLFLOW_EXP_NAME\", MLFLOW_EXP_NAME,\n",
    "                    \"--MODEL_REGISTRY_NAME\", MODEL_REGISTRY_NAME\n",
    "                ],\n",
    "            },\n",
    "        }],\n",
    "\n",
    "    ).set_display_name(\"element-mlflow-model-registry\")\n",
    "    element_model_registry.after(train_eval_data)\n",
    "    \n",
    "    \n",
    "#     element_model_registry = CustomTrainingJobOp(\n",
    "#         project=PROJECT_ID,\n",
    "#         location=REGION,\n",
    "#         service_account=SERVICE_ACCOUNT,\n",
    "#         network=\"projects/12856960411/global/networks/vpcnet-shared-prod-01\",\n",
    "#         reserved_ip_ranges=[\"vpcnet-shared-prod-01-datafusion-01\"],\n",
    "\n",
    "#         display_name=\"mlflow-model-registry\",\n",
    "\n",
    "#         worker_pool_specs=[{\n",
    "#             \"replica_count\": 1,\n",
    "#             \"machine_spec\": {\n",
    "#                 \"machine_type\": \"n1-standard-4\",\n",
    "#                 \"accelerator_count\": 0,\n",
    "#             },\n",
    "#             # The below dictionary specifies:\n",
    "#             #   1. The URI of the custom image to run this CustomTrainingJobOp against\n",
    "#             #      - this image is built from ../../custom_image_builds/model_registry_image_build.ipynb\n",
    "#             #   2. The command to run against that image\n",
    "#             #   3. The arguments to supply to that custom image \n",
    "#             \"container_spec\": {\n",
    "#                 \"image_uri\": MLFLOW_IMAGE,\n",
    "#                 \"command\": [\n",
    "#                     \"python3\", \"cancelled_model_registry.py\"\n",
    "#                 ],\n",
    "#                 \"args\": [\n",
    "#                     \"--GCS_MODEL_PATH\", CANCELLED_MODEL_PATH,\n",
    "#                     \"--MLFLOW_EXP_NAME\", MLFLOW_EXP_NAME,\n",
    "#                     \"--MODEL_REGISTRY_NAME\", MODEL_REGISTRY_NAME,\n",
    "#                 ],\n",
    "#             },\n",
    "#         }],\n",
    "\n",
    "#     ).set_display_name(\"element-mlflow-model-registry\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=TMP_NOSALE_JSON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{NOSALE_PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "    template_path=TMP_NOSALE_JSON,\n",
    "    pipeline_root=NOSALE_PIPELINE_ROOT,\n",
    "    parameter_values={},\n",
    "    enable_caching=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/oyi-nosales-model-pipeline-dev.json\n",
      "contents /tmp/oyi-nosales-model-pipeline-dev.json uploaded to gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_nosales_model_output_dev.json.\n"
     ]
    }
   ],
   "source": [
    "pipeline_utils.store_pipeline(\n",
    "    storage_path=LATEST_NOSALES_PATH_JSON, \n",
    "    filename=TMP_NOSALE_JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/335163835346/locations/us-central1/pipelineJobs/oyi-nosales-model-pipeline-dev-20221013052512\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/335163835346/locations/us-central1/pipelineJobs/oyi-nosales-model-pipeline-dev-20221013052512')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/oyi-nosales-model-pipeline-dev-20221013052512?project=335163835346\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.submit(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "train_model",
   "notebookOrigID": 2093267122234119,
   "widgets": {}
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
