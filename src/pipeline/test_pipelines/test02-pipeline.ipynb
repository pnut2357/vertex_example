{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cebf5f52-1106-4a6d-afa8-4ae9bef7590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV: dev \n",
      "PROJECT_ID: wmt-mlp-p-oyi-ds-or-oyi-dsns \n",
      "BASE_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest \n",
      "MLFLOW_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest \n",
      "PIPELINE_NAME: test \n",
      "PIPELINE_JSON: test.json\n",
      "\n",
      "TMP_PIPELINE_JSON: /tmp/test.json \n",
      "\n",
      "\n",
      "PIPELINE_ROOT: gs://oyi-ds-vertex-test-input-bucket \n",
      "CLUB_THRESH_PATH: gs://oyi-ds-vertex-test-input-bucket/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --MODE MODE --STAGE1_FLAG STAGE1_FLAG\n",
      "                             --ENSEMBLE_FLAG ENSEMBLE_FLAG --RF_CLF_MODEL_PATH\n",
      "                             RF_CLF_MODEL_PATH --LOGISTIC_CLF_MODEL_PATH\n",
      "                             LOGISTIC_CLF_MODEL_PATH --STAGE1_NN_MODEL_PATH\n",
      "                             STAGE1_NN_MODEL_PATH --GNB_MODEL_PATH\n",
      "                             GNB_MODEL_PATH --STG1_FEATURE_SELECTOR_MODEL_PATH\n",
      "                             STG1_FEATURE_SELECTOR_MODEL_PATH\n",
      "                             --NOSALES_MODEL_PATH NOSALES_MODEL_PATH\n",
      "ipykernel_launcher.py: error: the following arguments are required: --MODE, --STAGE1_FLAG, --ENSEMBLE_FLAG, --RF_CLF_MODEL_PATH, --LOGISTIC_CLF_MODEL_PATH, --STAGE1_NN_MODEL_PATH, --GNB_MODEL_PATH, --STG1_FEATURE_SELECTOR_MODEL_PATH, --NOSALES_MODEL_PATH\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Imports for vertex pipeline\n",
    "from google.cloud import aiplatform\n",
    "import google_cloud_pipeline_components\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    component,\n",
    "    pipeline,\n",
    "    Input,\n",
    "    Output,\n",
    "    Model,\n",
    "    Dataset,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    ")\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(str(Path(\".\").absolute().parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent) + \"/utils\")\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent.parent))\n",
    "\n",
    "import pipeline_utils\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    args = pipeline_utils.get_args()\n",
    "except:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--MODE\", required=True, type=str)\n",
    "    parser.add_argument(\"--STAGE1_FLAG\", required=True, type=str)\n",
    "    parser.add_argument(\"--ENSEMBLE_FLAG\", required=True, type=str)\n",
    "    parser.add_argument(\"--RF_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--LOGISTIC_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--STAGE1_NN_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--GNB_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--STG1_FEATURE_SELECTOR_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--NOSALES_MODEL_PATH\", required=True, type=str)\n",
    "    sys.args = [\n",
    "        \"--MODE\", \"test\",\n",
    "        \"--STAGE1_FLAG\", \"train\",\n",
    "        \"--ENSEMBLE_FLAG\", \"train\",\n",
    "        \"--RF_CLF_MODEL_PATH\", \"\",\n",
    "        \"--LOGISTIC_CLF_MODEL_PATH\", \"\",\n",
    "        \"--STAGE1_NN_MODEL_PATH\", \"\",\n",
    "        \"--GNB_MODEL_PATH\", \"\",\n",
    "        \"--STG1_FEATURE_SELECTOR_MODEL_PATH\", \"\",\n",
    "        \"--NOSALES_MODEL_PATH\", \"\",\n",
    "    ]\n",
    "    args = parser.parse_args(sys.args)\n",
    "\n",
    "PARAMS = pipeline_utils.yaml_import('settings.yml')\n",
    "\n",
    "# Env flag for indentifying what env is used. \n",
    "ENV = PARAMS['env_flag']\n",
    "\n",
    "# GCP Project id, service account, region, and docker images. \n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "SERVICE_ACCOUNT = PARAMS['envs'][ENV]['SERVICE_ACCOUNT']\n",
    "REGION = PARAMS['envs'][ENV]['REGION']\n",
    "BASE_IMAGE = PARAMS['envs'][ENV]['BASE_IMAGE']\n",
    "MLFLOW_IMAGE = PARAMS['envs'][ENV]['MLFLOW_IMAGE']\n",
    "\n",
    "# Training Pipeline.\n",
    "RUN_PIPELINE = PARAMS['envs'][ENV]['RUN_PIPELINE']\n",
    "PIPELINE_ROOT = \"gs://oyi-ds-vertex-test-input-bucket\" #PARAMS['envs'][ENV]['PIPELINE_ROOT']\n",
    "PIPELINE_NAME = \"test\" #PARAMS['envs'][ENV]['PIPELINE_NAME']\n",
    "PIPELINE_JSON = PIPELINE_NAME + \".json\" # PARAMS['envs'][ENV]['PIPELINE_JSON']\n",
    "TMP_PIPELINE_JSON = os.path.join(\"/tmp\", PIPELINE_JSON)\n",
    "# LATEST_PIPELINE_PATH = PARAMS['envs'][ENV]['LATEST_NOSALES_MODEL_PATH']\n",
    "# LATEST_PIPELINE_PATH_JSON = LATEST_PIPELINE_PATH + \".json\"\n",
    "# LATEST_NOSALES_MODEL_PATH = LATEST_PIPELINE_PATH + \".json\"\n",
    "\n",
    "\n",
    "TRAINING_TABLE_NAME = PARAMS['envs'][ENV]['TRAINING_TABLE_NAME']\n",
    "TRAINING_DATA_BQ_QUERY = f'select * from {TRAINING_TABLE_NAME}'\n",
    "\n",
    "MLFLOW_EXP_NAME = PARAMS['envs'][ENV]['MLFLOW_EXP_NAME']\n",
    "MODEL_REGISTRY_NAME = PARAMS['envs'][ENV]['MODEL_REGISTRY_NAME']\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    " \n",
    "# Matches on non-word, non-regular-punctuation characters.\n",
    "MATCHER = r\"\"\"[^a-zA-Z0-9'\"!@#$%\\^&*()\\[\\]{}:;<>?,.-=_+ ]+\"\"\" \n",
    "\n",
    "CLUB_THRESH_PATH = f\"{PIPELINE_ROOT}/test\" # PARAMS['envs'][ENV]['CLUB_THRESH_PATH']\n",
    "\n",
    "print(f\"ENV: {ENV} \\nPROJECT_ID: {PROJECT_ID} \\nBASE_IMAGE: {BASE_IMAGE} \\nMLFLOW_IMAGE: {MLFLOW_IMAGE} \\nPIPELINE_NAME: {PIPELINE_NAME} \\nPIPELINE_JSON: {PIPELINE_JSON}\")\n",
    "print(f\"\\nTMP_PIPELINE_JSON: {TMP_PIPELINE_JSON} \") #\\nLATEST_PIPELINE_PATH: {LATEST_PIPELINE_PATH} \\nLATEST_PIPELINE_PATH_JSON: {LATEST_PIPELINE_PATH_JSON}\")\n",
    "print(f\"\\n\\nPIPELINE_ROOT: {PIPELINE_ROOT} \\nCLUB_THRESH_PATH: {CLUB_THRESH_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba11657d-c986-457c-978e-362ae93dd8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oyi_prod.oyi_train_no_testscan'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_TABLE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b61e3463-2573-4ae8-9b4e-70bfa52ac9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://oyi-ds-vertex-test-input-bucket/test_tpr.joblib'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(PIPELINE_ROOT, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5af5f4f2-dc87-41dc-b5a3-a95105fca531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://oyi-ds-vertex-test-input-bucket/test_tpr.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "import os\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "data = {\n",
    "  \"calories\": [420, 380, 390],\n",
    "  \"duration\": [50, 40, 45]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# local_path\n",
    "# to_dump = joined.loc[joined['run_date']==joined.run_date.max()]\n",
    "# to_dump = to_dump[select_col]\n",
    "path = ''\n",
    "level_name = \"test\"\n",
    "local_path = level_name + '_tpr.joblib'\n",
    "local_path_csv = level_name + '_tpr.csv'\n",
    "csv_file_path = f\"{PIPELINE_ROOT}/test_tpr_{ENV}\"\n",
    "dump(df, local_path)\n",
    "storage_path = os.path.join(PIPELINE_ROOT, local_path)\n",
    "print(storage_path)\n",
    "blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b311661-4656-4f29-b668-c95013da1854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test_tpr.joblib',\n",
       " 'gs://oyi-ds-vertex-test-input-bucket/test_tpr_dev/test_tpr.csv')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path, f\"{csv_file_path}/{local_path_csv}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe88b94c-395b-4d09-a04a-16921785fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.upload_from_filename(local_path)\n",
    "df.to_csv(f\"{csv_file_path}/{local_path_csv}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb36d34a-1667-48ed-bf3a-dcfdbf282c1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32328/4193692933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_dataframe'"
     ]
    }
   ],
   "source": [
    "data.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c03ab18-2f5f-4e4d-aab8-79d82163c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(base_image=BASE_IMAGE)\n",
    "def test1(out: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    \n",
    "    data = {\n",
    "      \"calories\": [420, 380, 390],\n",
    "      \"duration\": [50, 40, 45]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(out.path)\n",
    "@component(base_image=BASE_IMAGE)\n",
    "def test_save(pipeline_root: str, env: str):\n",
    "    import pandas as pd\n",
    "    from joblib import dump, load\n",
    "    import os\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    data = {\n",
    "      \"calories\": [420, 380, 390],\n",
    "      \"duration\": [50, 40, 45]\n",
    "    }\n",
    "    # local_path\n",
    "    # to_dump = joined.loc[joined['run_date']==joined.run_date.max()]\n",
    "    # to_dump = to_dump[select_col]\n",
    "    path = f\"{pipeline_root}/test_all_level_tpr_{env}\"\n",
    "    level_name = \"test\"\n",
    "    local_path = level_name + '_tpr.joblib'\n",
    "    local_path_csv = level_name + '_tpr.csv'\n",
    "    csv_file_path = f\"{pipeline_root}/test_tpr_{env}\"\n",
    "    dump(data, local_path)\n",
    "    storage_path = os.path.join(path, local_path)\n",
    "    blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "    blob.upload_from_filename(local_path)\n",
    "    \n",
    "    data.to_csv(f\"{csv_file_path}/{local_path_csv}\", index=False)\n",
    "        \n",
    "@component(base_image=BASE_IMAGE, packages_to_install=[\"google_cloud_storage\"])\n",
    "def data_preprocessing(\n",
    "        training_data_bq_query_input: str,\n",
    "        matcher: str,\n",
    "        project_id: str,\n",
    "        env: str,\n",
    "        pipeline_root: str,\n",
    "        training_data_output: Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    import utils\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    data = client.query(training_data_bq_query_input).to_dataframe()\n",
    "    nosales_data = data[\n",
    "      (data.report_type!='C') &\n",
    "      (data.display_ind == \"Display\") &\n",
    "      (data.oh_qty>=0)]\n",
    "    nosales_data[\"item_desc\"] = nosales_data['item_desc'].str.replace(matcher, \"\", regex=True)\n",
    "    nosales_data['run_date'] = pd.to_datetime(nosales_data['run_date'])\n",
    "    max_date = nosales_data['run_date'].max()\n",
    "    cutoff_date = (max_date - timedelta(days=182)).strftime('%Y-%m-%d')\n",
    "    nosales_data = nosales_data[nosales_data.run_date > cutoff_date]\n",
    "    \n",
    "    nosales_data.replace(\"No Action Taken, already OFS\", \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.replace('Updated the NOSALES type with scrubber event', \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.sort_values(by = ['run_date','club_nbr','item_nbr','event_ts'],inplace = True)\n",
    "    nosales_data.drop_duplicates(['old_nbr','club_nbr','run_date'], keep='first',inplace = True)\n",
    "    \n",
    "    nosales_ext = utils.calculate_all_level_tpr(nosales_data, env, pipeline_root, save=False)\n",
    "    nosales_ext.fillna(0, inplace=True)\n",
    "    nosales_ext.to_csv(training_data_output.path, index=False)\n",
    "\n",
    "@component(base_image=BASE_IMAGE)\n",
    "def test(training_data_bq_query_input: str,\n",
    "         matcher: str,\n",
    "         project_id: str,\n",
    "         env: str,\n",
    "         pipeline_root: str,\n",
    "         training_data_output: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    import utils\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    client = bigquery.Client(project=project_id)\n",
    "    data = client.query(training_data_bq_query_input).to_dataframe()\n",
    "    nosales_data = data[\n",
    "      (data.report_type!='C') &\n",
    "      (data.display_ind == \"Display\") &\n",
    "      (data.oh_qty>=0)]\n",
    "    nosales_data[\"item_desc\"] = nosales_data['item_desc'].str.replace(matcher, \"\", regex=True)\n",
    "    nosales_data['run_date'] = pd.to_datetime(nosales_data['run_date'])\n",
    "    max_date = nosales_data['run_date'].max()\n",
    "    cutoff_date = (max_date - timedelta(days=182)).strftime('%Y-%m-%d')\n",
    "    nosales_data = nosales_data[nosales_data.run_date > cutoff_date]\n",
    "    \n",
    "    nosales_data.replace(\"No Action Taken, already OFS\", \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.replace('Updated the NOSALES type with scrubber event', \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.sort_values(by = ['run_date','club_nbr','item_nbr','event_ts'],inplace = True)\n",
    "    nosales_data.drop_duplicates(['old_nbr','club_nbr','run_date'], keep='first',inplace = True)\n",
    "    \n",
    "    nosales_ext = utils.calculate_all_level_tpr(nosales_data, env, pipeline_root, save=False)\n",
    "    nosales_ext.fillna(0, inplace=True)\n",
    "    nosales_ext.to_csv(training_data_output.path, index=False)\n",
    "    \n",
    "@component(base_image=BASE_IMAGE)\n",
    "def test_upload(training_data_bq_query_input: str,\n",
    "                matcher: str,\n",
    "                project_id: str,\n",
    "                env: str,\n",
    "                pipeline_root: str,\n",
    "                path_output: Output[Dataset]):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    import utils\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    data = client.query(training_data_bq_query_input).to_dataframe()\n",
    "    nosales_data = data[\n",
    "      (data.report_type!='C') &\n",
    "      (data.display_ind == \"Display\") &\n",
    "      (data.oh_qty>=0)]\n",
    "    nosales_data[\"item_desc\"] = nosales_data['item_desc'].str.replace(matcher, \"\", regex=True)\n",
    "    nosales_data['run_date'] = pd.to_datetime(nosales_data['run_date'])\n",
    "    max_date = nosales_data['run_date'].max()\n",
    "    cutoff_date = (max_date - timedelta(days=182)).strftime('%Y-%m-%d')\n",
    "    nosales_data = nosales_data[nosales_data.run_date > cutoff_date]\n",
    "    \n",
    "    nosales_data.replace(\"No Action Taken, already OFS\", \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.replace('Updated the NOSALES type with scrubber event', \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.sort_values(by = ['run_date','club_nbr','item_nbr','event_ts'],inplace = True)\n",
    "    nosales_data.drop_duplicates(['old_nbr','club_nbr','run_date'], keep='first',inplace = True)\n",
    "    \n",
    "    nosales_ext = utils.calculate_all_level_tpr(nosales_data, env, pipeline_root, save=False)\n",
    "    nosales_ext.fillna(0, inplace=True)\n",
    "    nosales_ext.to_csv(path_output.path, index=False)\n",
    "    \n",
    "    \n",
    "    # path_input_csv = os.path.join(path_input, \"test.csv\")\n",
    "    # df_test = pd.read_csv(path_input_csv).drop(columns = 'nosales_club_thresh')\n",
    "    # # club_threshold_output.path = path_input_csv\n",
    "    # df_test.to_csv(club_threshold_output.path, index=False)\n",
    "    \n",
    "\n",
    "# @component(base_image=IMAGE_URI)\n",
    "# def update_thresholds(\n",
    "#     nosales_test_ext_input: Input[Dataset],\n",
    "#     models_dir_path_input: str,\n",
    "#     nosales_model_input: Input[Model],\n",
    "#     club_threshold_outout: Output[Artifact]\n",
    "# ):\n",
    "    \n",
    "#     import utils\n",
    "#     import pandas as pd\n",
    "#     from joblib import load, dump\n",
    "#     import os\n",
    "#     from google.cloud import storage\n",
    "#     from tempfile import TemporaryFile\n",
    "    \n",
    "#     nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "#     nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "#     blob = storage.blob.Blob.from_string(nosales_model_input.path, client=storage.Client())\n",
    "#     with TemporaryFile() as temp_file:\n",
    "#         #download blob into temp file\n",
    "#         blob.download_to_file(temp_file)\n",
    "#         temp_file.seek(0)\n",
    "#         #load into joblib\n",
    "#         stack_pipeline=load(temp_file)\n",
    "    \n",
    "#     thresh = utils.gen_thresholds(df = nosales_test_ext,  predictions = stack_pipeline.predict_proba(X=nosales_test_ext), classes = stack_pipeline.classes_)\n",
    " \n",
    "#     club_threshold_file_path = os.path.join(models_dir_path_input, \"club_thresh_chain.joblib\")\n",
    "#     blob = storage.blob.Blob.from_string(club_threshold_file_path, client=storage.Client())\n",
    "#     with TemporaryFile() as temp_file:\n",
    "#         #download blob into temp file\n",
    "#         blob.download_to_file(temp_file)\n",
    "#         temp_file.seek(0)\n",
    "#         #load into joblib\n",
    "#         all_thresh=load(temp_file)\n",
    "    \n",
    "#     all_thresh['nosales_club_thresh'] = thresh\n",
    "#     dump(all_thresh, \"club_thresh_chain.joblib\")\n",
    "    \n",
    "#     blob = storage.blob.Blob.from_string(club_threshold_output.path, client=storage.Client())\n",
    "#     blob.upload_from_filename(\"club_thresh_chain.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbaa8e2-39ac-4ecf-a05c-86f9628bf19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(base_image=BASE_IMAGE)\n",
    "def update_thresholds(\n",
    "        nosales_test_ext_input: Input[Dataset],\n",
    "        club_thresh_path_input: str,\n",
    "        nosales_model_input: Input[Model],\n",
    "        club_threshold_output: Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    import utils\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    from google.cloud import storage\n",
    "    from tempfile import TemporaryFile\n",
    "    \n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "    nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "   \n",
    "    with open(nosales_model_input.path, \"rb\") as handler:\n",
    "        stack_pipeline = pickle.load(handler)\n",
    "    \n",
    "    nosales_thresh = utils.gen_thresholds(df = nosales_test_ext,  predictions = stack_pipeline.predict_proba(X=nosales_test_ext), classes = stack_pipeline.classes_)\n",
    "    df_nosales_thresh = pd.DataFrame(nosales_thresh.items(), columns = ['club_nbr','nosales_club_thresh']) # nosales_club_thresh\n",
    "    \n",
    "    # club_threshold_file_path = os.path.join(club_thresh_path_input, \"club_thresh_chain.csv\")\n",
    "    # df_cancelled_thresh = pd.read_csv(club_threshold_file_path).drop(columns = 'nosales_club_thresh')\n",
    "    # all_thresh = df_cancelled_thresh.merge(df_nosales_thresh, how = 'left', on = 'club_nbr')\n",
    "    # club_threshold_output.path = club_threshold_file_path\n",
    "    # all_thresh.to_csv(club_threshold_file_path, index = False)\n",
    "    \n",
    "    club_threshold_file_path = os.path.join(club_thresh_path_input, \"club_thresh_chain.csv\")\n",
    "    df_cancelled_thresh = pd.read_csv(club_threshold_file_path).drop(columns = 'nosales_club_thresh')\n",
    "    all_thresh = df_cancelled_thresh.merge(df_nosales_thresh, how = 'left', on = 'club_nbr')\n",
    "    all_thresh.to_csv(club_threshold_output.path, index = False)\n",
    "    # df_output.to_csv(arg_path.path, index=False)\n",
    "\n",
    "data = data_preprocessing(training_data_bq_query_input = TRAINING_DATA_BQ_QUERY,\n",
    "                              matcher=MATCHER,\n",
    "                              project_id = PROJECT_ID, \n",
    "                              env=ENV, \n",
    "                              pipeline_root=PIPELINE_ROOT)\n",
    "    \n",
    "train_test_data = train_test_split(nosales_ext_input=data.outputs['training_data_output'])\n",
    "\n",
    "train_eval_data = train_eval_model(nosales_ext_input=data.outputs['training_data_output'],\n",
    "                                   nosales_train_ext_input=train_test_data.outputs['nosales_train_ext_output'],\n",
    "                                   nosales_test_ext_input=train_test_data.outputs['nosales_test_ext_output'],\n",
    "                                   nosales_train_usampled_input=train_test_data.outputs['nosales_train_usampled_output'],\n",
    "                                   mode=args.MODE,\n",
    "                                   stage1_flag=args.STAGE1_FLAG,\n",
    "                                   ensemble_flag=args.ENSEMBLE_FLAG,\n",
    "                                   rf_clf_model_path_input=args.RF_CLF_MODEL_PATH,\n",
    "                                   logistic_clf_model_path_input=args.LOGISTIC_CLF_MODEL_PATH,\n",
    "                                   stage1_nn_model_path_input=args.STAGE1_NN_MODEL_PATH,\n",
    "                                   gnb_model_path_input=args.GNB_MODEL_PATH,\n",
    "                                   stg1_feature_selector_model_path_input=args.STG1_FEATURE_SELECTOR_MODEL_PATH,\n",
    "                                   nosales_model_path_input=args.NOSALES_MODEL_PATH,\n",
    "                                   latest_pipeline_path_input=LATEST_PIPELINE_PATH,\n",
    "                                   project_id=PROJECT_ID,\n",
    "                                   region=REGION,\n",
    "                                   timestamp=TIMESTAMP)\n",
    "\n",
    "updated_thresholds = update_thresholds(nosales_test_ext_input=train_eval_data.outputs['nosales_test_ext_output'],  \n",
    "                                       club_thresh_path_input=CLUB_THRESH_PATH,\n",
    "                                       nosales_model_input=train_eval_data.outputs['nosales_model_output'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2708607-c35d-4431-bad4-3037a800b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_sale = pd.read_csv(os.path.join(PARAMS['envs'][ENV]['CLUB_THRESH_PATH'], \"club_thresh_chain.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced922f-cda9-49c4-a987-c589bb6de2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nosales_test_ext_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/nosales_test_ext_output\"\n",
    "nosales_test_ext = pd.read_csv(nosales_test_ext_path)\n",
    "nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18b6dc-6ed1-4fc9-9c5f-f497463ce865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/nosales_model_output\"\n",
    "model_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/nosales_model_output\"\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\"feature file not found: {0}\".format(model_path))\n",
    "\n",
    "with open(model_path, 'rb') as file:\n",
    "    features = pickle.load(file)\n",
    "    # return features\n",
    "# with open(model_path, \"rb\") as handler:\n",
    "#     stack_pipeline = pickle.load(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde4b93-e25a-4942-9bee-a6005a0e5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nosales_thresh = utils.gen_thresholds(df = nosales_test_ext,  predictions = stack_pipeline.predict_proba(X=nosales_test_ext), classes = stack_pipeline.classes_)\n",
    "#     df_nosales_thresh = pd.DataFrame(nosales_thresh.items(), columns = ['club_nbr','nosales_club_thresh']) # nosales_club_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3facab51-ddcb-4186-9545-7f467ade51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "with open(nosales_model_input.path, \"rb\") as handler:\n",
    "    stack_pipeline = pickle.load(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd85bbb-4f10-4b11-b738-4dad982c83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ce872-bd6f-4e64-8437-f81d23b2737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7386c3-39a5-451a-818d-eb689698377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f103b68-46a2-4fe7-ba10-2b7779a16afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUB_THRESH_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdede18-b254-4a11-8481-05ed3c89b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77cdc3-d95d-46f9-a8bc-cda46b086979",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUB_THRESH_PATH,PARAMS['envs'][ENV]['CLUB_THRESH_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a5133-c740-4ffc-84bd-163465b6fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# path = CLUB_THRESH_PATH\n",
    "file_path = os.path.join(PARAMS['envs'][ENV]['CLUB_THRESH_PATH'], \"club_thresh_chain.csv\")\n",
    "df_no_sale = pd.read_csv(file_path)\n",
    "print(df_no_sale.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22505216-1f54-4e6d-ae98-ea4c10e84b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ba5e3-afcb-400f-b6a8-1f97ab1fe97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUB_THRESH_PATH\n",
    "# oyi-ds-vertex-test-input-bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15871fd2-5565-4ce7-8fa1-8394da315750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# club_threshold_file_path = os.path.join(club_thresh_path_input, \"club_thresh_chain.csv\")\n",
    "# df_cancelled_thresh = pd.read_csv(club_threshold_file_path).drop(columns = 'nosales_club_thresh')\n",
    "# all_thresh = df_cancelled_thresh.merge(df_nosales_thresh, how = 'left', on = 'club_nbr')\n",
    "# club_threshold_output.path = club_threshold_file_path\n",
    "# all_thresh.to_csv(club_threshold_file_path, index = False)\n",
    "\n",
    "club_threshold_file_path = os.path.join(club_thresh_path_input, \"club_thresh_chain.csv\")\n",
    "# df_cancelled_thresh = pd.read_csv(club_threshold_file_path).drop(columns = 'nosales_club_thresh')\n",
    "all_thresh = df_cancelled_thresh.merge(df_nosales_thresh, how = 'left', on = 'club_nbr')\n",
    "all_thresh.to_csv(club_threshold_output.path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832975f-353a-48c3-9e32-376596561cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_input_csv = os.path.join(CLUB_THRESH_PATH, \"club_thresh_chain.csv\")\n",
    "# # print(path_input_csv)\n",
    "# df_test = pd.read_csv(path_input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe04230-6d6a-460c-ad8c-52bdfddf40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check(training_data_bq_query_input: str,\n",
    "#           matcher: str,\n",
    "#           project_id: str,\n",
    "#           env: str,\n",
    "#           pipeline_root: str,\n",
    "#           path_output: Output[Dataset]):\n",
    "#     import os\n",
    "#     import pandas as pd\n",
    "    \n",
    "#     path_input_csv = os.path.join(path_input, \"test.csv\")\n",
    "#     df_test = pd.read_csv(path_input_csv)#.drop(columns = 'nosales_club_thresh')\n",
    "#     print(df_test)\n",
    "# check(path_input=CLUB_THRESH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7485a30-805e-440f-a2bb-c82b5e51f7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d455e-6630-4bed-a32f-ed7f9805526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUB_THRESH_PATH, PIPELINE_ROOT, PIPELINE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec18ac8-1952-46c6-ae12-be8e0a8d206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=PIPELINE_NAME)\n",
    "# def pipeline():\n",
    "#     data = data_preprocessing(training_data_bq_query_input = TRAINING_DATA_BQ_QUERY,\n",
    "#                               matcher=MATCHER,\n",
    "#                               project_id = PROJECT_ID, \n",
    "#                               env=ENV, \n",
    "#                               pipeline_root=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de3f18e-8d5e-49a9-b158-0185265291f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://oyi-ds-vertex-test-input-bucket'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca42d79a-4128-4abb-a608-b0558f1f2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=PIPELINE_NAME)\n",
    "def pipeline():\n",
    "    # test1_job = test1()\n",
    "    test_save_job = test_save(pipeline_root=PIPELINE_ROOT, env=ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "109f0117-9731-494b-88fa-34c410279902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=PIPELINE_NAME)\n",
    "# def pipeline():\n",
    "#     test1_job = test1()\n",
    "    # test_job = test(training_data_bq_query_input=TRAINING_DATA_BQ_QUERY,\n",
    "    #                           matcher=MATCHER,\n",
    "    #                           project_id=PROJECT_ID, \n",
    "    #                           env=ENV, \n",
    "    #                           pipeline_root=PIPELINE_ROOT)\n",
    "    \n",
    "    # test_upload_job = test_upload(path_input=CLUB_THRESH_PATH)\n",
    "    \n",
    "#     train_test_data = train_test_split(nosales_ext_input=data.outputs['training_data_output'])\n",
    "    \n",
    "#     train_eval_data = train_eval_model(nosales_ext_input=data.outputs['training_data_output'],\n",
    "#                                        nosales_train_ext_input=train_test_data.outputs['nosales_train_ext_output'],\n",
    "#                                        nosales_test_ext_input=train_test_data.outputs['nosales_test_ext_output'],\n",
    "#                                        nosales_train_usampled_input=train_test_data.outputs['nosales_train_usampled_output'],\n",
    "#                                        mode=args.MODE,\n",
    "#                                        stage1_flag=args.STAGE1_FLAG,\n",
    "#                                        ensemble_flag=args.ENSEMBLE_FLAG,\n",
    "#                                        rf_clf_model_path_input=args.RF_CLF_MODEL_PATH,\n",
    "#                                        logistic_clf_model_path_input=args.LOGISTIC_CLF_MODEL_PATH,\n",
    "#                                        stage1_nn_model_path_input=args.STAGE1_NN_MODEL_PATH,\n",
    "#                                        gnb_model_path_input=args.GNB_MODEL_PATH,\n",
    "#                                        stg1_feature_selector_model_path_input=args.STG1_FEATURE_SELECTOR_MODEL_PATH,\n",
    "#                                        nosales_model_path_input=args.NOSALES_MODEL_PATH,\n",
    "#                                        latest_pipeline_path_input=LATEST_PIPELINE_PATH,\n",
    "#                                        project_id=PROJECT_ID,\n",
    "#                                        region=REGION,\n",
    "#                                        timestamp=TIMESTAMP)\n",
    "   \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed3fac5b-4ae7-4c3f-8925-6356d52ca8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/test.json'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TMP_PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dfe1432-b06b-44ab-ab31-21d428c86fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=TMP_PIPELINE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "835429be-0621-4173-8328-e33adb8b7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "    template_path=TMP_PIPELINE_JSON,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={},\n",
    "    # labels={\"user\":\"Jae\"},\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# PipelineJob(\n",
    "#     display_name: str,\n",
    "#     template_path: str,\n",
    "#     job_id: Optional[str] = None,\n",
    "#     pipeline_root: Optional[str] = None,\n",
    "#     parameter_values: Optional[Dict[str, Any]] = None,\n",
    "#     input_artifacts: Optional[Dict[str, str]] = None,\n",
    "#     enable_caching: Optional[bool] = None,\n",
    "#     encryption_spec_key_name: Optional[str] = None,\n",
    "#     labels: Optional[Dict[str, str]] = None,\n",
    "#     credentials: Optional[google.auth.credentials.Credentials] = None,\n",
    "#     project: Optional[str] = None,\n",
    "#     location: Optional[str] = None,\n",
    "#     failure_policy: Optional[str] = None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c7611f1-878d-4f0c-87a7-61f02114364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/335163835346/locations/us-central1/pipelineJobs/test-20221029182856\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/335163835346/locations/us-central1/pipelineJobs/test-20221029182856')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/test-20221029182856?project=335163835346\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.submit(service_account=SERVICE_ACCOUNT,network='projects/12856960411/global/networks/vpcnet-private-svc-access-usc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5c311-c27e-4d02-bbbe-ccc6327afb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################\n",
    "\n",
    "# pipeline_job = aiplatform.PipelineJob(\n",
    "#     display_name=f\"{PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "#     template_path=TMP_PIPELINE_JSON,\n",
    "#     pipeline_root=PIPELINE_ROOT,\n",
    "#     parameter_values={},\n",
    "#     enable_caching=False,\n",
    "# )\n",
    "\n",
    "# # pipeline_utils.store_pipeline(\n",
    "# #     storage_path=LATEST_PIPELINE_PATH_JSON, \n",
    "# #     filename=TMP_PIPELINE_JSON\n",
    "# # )\n",
    "\n",
    "# pipeline_job.submit(service_account=SERVICE_ACCOUNT,network='projects/12856960411/global/networks/vpcnet-private-svc-access-usc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6a10a-c3de-4739-8285-a4bfee83c0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2d8b3-0536-463b-ae91-1c40f2cd874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 2/2 [00:00<00:00, 1030.29query/s]                        \n",
      "Downloading: 100%|██████████| 7/7 [00:01<00:00,  3.98rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbey Grove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbey Grove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbey Road View</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbey Road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abbey Terrace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abbey Wood Road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Abbey Grove</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f0_\n",
       "0      Abbey Grove\n",
       "1      Abbey Grove\n",
       "2  Abbey Road View\n",
       "3       Abbey Road\n",
       "4    Abbey Terrace\n",
       "5  Abbey Wood Road\n",
       "6      Abbey Grove"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb6f70-18a5-4e96-b33c-0f9aea961b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT regexp_replace(t, '.*[0-9]+[a-zA-Z]?[^a-zA-Z]*', '') FROM UNNEST(['23a, Abbey Grove','43a Abbey Grove','Block 509a Abbey Road View','511 Abbey Road','Flat 8a, Abbey Terrace','14 Abbey Wood Road','100 Abbey Grove']) t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22d6ef80-e949-476c-8ae5-4cfa05b510ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_BQ_QUERY = \"SELECT * FROM `wmt-mlp-p-oyi-ds-or-oyi-dsns.oyi_prod.oyi_train_no_testscan`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c6c233d-3f8e-4c80-b42b-1fa4f8238d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2501710994.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_32328/2501710994.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    SELECT * FROM `wmt-mlp-p-oyi-ds-or-oyi-dsns.oyi_prod.oyi_train_no_testscan` LIMIT 100\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# SELECT * FROM `wmt-mlp-p-oyi-ds-or-oyi-dsns.oyi_prod.oyi_train_no_testscan` LIMIT 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95395114-b767-4fed-a812-5a667ddbb916",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowException",
     "evalue": "Unknown error: Wrapping SWEET�RED�ONION�6LB,6LB BIN failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32328/2283295921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#                               project_id = PROJECT_ID, env=ENV, pipeline_root=PIPELINE_ROOT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_DATA_BQ_QUERY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# nosales_data = data[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#   (data.report_type!='C') &\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, max_results, geography_as_object)\u001b[0m\n\u001b[1;32m   1700\u001b[0m             \u001b[0mcreate_bqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_bqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m             \u001b[0mdate_as_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_as_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mgeography_as_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeography_as_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m         )\n\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, geography_as_object)\u001b[0m\n\u001b[1;32m   2009\u001b[0m         \u001b[0mextra_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"timestamp_as_object\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtimestamp_as_object\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_as_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_as_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0m_check_data_column_metadata_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_column_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_table_to_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_columns_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_table_to_blocks\u001b[0;34m(options, block_table, categories, extension_columns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     result = pa.lib.table_to_blocks(options, block_table, categories,\n\u001b[0;32m-> 1169\u001b[0;31m                                     list(extension_columns.keys()))\n\u001b[0m\u001b[1;32m   1170\u001b[0m     return [_reconstruct_block(item, columns, extension_columns)\n\u001b[1;32m   1171\u001b[0m             for item in result]\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.table_to_blocks\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowException\u001b[0m: Unknown error: Wrapping SWEET�RED�ONION�6LB,6LB BIN failed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import utils\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# data_preprocessing(training_data_bq_query_input = TRAINING_DATA_BQ_QUERY,\n",
    "#                               matcher=MATCHER,\n",
    "#                               project_id = PROJECT_ID, env=ENV, pipeline_root=PIPELINE_ROOT)\n",
    "\n",
    "data = client.query(TRAINING_DATA_BQ_QUERY).to_dataframe()\n",
    "# nosales_data = data[\n",
    "#   (data.report_type!='C') &\n",
    "#   (data.display_ind == \"Display\") &\n",
    "#   (data.oh_qty>=0)]\n",
    "# nosales_data[\"item_desc\"] = nosales_data['item_desc'].str.replace(MATCHER, \"\", regex=True)\n",
    "# nosales_data['run_date'] = pd.to_datetime(nosales_data['run_date'])\n",
    "# max_date = nosales_data['run_date'].max()\n",
    "# cutoff_date = (max_date - timedelta(days=182)).strftime('%Y-%m-%d')\n",
    "# nosales_data = nosales_data[nosales_data.run_date > cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6829191-5115-4da1-bd10-a12a5a488c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryJob<project=wmt-mlp-p-oyi-ds-or-oyi-dsns, location=US, id=a9cf4b0e-7a58-4f29-85d1-3c04fe1b5acd>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import utils\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "client.query(TRAINING_DATA_BQ_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "965ad857-621e-444c-8016-ff74fdae2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: f37d27e7-6c85-4e16-ac57-30e7a0b26bfd\n",
      "Query executing: 0.22s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 400 Syntax error: Expected \")\" or \",\" but got string literal 'RED,ONION' at [3:26]\n",
      "\n",
      "Location: US\n",
      "Job ID: f37d27e7-6c85-4e16-ac57-30e7a0b26bfd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery\n",
    "WITH w AS (\n",
    "      SELECT value AS word\n",
    "      FROM STRING_SPLIT(N'RED,ONION', ',')\n",
    "     )\n",
    "SELECT t.*\n",
    "FROM `wmt-mlp-p-oyi-ds-or-oyi-dsns.oyi_prod.oyi_train_no_testscan` t\n",
    "WHERE NOT EXISTS (SELECT 1\n",
    "                  FROM w\n",
    "                  WHERE t.item_desc LIKE CONCAT('%', w.word, '%')\n",
    "                 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee3a3b7e-7036-4972-bdb0-dbadb5bf6823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.12s: 100%|██████████| 2/2 [00:00<00:00, 16.25query/s]                                  \n",
      "Downloading: 100%|██████████| 4668/4668 [00:02<00:00, 2319.81rows/s]\n"
     ]
    },
    {
     "ename": "ArrowException",
     "evalue": "Unknown error: Wrapping SWEET�RED�ONION�6LB,6LB BIN failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32328/1357732640.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bigquery'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SELECT * FROM `wmt-mlp-p-oyi-ds-or-oyi-dsns.oyi_prod.oyi_train_no_testscan` WHERE item_desc LIKE '%ONION%'\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2470\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2473\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/magics/magics.py\u001b[0m in \u001b[0;36m_cell_magic\u001b[0;34m(line, query)\u001b[0m\n\u001b[1;32m    711\u001b[0m                 \u001b[0mbqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0mcreate_bqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                 \u001b[0mprogress_bar_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             )\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, max_results, geography_as_object)\u001b[0m\n\u001b[1;32m   1700\u001b[0m             \u001b[0mcreate_bqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_bqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m             \u001b[0mdate_as_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_as_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mgeography_as_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeography_as_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m         )\n\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, geography_as_object)\u001b[0m\n\u001b[1;32m   2009\u001b[0m         \u001b[0mextra_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"timestamp_as_object\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtimestamp_as_object\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_as_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_as_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0m_check_data_column_metadata_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_column_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_table_to_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_columns_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_table_to_blocks\u001b[0;34m(options, block_table, categories, extension_columns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     result = pa.lib.table_to_blocks(options, block_table, categories,\n\u001b[0;32m-> 1169\u001b[0;31m                                     list(extension_columns.keys()))\n\u001b[0m\u001b[1;32m   1170\u001b[0m     return [_reconstruct_block(item, columns, extension_columns)\n\u001b[1;32m   1171\u001b[0m             for item in result]\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.table_to_blocks\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowException\u001b[0m: Unknown error: Wrapping SWEET�RED�ONION�6LB,6LB BIN failed"
     ]
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM `wmt-mlp-p-oyi-ds-or-oyi-dsns.oyi_prod.oyi_train_no_testscan` WHERE item_desc LIKE '%ONION%'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1bc32391-2594-49f1-a7fb-c00cb5c7c439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 3/3 [00:00<00:00, 1442.50query/s]                        \n",
      "Downloading: 100%|██████████| 10000/10000 [00:01<00:00, 5411.06rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>club_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>old_nbr</th>\n",
       "      <th>days_no_sale</th>\n",
       "      <th>cat</th>\n",
       "      <th>subcat</th>\n",
       "      <th>state</th>\n",
       "      <th>unit_retail</th>\n",
       "      <th>oh_qty</th>\n",
       "      <th>avg_sales_interval</th>\n",
       "      <th>...</th>\n",
       "      <th>event_txt</th>\n",
       "      <th>event_user</th>\n",
       "      <th>event_ts</th>\n",
       "      <th>event_note</th>\n",
       "      <th>exception_type</th>\n",
       "      <th>central_dt</th>\n",
       "      <th>central_ts</th>\n",
       "      <th>ts_diff</th>\n",
       "      <th>spurious</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4839</td>\n",
       "      <td>54132736</td>\n",
       "      <td>980129998</td>\n",
       "      <td>2.0</td>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "      <td>OK</td>\n",
       "      <td>31.98</td>\n",
       "      <td>18</td>\n",
       "      <td>0.395716</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>bll0022.s04839</td>\n",
       "      <td>2022-09-05 20:06:47+00:00</td>\n",
       "      <td>No Action Taken, already OFS</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>2022-09-05 14:06:47</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4843</td>\n",
       "      <td>22743296</td>\n",
       "      <td>754872</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>TX</td>\n",
       "      <td>4.82</td>\n",
       "      <td>14</td>\n",
       "      <td>3.965859</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>wcswart.s04843</td>\n",
       "      <td>2022-08-09 15:08:50+00:00</td>\n",
       "      <td>No Action Taken, already OFS</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-08-09</td>\n",
       "      <td>2022-08-09 09:08:50</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6239</td>\n",
       "      <td>22743296</td>\n",
       "      <td>754872</td>\n",
       "      <td>4.0</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>MD</td>\n",
       "      <td>4.82</td>\n",
       "      <td>15</td>\n",
       "      <td>4.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>oteuter.s06239</td>\n",
       "      <td>2022-09-04 00:55:52+00:00</td>\n",
       "      <td>No Action Taken, already OFS</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-09-03</td>\n",
       "      <td>2022-09-03 18:55:52</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4817</td>\n",
       "      <td>22743296</td>\n",
       "      <td>754872</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>AL</td>\n",
       "      <td>4.82</td>\n",
       "      <td>10</td>\n",
       "      <td>6.506977</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>jac007c.s04817</td>\n",
       "      <td>2022-08-25 16:45:20+00:00</td>\n",
       "      <td>Add to picklist</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-08-25</td>\n",
       "      <td>2022-08-25 10:45:20</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8190</td>\n",
       "      <td>22743296</td>\n",
       "      <td>754872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>TX</td>\n",
       "      <td>4.82</td>\n",
       "      <td>38</td>\n",
       "      <td>9.404598</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>awbass.s08190</td>\n",
       "      <td>2022-05-01 13:52:36+00:00</td>\n",
       "      <td>New price print sign has been printed</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>2022-05-01 07:52:36</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>8152</td>\n",
       "      <td>53646862</td>\n",
       "      <td>980044441</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>OH</td>\n",
       "      <td>17.98</td>\n",
       "      <td>6</td>\n",
       "      <td>0.480631</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>asadkin.s08152</td>\n",
       "      <td>2022-06-09 11:00:16+00:00</td>\n",
       "      <td>No Action Taken, already OFS</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>2022-06-09 05:00:16</td>\n",
       "      <td>1366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>4969</td>\n",
       "      <td>53646862</td>\n",
       "      <td>980044441</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>AR</td>\n",
       "      <td>17.98</td>\n",
       "      <td>16</td>\n",
       "      <td>0.614921</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>G0G00HV.s04969</td>\n",
       "      <td>2022-09-30 15:05:30+00:00</td>\n",
       "      <td>No Action Taken, already OFS</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>2022-09-30 09:05:30</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>8202</td>\n",
       "      <td>53646862</td>\n",
       "      <td>980044441</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>GA</td>\n",
       "      <td>17.98</td>\n",
       "      <td>2</td>\n",
       "      <td>1.008269</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>SYSTEM</td>\n",
       "      <td>2022-05-29 10:50:15+00:00</td>\n",
       "      <td>Updated the on hands quantity for the item</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-05-29</td>\n",
       "      <td>2022-05-29 04:50:15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>6867</td>\n",
       "      <td>53646862</td>\n",
       "      <td>980044441</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>TX</td>\n",
       "      <td>17.98</td>\n",
       "      <td>2</td>\n",
       "      <td>1.147892</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>SYSTEM</td>\n",
       "      <td>2022-09-28 10:51:37+00:00</td>\n",
       "      <td>Updated the on hands quantity for the item</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>2022-09-28 04:51:37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>6325</td>\n",
       "      <td>53646862</td>\n",
       "      <td>980044441</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>IN</td>\n",
       "      <td>17.98</td>\n",
       "      <td>2</td>\n",
       "      <td>0.649054</td>\n",
       "      <td>...</td>\n",
       "      <td>root_cause</td>\n",
       "      <td>SYSTEM</td>\n",
       "      <td>2022-09-15 10:50:44+00:00</td>\n",
       "      <td>Updated the on hands quantity for the item</td>\n",
       "      <td>NOSALES</td>\n",
       "      <td>2022-09-15</td>\n",
       "      <td>2022-09-15 04:50:44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      club_nbr  item_nbr    old_nbr  days_no_sale  cat  subcat state  \\\n",
       "0         4839  54132736  980129998           2.0   96       5    OK   \n",
       "1         4843  22743296     754872           3.0   49      20    TX   \n",
       "2         6239  22743296     754872           4.0   49      20    MD   \n",
       "3         4817  22743296     754872           2.0   49      20    AL   \n",
       "4         8190  22743296     754872           1.0   49      20    TX   \n",
       "...        ...       ...        ...           ...  ...     ...   ...   \n",
       "9995      8152  53646862  980044441           1.0    8      50    OH   \n",
       "9996      4969  53646862  980044441           2.0    8      50    AR   \n",
       "9997      8202  53646862  980044441           1.0    8      50    GA   \n",
       "9998      6867  53646862  980044441           1.0    8      50    TX   \n",
       "9999      6325  53646862  980044441           1.0    8      50    IN   \n",
       "\n",
       "      unit_retail  oh_qty  avg_sales_interval  ...   event_txt  \\\n",
       "0           31.98      18            0.395716  ...  root_cause   \n",
       "1            4.82      14            3.965859  ...  root_cause   \n",
       "2            4.82      15            4.272727  ...  root_cause   \n",
       "3            4.82      10            6.506977  ...  root_cause   \n",
       "4            4.82      38            9.404598  ...  root_cause   \n",
       "...           ...     ...                 ...  ...         ...   \n",
       "9995        17.98       6            0.480631  ...  root_cause   \n",
       "9996        17.98      16            0.614921  ...  root_cause   \n",
       "9997        17.98       2            1.008269  ...  root_cause   \n",
       "9998        17.98       2            1.147892  ...  root_cause   \n",
       "9999        17.98       2            0.649054  ...  root_cause   \n",
       "\n",
       "          event_user                  event_ts  \\\n",
       "0     bll0022.s04839 2022-09-05 20:06:47+00:00   \n",
       "1     wcswart.s04843 2022-08-09 15:08:50+00:00   \n",
       "2     oteuter.s06239 2022-09-04 00:55:52+00:00   \n",
       "3     jac007c.s04817 2022-08-25 16:45:20+00:00   \n",
       "4      awbass.s08190 2022-05-01 13:52:36+00:00   \n",
       "...              ...                       ...   \n",
       "9995  asadkin.s08152 2022-06-09 11:00:16+00:00   \n",
       "9996  G0G00HV.s04969 2022-09-30 15:05:30+00:00   \n",
       "9997          SYSTEM 2022-05-29 10:50:15+00:00   \n",
       "9998          SYSTEM 2022-09-28 10:51:37+00:00   \n",
       "9999          SYSTEM 2022-09-15 10:50:44+00:00   \n",
       "\n",
       "                                      event_note  exception_type  central_dt  \\\n",
       "0                   No Action Taken, already OFS         NOSALES  2022-09-05   \n",
       "1                   No Action Taken, already OFS         NOSALES  2022-08-09   \n",
       "2                   No Action Taken, already OFS         NOSALES  2022-09-03   \n",
       "3                                Add to picklist         NOSALES  2022-08-25   \n",
       "4          New price print sign has been printed         NOSALES  2022-05-01   \n",
       "...                                          ...             ...         ...   \n",
       "9995                No Action Taken, already OFS         NOSALES  2022-06-09   \n",
       "9996                No Action Taken, already OFS         NOSALES  2022-09-30   \n",
       "9997  Updated the on hands quantity for the item         NOSALES  2022-05-29   \n",
       "9998  Updated the on hands quantity for the item         NOSALES  2022-09-28   \n",
       "9999  Updated the on hands quantity for the item         NOSALES  2022-09-15   \n",
       "\n",
       "              central_ts ts_diff spurious  action  \n",
       "0    2022-09-05 14:06:47      36        0       0  \n",
       "1    2022-08-09 09:08:50      20        0       0  \n",
       "2    2022-09-03 18:55:52     118        0       0  \n",
       "3    2022-08-25 10:45:20      12        1       1  \n",
       "4    2022-05-01 07:52:36      23        0       1  \n",
       "...                  ...     ...      ...     ...  \n",
       "9995 2022-06-09 05:00:16    1366        0       0  \n",
       "9996 2022-09-30 09:05:30      26        0       0  \n",
       "9997 2022-05-29 04:50:15       0        1       1  \n",
       "9998 2022-09-28 04:51:37       0        1       1  \n",
       "9999 2022-09-15 04:50:44       0        1       1  \n",
       "\n",
       "[10000 rows x 37 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM `wmt-mlp-p-oyi-ds-or-oyi-dsns.oyi_prod.oyi_train_no_testscan` LIMIT 10000 \n",
    "# 1689514 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "641d0bec-b86f-4775-b3d4-4c354511fe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 4f589852-9fc7-4627-87d0-911b209352d7\n",
      "Query executing: 0.24s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 400 Syntax error: Expected end of input but got identifier \"COUNT\" at [1:1]\n",
      "\n",
      "Location: US\n",
      "Job ID: 4f589852-9fc7-4627-87d0-911b209352d7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery\n",
    "COUNT (*) FROM oyi_prod.oyi_train_no_testscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffcd1007-8a28-4664-a5a4-e4ea3b90e07c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowException",
     "evalue": "Unknown error: Wrapping SWEET�RED�ONION�6LB,6LB BIN failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32328/790467948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_DATA_BQ_QUERY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, max_results, geography_as_object)\u001b[0m\n\u001b[1;32m   1700\u001b[0m             \u001b[0mcreate_bqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_bqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m             \u001b[0mdate_as_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_as_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mgeography_as_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeography_as_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m         )\n\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, geography_as_object)\u001b[0m\n\u001b[1;32m   2009\u001b[0m         \u001b[0mextra_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"timestamp_as_object\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtimestamp_as_object\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_as_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_as_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0m_check_data_column_metadata_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_column_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_table_to_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_columns_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_table_to_blocks\u001b[0;34m(options, block_table, categories, extension_columns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     result = pa.lib.table_to_blocks(options, block_table, categories,\n\u001b[0;32m-> 1169\u001b[0;31m                                     list(extension_columns.keys()))\n\u001b[0m\u001b[1;32m   1170\u001b[0m     return [_reconstruct_block(item, columns, extension_columns)\n\u001b[1;32m   1171\u001b[0m             for item in result]\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.table_to_blocks\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowException\u001b[0m: Unknown error: Wrapping SWEET�RED�ONION�6LB,6LB BIN failed"
     ]
    }
   ],
   "source": [
    "client.query(TRAINING_DATA_BQ_QUERY).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c609f-00b9-4325-8184-a433b9d5652c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa86f98-dd4f-40bc-a0f4-dd3811f14315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
