{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire Pipeline\n",
    "data-preprocessing >> [train-test-split, train-eval-model] >> element-mlflow-model-registry, update-thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d75fbeca-a613-4cd4-8ea2-4c804747240c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Imports for vertex pipeline\n",
    "from google.cloud import aiplatform\n",
    "import google_cloud_pipeline_components\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    component,\n",
    "    pipeline,\n",
    "    Input,\n",
    "    Output,\n",
    "    Model,\n",
    "    Dataset,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    ")\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(str(Path(\".\").absolute().parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent) + \"/utils\")\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent.parent))\n",
    "\n",
    "import pipeline_utils\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --MODE MODE --STAGE1_FLAG STAGE1_FLAG\n",
      "                             --ENSEMBLE_FLAG ENSEMBLE_FLAG --RF_CLF_MODEL_PATH\n",
      "                             RF_CLF_MODEL_PATH --LOGISTIC_CLF_MODEL_PATH\n",
      "                             LOGISTIC_CLF_MODEL_PATH --STAGE1_NN_MODEL_PATH\n",
      "                             STAGE1_NN_MODEL_PATH --GNB_MODEL_PATH\n",
      "                             GNB_MODEL_PATH --STG1_FEATURE_SELECTOR_MODEL_PATH\n",
      "                             STG1_FEATURE_SELECTOR_MODEL_PATH\n",
      "                             --NOSALES_MODEL_PATH NOSALES_MODEL_PATH\n",
      "ipykernel_launcher.py: error: the following arguments are required: --MODE, --STAGE1_FLAG, --ENSEMBLE_FLAG, --RF_CLF_MODEL_PATH, --LOGISTIC_CLF_MODEL_PATH, --STAGE1_NN_MODEL_PATH, --GNB_MODEL_PATH, --STG1_FEATURE_SELECTOR_MODEL_PATH, --NOSALES_MODEL_PATH\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    args = pipeline_utils.get_args()\n",
    "except:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--MODE\", required=True, type=str)\n",
    "    parser.add_argument(\"--STAGE1_FLAG\", required=True, type=str)\n",
    "    parser.add_argument(\"--ENSEMBLE_FLAG\", required=True, type=str)\n",
    "    parser.add_argument(\"--RF_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--LOGISTIC_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--STAGE1_NN_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--GNB_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--STG1_FEATURE_SELECTOR_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--NOSALES_MODEL_PATH\", required=True, type=str)\n",
    "    sys.args = [\n",
    "        \"--MODE\", \"test\",\n",
    "        \"--STAGE1_FLAG\", \"train\",\n",
    "        \"--ENSEMBLE_FLAG\", \"train\",\n",
    "        \"--RF_CLF_MODEL_PATH\", \"\",\n",
    "        \"--LOGISTIC_CLF_MODEL_PATH\", \"\",\n",
    "        \"--STAGE1_NN_MODEL_PATH\", \"\",\n",
    "        \"--GNB_MODEL_PATH\", \"\",\n",
    "        \"--STG1_FEATURE_SELECTOR_MODEL_PATH\", \"\",\n",
    "        \"--NOSALES_MODEL_PATH\", \"\",\n",
    "    ]\n",
    "    args = parser.parse_args(sys.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV: dev, \n",
      "PROJECT_ID: wmt-mlp-p-oyi-ds-or-oyi-dsns, \n",
      "BASE_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest, \n",
      "MLFLOW_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest, \n",
      "PIPELINE_NAME: oyi-nosales-model-pipeline-dev, \n",
      "PIPELINE_JSON: oyi-nosales-model-pipeline-dev.json\n"
     ]
    }
   ],
   "source": [
    "PARAMS = pipeline_utils.yaml_import('settings.yml')\n",
    "\n",
    "ENV = PARAMS['env_flag']\n",
    "\n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "REGION = PARAMS['envs'][ENV]['REGION']\n",
    "BASE_IMAGE = PARAMS['envs'][ENV]['BASE_IMAGE']\n",
    "MLFLOW_IMAGE = PARAMS['envs'][ENV]['MLFLOW_IMAGE']\n",
    "\n",
    "PIPELINE_ROOT = PARAMS['envs'][ENV]['PIPELINE_ROOT']\n",
    "PIPELINE_NAME = PARAMS['envs'][ENV]['PIPELINE_NAME']\n",
    "PIPELINE_JSON = PARAMS['envs'][ENV]['PIPELINE_JSON']\n",
    "TMP_PIPELINE_JSON = os.path.join(\"/tmp\", PIPELINE_JSON)\n",
    "\n",
    "\n",
    "TRAINING_TABLE_NAME = PARAMS['envs'][ENV]['TRAINING_TABLE_NAME']\n",
    "TRAINING_DATA_BQ_QUERY = f'select * from {TRAINING_TABLE_NAME}'\n",
    "\n",
    "MLFLOW_EXP_NAME = PARAMS['envs'][ENV]['MLFLOW_EXP_NAME']\n",
    "MODEL_REGISTRY_NAME = PARAMS['envs'][ENV]['MODEL_REGISTRY_NAME']\n",
    "\n",
    "SERVICE_ACCOUNT = PARAMS['envs'][ENV]['SERVICE_ACCOUNT']\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    " \n",
    "# Matches on non-word, non-regular-punctuation characters.\n",
    "MATCHER = r\"\"\"[^a-zA-Z0-9'\"!@#$%\\^&*()\\[\\]{}:;<>?,.-=_+ ]+\"\"\" \n",
    "\n",
    "CLUB_THRESH_PATH = PARAMS['envs'][ENV]['CLUB_THRESH_PATH']\n",
    "LATEST_NOSALES_MODEL_PATH = PARAMS['envs'][ENV]['LATEST_NOSALES_MODEL_PATH']\n",
    "LATEST_PIPELINE_PATH = PARAMS['envs'][ENV]['LATEST_PIPELINE_PATH']\n",
    "RUN_PIPELINE = PARAMS['envs'][ENV]['RUN_PIPELINE']\n",
    "print(f\"ENV: {ENV}, \\nPROJECT_ID: {PROJECT_ID}, \\nBASE_IMAGE: {BASE_IMAGE}, \\nMLFLOW_IMAGE: {MLFLOW_IMAGE}, \\nPIPELINE_NAME: {PIPELINE_NAME}, \\nPIPELINE_JSON: {PIPELINE_JSON}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def data_preprocessing(\n",
    "    training_data_bq_query_input: str,\n",
    "    matcher: str,\n",
    "    project_id: str,\n",
    "    env: str,\n",
    "    pipeline_root: str,\n",
    "    training_data_output: Output[Dataset]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    import utils\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    data = client.query(training_data_bq_query_input).to_dataframe()\n",
    "    nosales_data = data[\n",
    "      (data.report_type!='C') &\n",
    "      (data.display_ind == \"Display\") &\n",
    "      (data.oh_qty>=0)]\n",
    "    nosales_data[\"item_desc\"] = nosales_data['item_desc'].str.replace(matcher, \"\", regex=True)\n",
    "    nosales_data['run_date'] = pd.to_datetime(nosales_data['run_date'])\n",
    "    max_date = nosales_data['run_date'].max()\n",
    "    cutoff_date = (max_date - timedelta(days=182)).strftime('%Y-%m-%d')\n",
    "    nosales_data = nosales_data[nosales_data.run_date > cutoff_date]\n",
    "    \n",
    "    nosales_data.replace(\"No Action Taken, already OFS\", \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.replace('Updated the NOSALES type with scrubber event', \"No Action Taken, already out for sale\", inplace=True)\n",
    "    nosales_data.sort_values(by = ['run_date','club_nbr','item_nbr','event_ts'],inplace = True)\n",
    "    nosales_data.drop_duplicates(['old_nbr','club_nbr','run_date'], keep='first',inplace = True)\n",
    "    \n",
    "    nosales_ext = utils.calculate_all_level_tpr(nosales_data, env, pipeline_root, save=True)\n",
    "    nosales_ext.fillna(0, inplace=True)\n",
    "    nosales_ext.to_csv(training_data_output.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def train_test_split(\n",
    "    nosales_ext_input: Input[Dataset],\n",
    "    nosales_train_ext_output: Output[Dataset],\n",
    "    nosales_test_ext_output: Output[Dataset],\n",
    "    nosales_train_usampled_output: Output[Dataset]\n",
    "    \n",
    "):\n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    nosales_ext = pd.read_csv(nosales_ext_input.path)\n",
    "    nosales_ext['run_date'] = pd.to_datetime(nosales_ext['run_date'])\n",
    "    split_date = (nosales_ext.run_date.max() - timedelta(days=50)).strftime('%Y-%m-%d')\n",
    "    nosales_train_ext = nosales_ext[pd.to_datetime(nosales_ext.run_date) < split_date].copy() \n",
    "    nosales_test_ext  = nosales_ext[pd.to_datetime(nosales_ext.run_date) >= split_date].copy() \n",
    "\n",
    "    x=nosales_train_ext.shape[0]\n",
    "    y=nosales_test_ext.shape[0]\n",
    "    print(f\"split_date is {split_date}.\")\n",
    "    print(\"Train/Test ratio:\", x*100/(x+y))\n",
    "    seed = 2019\n",
    "    frac = 11\n",
    "    grouped = nosales_train_ext[nosales_train_ext.event_note == \"No Action Taken, already out for sale\"].groupby('club_nbr')\n",
    "    u1 = grouped.apply(lambda x: x.sample(n=int(x.shape[0]/frac),  random_state=seed)).reset_index(drop=True)\n",
    "\n",
    "    u2 = nosales_train_ext[nosales_train_ext.event_note != \"No Action Taken, already out for sale\"]\n",
    "\n",
    "    nosales_train_usampled = pd.concat([u1, u2])\n",
    "    nosales_train_usampled = nosales_train_usampled.sample(frac=1)\n",
    "    print(nosales_train_usampled.shape)\n",
    "    nosales_train_usampled.event_note.value_counts()\n",
    "    \n",
    "    nosales_train_ext.to_csv(nosales_train_ext_output.path, index=False)\n",
    "    nosales_test_ext.to_csv(nosales_test_ext_output.path, index=False)\n",
    "    nosales_train_usampled.to_csv(nosales_train_usampled_output.path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be3b59b6-65c2-465f-827d-9edca7b83947",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def train_eval_model(\n",
    "    nosales_ext_input: Input[Dataset],\n",
    "    nosales_train_ext_input: Input[Dataset],\n",
    "    nosales_test_ext_input: Input[Dataset],\n",
    "    nosales_train_usampled_input: Input[Dataset],\n",
    "    mode: str,\n",
    "    stage1_flag: str,\n",
    "    ensemble_flag: str,\n",
    "    rf_clf_model_path_input: str,\n",
    "    logistic_clf_model_path_input: str,\n",
    "    stage1_nn_model_path_input: str,\n",
    "    gnb_model_path_input: str,\n",
    "    stg1_feature_selector_model_path_input: str,\n",
    "    nosales_model_path_input: str,\n",
    "    latest_nosales_model_path_input: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    timestamp: str,\n",
    "    rf_clf_model_output: Output[Model],\n",
    "    logistic_clf_model_output: Output[Model],\n",
    "    stage1_nn_model_output: Output[Model],\n",
    "    gnb_model_output: Output[Model],\n",
    "    stg1_feature_selector_model_output: Output[Model],\n",
    "    nosales_model_output: Output[Model],\n",
    "    nosales_test_ext_output: Output[Dataset]\n",
    "):\n",
    "    import os \n",
    "    import pandas as pd\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    import utils\n",
    "    import diagnosis_utils\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.cluster import KMeans\n",
    "    import pickle\n",
    "    from google.cloud import storage, aiplatform\n",
    "    \n",
    "    nosales_ext = pd.read_csv(nosales_ext_input.path)\n",
    "    nosales_train_ext = pd.read_csv(nosales_train_ext_input.path)\n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "    nosales_train_usampled = pd.read_csv(nosales_train_usampled_input.path)\n",
    "    \n",
    "    nosales_ext['run_date'] = pd.to_datetime(nosales_ext['run_date'])\n",
    "    nosales_train_ext['run_date'] = pd.to_datetime(nosales_train_ext['run_date'])\n",
    "    nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "    nosales_train_usampled['run_date'] = pd.to_datetime(nosales_train_usampled['run_date'])\n",
    "    \n",
    "    tpr_features = [col for col in nosales_train_ext.columns if '_tpr' in col]  # len(tpr_features) : 45\n",
    "\n",
    "    numerical_features= ['gap_days','exp_scn_in_nosale_period', 'unit_retail','oh_qty','avg_sales_interval']\n",
    "    numerical_features.extend(tpr_features)\n",
    "    categorical_features = ['club_nbr','state','cat']\n",
    "\n",
    "    all_features = numerical_features + categorical_features\n",
    "    target = ['event_note']\n",
    "\n",
    "    top_features = list(['oh_qty_log',  'club_nbr_cat_update_loc_tpr_log',  'club_nbr_cat_new_price_sign_tpr_log',  'club_nbr_update_loc_tpr_log',\n",
    "    'club_nbr_new_price_sign_tpr_log',  'club_nbr_cat_add_to_picklist_tpr_log',  'item_nbr_update_ohq_tpr_log',\n",
    "    'item_nbr_add_to_picklist_tpr_log',  'club_nbr_add_to_picklist_tpr_log',  'avg_sales_interval_log', \n",
    "    'club_nbr_cat_no_action_taken_tpr_log',  'club_nbr_no_action_taken_tpr_log',  'item_nbr_no_action_taken_tpr_log',\n",
    "    'cat_add_to_picklist_tpr_log',  'unit_retail_log',  'exp_scn_in_nosale_period_log',  'club_nbr_cat_update_ohq_tpr_log', \n",
    "    'cat_update_ohq_tpr_log',  'club_nbr_update_ohq_tpr_log',  'state_cat_add_to_picklist_tpr_log',  'reg_cat_update_ohq_tpr_log',\n",
    "    'state_cat_new_price_sign_tpr_log',  'mkt_cat_new_price_sign_tpr_log',  'mkt_cat_update_ohq_tpr_log', \n",
    "    'reg_cat_add_to_picklist_tpr_log',  'state_cat_update_ohq_tpr_log',  'cat_new_price_sign_tpr_log', \n",
    "    'mkt_cat_update_loc_tpr_log',  'mkt_update_loc_tpr_log',  'mkt_new_price_sign_tpr_log', \n",
    "    'mkt_cat_add_to_picklist_tpr_log',  'mkt_no_action_taken_tpr_log',  'reg_no_action_taken_tpr_log', \n",
    "    'cat_no_action_taken_tpr_log',  'mkt_cat_no_action_taken_tpr_log',  'state_cat_update_loc_tpr_log', \n",
    "    'gap_days_log',  'reg_new_price_sign_tpr_log',  'mkt_update_ohq_tpr_log',  'state_cat_no_action_taken_tpr_log'])\n",
    "\n",
    "    if mode == 'test':\n",
    "        verbose_flag = True\n",
    "    else:\n",
    "        verbose_flag = False\n",
    "\n",
    "\n",
    "    feature_flags = {'kmeans_clustering': False}\n",
    "\n",
    "    class_weights = dict(nosales_train_usampled.event_note.value_counts()[0]/nosales_train_usampled.event_note.value_counts()[:])\n",
    "\n",
    "\n",
    "    # pipeline: location-feat\n",
    "    location_features_tf= Pipeline([\n",
    "        ('select_loc', utils.DataFrameSelector(['sales_floor_location']))\n",
    "    ])\n",
    "\n",
    "    # pipeline: time-feat\n",
    "    time_features_tf= Pipeline([\n",
    "        ('select_rundate', utils.DataFrameSelector(['run_date'])),\n",
    "        ('time_featurize', utils.TimeExtractor())\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline: other-catg-feat\n",
    "    add_cat_tf= Pipeline([\n",
    "        ('select_other_cat', utils.DataFrameSelector(['club_nbr','cat','state']))\n",
    "    ])  \n",
    "\n",
    "\n",
    "    # pipeline: K-means clustering\n",
    "    kmeans_tf = make_pipeline(\n",
    "        utils.DataFrameSelector(numerical_features),\n",
    "        utils.MinMaxScalerTransformer(),\n",
    "        utils.ModelTransformer(KMeans(2))\n",
    "    )\n",
    "\n",
    "    ######################################## Assembling 'Catg' n 'Numeric' Features  #####################################\n",
    "\n",
    "    # list(catg pipelines)\n",
    "    list_of_pipelines_for_catg_feat = [\n",
    "        ('loc_features',location_features_tf),\n",
    "        ('time_features',time_features_tf),\n",
    "        ('other_cat_features', add_cat_tf)\n",
    "    ]\n",
    "    if feature_flags['kmeans_clustering']:\n",
    "        list_of_pipelines_for_catg_feat.append(('clusters', kmeans_tf))\n",
    "\n",
    "\n",
    "    # pipeline: encoding the catg features.\n",
    "    cat_tf = Pipeline([\n",
    "        ('combine_cats', utils.ColumnMerge(transformer_list=list_of_pipelines_for_catg_feat)),\n",
    "        ('cat_featurize', utils.CategoryFeaturizer())\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline: numeric_features + log-transformation   \n",
    "    num_features_tf= Pipeline([\n",
    "        ('select_num', utils.DataFrameSelector(numerical_features)),\n",
    "        ('log', utils.LogFeaturizer()),\n",
    "        ('select_top_features', utils.DataFrameSelector(top_features))\n",
    "    ])\n",
    "\n",
    "    stage2_init_feature_num = 20\n",
    "    num_features_tf2= Pipeline([\n",
    "        ('select_num', utils.DataFrameSelector(numerical_features)),\n",
    "        ('log', utils.LogFeaturizer()),\n",
    "        ('select_top_features', utils.DataFrameSelector(top_features[:stage2_init_feature_num]))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # all_feat => catg_feat + numerical_feat\n",
    "    add_all_tf= utils.ColumnMerge([\n",
    "        ('num_features',num_features_tf),\n",
    "        ('cat_features',cat_tf)\n",
    "    ])\n",
    "\n",
    "    ############################################################## Final pipelines ######################################################################\n",
    "\n",
    "    # Lone classifier-pipelines and pre-processors\n",
    "\n",
    "    #1\n",
    "    rf_clf = RandomForestClassifier(n_jobs=-1, criterion='gini',n_estimators=50, max_depth=7,max_features='sqrt',\n",
    "                                    class_weight = class_weights )\n",
    "\n",
    "    #2\n",
    "    logistic_clf = LogisticRegression(n_jobs=-1, multi_class='multinomial', solver='lbfgs', max_iter=1000, penalty='l2', class_weight=class_weights)\n",
    "    \n",
    "\n",
    "    #3\n",
    "    gnb = utils.CustomizedGaussianNB()\n",
    "\n",
    "    #4\n",
    "    stage1_nn = utils.Stage1_NeuralNetwork(num_classes=5, batch_size=128, epochs=25, verbose=verbose_flag)\n",
    "\n",
    "\n",
    "    stage1_classifiers = {'rf_clf':rf_clf, 'logistic_clf':logistic_clf, 'stage1_nn':stage1_nn, 'gnb':gnb}\n",
    "\n",
    "    stage2_nn_input_dimen = stage2_init_feature_num + len(stage1_classifiers)*5\n",
    "    stage2_estimator = KerasClassifier(build_fn=utils.stage2_nn, input_dimen=stage2_nn_input_dimen, epochs=5, batch_size=128, verbose=verbose_flag)\n",
    "    \n",
    "    ##set flags when in mode: 'test'#####\n",
    "    # True: if you want to save stage1 models during test. Will automatically set to False when in prod\n",
    "    s1_save_flag = True\n",
    "\n",
    "    # Stage 1 models\n",
    "    ####################################\n",
    "    # Force flag to be 'train' during prod\n",
    "    if mode == 'prod':\n",
    "        s1_save_flag = False\n",
    "        stage1_flag = 'train'\n",
    "\n",
    "    stg1_feature_selector = num_features_tf\n",
    "\n",
    "\n",
    "\n",
    "    if stage1_flag == 'train': \n",
    "        print(\"Training and saving models...\")\n",
    "        X_train = stg1_feature_selector.fit_transform(nosales_train_usampled, nosales_train_usampled.event_note)\n",
    "        X_train = X_train.astype('float128')\n",
    "        y_train = nosales_train_usampled.event_note\n",
    "        if s1_save_flag:\n",
    "            with open(stg1_feature_selector_model_output.path, 'wb') as file:  \n",
    "                pickle.dump(stg1_feature_selector, file)\n",
    "    \n",
    "        X_test= stg1_feature_selector.transform(nosales_test_ext)\n",
    "        stage1_model_output_paths = {'rf_clf':rf_clf_model_output.path, 'logistic_clf':logistic_clf_model_output.path,\n",
    "                               'stage1_nn':stage1_nn_model_output.path, 'gnb':gnb_model_output.path}\n",
    "        for clf in stage1_classifiers:\n",
    "            print(clf)\n",
    "\n",
    "            model = stage1_classifiers[clf]\n",
    "            # filename = clf + \".model\"\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            print(\"\\n\")\n",
    "            if s1_save_flag:\n",
    "                save_path = stage1_model_output_paths[clf]\n",
    "                with open(save_path, 'wb') as file:  \n",
    "                    pickle.dump(model, file)\n",
    "\n",
    "    else:\n",
    "        print(\"Loading models...\")\n",
    "        \n",
    "        with open(rf_clf_model_path_input, \"rb\") as handler:\n",
    "            rf_clf = pickle.load(handler)\n",
    "       \n",
    "        with open(logistic_clf_model_path_input, \"rb\") as handler:\n",
    "            logistic_clf = pickle.load(handler)\n",
    "        \n",
    "        with open(stage1_nn_model_path_input, \"rb\") as handler:\n",
    "            stage1_nn = pickle.load(handler)\n",
    "        \n",
    "        with open(gnb_model_path_input, \"rb\") as handler:\n",
    "            gnb = pickle.load(handler)\n",
    "       \n",
    "        stage1_classifiers = {'rf_clf':rf_clf, 'logistic_clf':logistic_clf, 'stage1_nn':stage1_nn, 'gnb':gnb}\n",
    "        \n",
    "        with open(stg1_feature_selector_model_path_input, \"rb\") as handler:\n",
    "            stg1_feature_selector = pickle.load(handler)\n",
    "        X_test= stg1_feature_selector.transform(nosales_test_ext)\n",
    "        for clf in stage1_classifiers:\n",
    "            print(clf)\n",
    "            model = stage1_classifiers[clf]\n",
    "            nosales_test_ext = diagnosis_utils.model_diag(nosales_test_ext, model.predict_proba(X_test), model.classes_)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        rf_clf_model_output.path = rf_clf_model_path_input\n",
    "        logistic_clf_model_output.path = logistic_clf_model_path_input\n",
    "        stage1_nn_model_output.path = stage1_nn_model_path_input\n",
    "        gnb_model_output.path = gnb_model_path_input\n",
    "        stg1_feature_selector_model_output.path = stg1_feature_selector_model_path_input\n",
    "        \n",
    "\n",
    "\n",
    "    # ensemble model\n",
    "    #################################################################### \n",
    "    if mode == 'test':\n",
    "        train_x = nosales_train_ext\n",
    "        train_y = nosales_train_ext.event_note\n",
    "\n",
    "    if mode == 'prod':\n",
    "        ensemble_flag = 'train'\n",
    "        train_x = nosales_ext\n",
    "        train_y = nosales_ext.event_note\n",
    "\n",
    "\n",
    "    print(mode, ensemble_flag, train_x.shape[0])  \n",
    "\n",
    "    stg2_feture_selector = num_features_tf2\n",
    "\n",
    "    if ensemble_flag == 'train': \n",
    "        print(\"Training and saving ensemble...\")\n",
    "        stack_pipeline = Pipeline([\n",
    "            ('ensemble_classifier', utils.EnsembleClassifier(stg1_feature_selector, list(stage1_classifiers.values()),\n",
    "                                                     stg2_feture_selector, stage2_estimator)) ])\n",
    "        stack_pipeline.fit(train_x, train_y)\n",
    "        with open(nosales_model_output.path, 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file)\n",
    "        \n",
    "        with open('latest_nosales_model_output', 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file) \n",
    "        blob = storage.blob.Blob.from_string(latest_nosales_model_path_input, client=storage.Client())\n",
    "        blob.upload_from_filename('latest_nosales_model_output')\n",
    "        print(\"Saved the final model\")\n",
    "        \n",
    "        if mode == 'test':\n",
    "            nosales_test_ext = diagnosis_utils.model_diag(nosales_test_ext, stack_pipeline.predict_proba(nosales_test_ext), stack_pipeline.classes_)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"Loading ensemble...\")\n",
    "        with open(nosales_model_path_input, \"rb\") as handler:\n",
    "            stack_pipeline = pickle.load(handler)\n",
    "        nosales_test_ext = diagnosis_utils.model_diag(nosales_test_ext, stack_pipeline.predict_proba(nosales_test_ext), stack_pipeline.classes_)\n",
    "        \n",
    "        nosales_model_output.path = nosales_model_path_input\n",
    "        with open('latest_nosales_model_output', 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file) \n",
    "        blob = storage.blob.Blob.from_string(latest_nosales_model_path_input, client=storage.Client())\n",
    "        blob.upload_from_filename('latest_nosales_model_output')\n",
    "       \n",
    "        \n",
    "    nosales_test_ext.to_csv(nosales_test_ext_output.path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def update_thresholds(\n",
    "    nosales_test_ext_input: Input[Dataset],\n",
    "    club_thresh_path_input: str,\n",
    "    nosales_model_input: Input[Model],\n",
    "    club_threshold_output: Output[Dataset]\n",
    "):\n",
    "    \n",
    "    import utils\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    from google.cloud import storage\n",
    "    from tempfile import TemporaryFile\n",
    "    \n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "    nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "   \n",
    "    with open(nosales_model_input.path, \"rb\") as handler:\n",
    "        stack_pipeline = pickle.load(handler)\n",
    "    \n",
    "    nosales_thresh = utils.gen_thresholds(df = nosales_test_ext,  predictions = stack_pipeline.predict_proba(X=nosales_test_ext), classes = stack_pipeline.classes_)\n",
    "    df_nosales_thresh = pd.DataFrame(nosales_thresh.items(), columns = ['club_nbr','nosales_club_tresh'])\n",
    "    \n",
    "    club_threshold_file_path = os.path.join(club_thresh_path_input, \"club_thresh_chain.csv\")\n",
    "    df_cancelled_thresh = pd.read_csv(club_threshold_file_path).drop(columns = 'nosales_club_tresh')\n",
    "    all_thresh = df_cancelled_thresh.merge(df_nosales_thresh, how = 'left', on = 'club_nbr')\n",
    "    club_threshold_output.path = club_threshold_file_path\n",
    "    all_thresh.to_csv(club_threshold_file_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(base_image=BASE_IMAGE)\n",
    "# def check2(\n",
    "#     training_data_bq_query_input: str,\n",
    "#     matcher: str,\n",
    "#     project_id: str,\n",
    "#     env: str,\n",
    "#     pipeline_root: str,\n",
    "#     training_data_output: Output[Dataset]):\n",
    "    \n",
    "#     import pandas as pd\n",
    "#     from datetime import timedelta\n",
    "#     import utils\n",
    "#     from google.cloud import bigquery\n",
    "\n",
    "#     client = bigquery.Client(project=project_id)\n",
    "#     data = client.query(training_data_bq_query_input).to_dataframe()\n",
    "#     nosales_data = data[\n",
    "#       (data.report_type!='C') &\n",
    "#       (data.display_ind == \"Display\") &\n",
    "#       (data.oh_qty>=0)]\n",
    "#     nosales_data[\"item_desc\"] = nosales_data['item_desc'].str.replace(matcher, \"\", regex=True)\n",
    "#     nosales_data['run_date'] = pd.to_datetime(nosales_data['run_date'])\n",
    "#     max_date = nosales_data['run_date'].max()\n",
    "#     cutoff_date = (max_date - timedelta(days=182)).strftime('%Y-%m-%d')\n",
    "#     nosales_data = nosales_data[nosales_data.run_date > cutoff_date]\n",
    "    \n",
    "#     nosales_data.replace(\"No Action Taken, already OFS\", \"No Action Taken, already out for sale\", inplace=True)\n",
    "#     nosales_data.replace('Updated the NOSALES type with scrubber event', \"No Action Taken, already out for sale\", inplace=True)\n",
    "#     nosales_data.sort_values(by = ['run_date','club_nbr','item_nbr','event_ts'],inplace = True)\n",
    "#     nosales_data.drop_duplicates(['old_nbr','club_nbr','run_date'], keep='first',inplace = True)\n",
    "    \n",
    "#     nosales_ext = utils.calculate_all_level_tpr(nosales_data, env, pipeline_root, save=True)\n",
    "#     nosales_ext.fillna(0, inplace=True)\n",
    "#     # nosales_ext.to_csv(training_data_output.path, index=False)\n",
    "#     print(nosales_ext.head())\n",
    "    \n",
    "# data = check2(training_data_bq_query_input = TRAINING_DATA_BQ_QUERY,\n",
    "#                               matcher=MATCHER,\n",
    "#                               project_id = PROJECT_ID, env=ENV, pipeline_root=PIPELINE_ROOT)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://oyi-ds-club-score-cutoff-pipeline-bucket-nonprod/latest_club_thresh_chain_dev/club_thresh_chain.csv\n",
      "     club_nbr  cancelled_club_thresh  nosales_club_thresh\n",
      "0        4041                 0.5448               0.5220\n",
      "1        4109                 0.5617               0.5857\n",
      "2        4702                 0.5333               0.4565\n",
      "3        4703                 0.3857               0.3308\n",
      "4        4704                 0.3947               0.3207\n",
      "..        ...                    ...                  ...\n",
      "594      8295                 0.3840               0.4089\n",
      "595      8296                 0.5001               0.5485\n",
      "596      8297                 0.2706               0.4746\n",
      "597      8298                 0.4145               0.4570\n",
      "598      8299                 0.3785               0.1485\n",
      "\n",
      "[599 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def check(path):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    file_path = os.path.join(path, \"club_thresh_chain.csv\")\n",
    "    print(file_path)\n",
    "    df_cancelled_thresh = pd.read_csv(file_path)\n",
    "    # df_cancelled_thresh.columns = ['club_nbr', 'cancelled_club_thresh', 'nosales_club_thresh']\n",
    "    print(df_cancelled_thresh)\n",
    "    # columns = ['club_nbr','nosales_club_thresh']\n",
    "check(CLUB_THRESH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=PIPELINE_NAME)\n",
    "def pipeline():\n",
    "    data = data_preprocessing(training_data_bq_query_input = TRAINING_DATA_BQ_QUERY,\n",
    "                              matcher=MATCHER,\n",
    "                              project_id = PROJECT_ID, env=ENV, pipeline_root=PIPELINE_ROOT)\n",
    "    \n",
    "    train_test_data = train_test_split(nosales_ext_input=data.outputs['training_data_output'])\n",
    "    \n",
    "    train_eval_data = train_eval_model(nosales_ext_input=data.outputs['training_data_output'],\n",
    "                                       nosales_train_ext_input=train_test_data.outputs['nosales_train_ext_output'],\n",
    "                                       nosales_test_ext_input=train_test_data.outputs['nosales_test_ext_output'],\n",
    "                                       nosales_train_usampled_input=train_test_data.outputs['nosales_train_usampled_output'],\n",
    "                                       mode=args.MODE,\n",
    "                                       stage1_flag=args.STAGE1_FLAG,\n",
    "                                       ensemble_flag=args.ENSEMBLE_FLAG,\n",
    "                                       rf_clf_model_path_input=args.RF_CLF_MODEL_PATH,\n",
    "                                       logistic_clf_model_path_input=args.LOGISTIC_CLF_MODEL_PATH,\n",
    "                                       stage1_nn_model_path_input=args.STAGE1_NN_MODEL_PATH,\n",
    "                                       gnb_model_path_input=args.GNB_MODEL_PATH,\n",
    "                                       stg1_feature_selector_model_path_input=args.STG1_FEATURE_SELECTOR_MODEL_PATH,\n",
    "                                       nosales_model_path_input=args.NOSALES_MODEL_PATH,\n",
    "                                       latest_nosales_model_path_input=LATEST_NOSALES_MODEL_PATH,\n",
    "                                       project_id=PROJECT_ID,\n",
    "                                       region=REGION,\n",
    "                                       timestamp=TIMESTAMP)\n",
    "   \n",
    "    updated_thresholds = update_thresholds(nosales_test_ext_input=train_eval_data.outputs['nosales_test_ext_output'],  \n",
    "                                           club_thresh_path_input=CLUB_THRESH_PATH,\n",
    "                                           nosales_model_input=train_eval_data.outputs['nosales_model_output'])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    element_model_registry = CustomTrainingJobOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        network=\"projects/12856960411/global/networks/vpcnet-shared-prod-01\",\n",
    "        reserved_ip_ranges=[\"vpcnet-shared-prod-01-datafusion-01\"],\n",
    "\n",
    "        display_name=\"mlflow-model-registry\",\n",
    "\n",
    "        worker_pool_specs=[{\n",
    "            \"replica_count\": 1,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_count\": 0,\n",
    "            },\n",
    "            # The below dictionary specifies:\n",
    "            #   1. The URI of the custom image to run this CustomTrainingJobOp against\n",
    "            #      - this image is built from ../../custom_image_builds/model_registry_image_build.ipynb\n",
    "            #   2. The command to run against that image\n",
    "            #   3. The arguments to supply to that custom image \n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": MLFLOW_IMAGE,\n",
    "                \"command\": [\n",
    "                    \"python3\", \"nosales_model_registry.py\"\n",
    "                ],\n",
    "                \"args\": [\n",
    "                    \"--GCS_MODEL_PATH\", LATEST_NOSALES_MODEL_PATH,\n",
    "                    \"--MLFLOW_EXP_NAME\", MLFLOW_EXP_NAME,\n",
    "                    \"--MODEL_REGISTRY_NAME\", MODEL_REGISTRY_NAME\n",
    "                ],\n",
    "            },\n",
    "        }],\n",
    "\n",
    "    ).set_display_name(\"element-mlflow-model-registry\")\n",
    "    element_model_registry.after(train_eval_data)\n",
    "    \n",
    "    \n",
    "#     element_model_registry = CustomTrainingJobOp(\n",
    "#         project=PROJECT_ID,\n",
    "#         location=REGION,\n",
    "#         service_account=SERVICE_ACCOUNT,\n",
    "#         network=\"projects/12856960411/global/networks/vpcnet-shared-prod-01\",\n",
    "#         reserved_ip_ranges=[\"vpcnet-shared-prod-01-datafusion-01\"],\n",
    "\n",
    "#         display_name=\"mlflow-model-registry\",\n",
    "\n",
    "#         worker_pool_specs=[{\n",
    "#             \"replica_count\": 1,\n",
    "#             \"machine_spec\": {\n",
    "#                 \"machine_type\": \"n1-standard-4\",\n",
    "#                 \"accelerator_count\": 0,\n",
    "#             },\n",
    "#             # The below dictionary specifies:\n",
    "#             #   1. The URI of the custom image to run this CustomTrainingJobOp against\n",
    "#             #      - this image is built from ../../custom_image_builds/model_registry_image_build.ipynb\n",
    "#             #   2. The command to run against that image\n",
    "#             #   3. The arguments to supply to that custom image \n",
    "#             \"container_spec\": {\n",
    "#                 \"image_uri\": MLFLOW_IMAGE,\n",
    "#                 \"command\": [\n",
    "#                     \"python3\", \"cancelled_model_registry.py\"\n",
    "#                 ],\n",
    "#                 \"args\": [\n",
    "#                     \"--GCS_MODEL_PATH\", CANCELLED_MODEL_PATH,\n",
    "#                     \"--MLFLOW_EXP_NAME\", MLFLOW_EXP_NAME,\n",
    "#                     \"--MODEL_REGISTRY_NAME\", MODEL_REGISTRY_NAME,\n",
    "#                 ],\n",
    "#             },\n",
    "#         }],\n",
    "\n",
    "#     ).set_display_name(\"element-mlflow-model-registry\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp                                   1.8.12\n",
      "kfp-pipeline-spec                     0.1.16\n",
      "kfp-server-api                        1.8.4\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep kfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/oyi-nosales-model-pipeline-dev.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TMP_PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The exact same schedule already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/schedule.py\u001b[0m in \u001b[0;36m_create_from_pipeline_dict\u001b[0;34m(pipeline_dict, schedule, project_id, region, time_zone, parameter_values, pipeline_root, service_account, app_engine_region, cloud_scheduler_service_account)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mproject_location_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_location_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mjob_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduled_job\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/schedule.py\u001b[0m in \u001b[0;36m_create_scheduler_job\u001b[0;34m(project_location_path, job_body)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_location_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     ).execute()\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 409 when requesting https://cloudscheduler.googleapis.com/v1/projects/wmt-mlp-p-oyi-ds-or-oyi-dsns/locations/us-central1/jobs?alt=json returned \"Job projects/wmt-mlp-p-oyi-ds-or-oyi-dsns/locations/us-central1/jobs/pipeline_oyi-nosales-model-pipeline-dev_1b94ead4_5-a-a-a-a already exists.\". Details: \"Job projects/wmt-mlp-p-oyi-ds-or-oyi-dsns/locations/us-central1/jobs/pipeline_oyi-nosales-model-pipeline-dev_1b94ead4_5-a-a-a-a already exists.\">",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30641/3178350237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mjob_spec_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTMP_PIPELINE_JSON\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCHEDULE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtime_zone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIME_ZONE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# parameter_values=PIPELINE_PARAMETERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/client.py\u001b[0m in \u001b[0;36mcreate_schedule_from_job_spec\u001b[0;34m(self, job_spec_path, schedule, time_zone, pipeline_root, parameter_values, service_account, enable_caching, app_engine_region, cloud_scheduler_service_account)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_account\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mapp_engine_region\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapp_engine_region\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             cloud_scheduler_service_account=cloud_scheduler_service_account)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/schedule.py\u001b[0m in \u001b[0;36m_create_from_pipeline_dict\u001b[0;34m(pipeline_dict, schedule, project_id, region, time_zone, parameter_values, pipeline_root, service_account, app_engine_region, cloud_scheduler_service_account)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'409'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             raise RuntimeError(\n\u001b[0;32m--> 201\u001b[0;31m                 'The exact same schedule already exists') from err\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The exact same schedule already exists"
     ]
    }
   ],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")\n",
    "\n",
    "# response = api_client.create_run_from_job_spec(\n",
    "#     job_spec_path=TMP_PIPELINE_JSON,\n",
    "#     enable_caching= True\n",
    "# )\n",
    "\n",
    "SCHEDULE = '5 * * * *'\n",
    "TIME_ZONE = \"America/Chicago\"\n",
    "api_client.create_schedule_from_job_spec(\n",
    "    job_spec_path=TMP_PIPELINE_JSON,\n",
    "    schedule=SCHEDULE,\n",
    "    time_zone=TIME_ZONE,\n",
    "    # parameter_values=PIPELINE_PARAMETERS\n",
    ")\n",
    "\n",
    "# from kfp.v2.google.client.schedule import create_from_pipeline_file\n",
    "\n",
    "# response = create_from_pipeline_file(\n",
    "#     project_id=PROJECT_ID,\n",
    "#     region=REGION,\n",
    "#     pipeline_path=TMP_PIPELINE_JSON,\n",
    "#     schedule=\"2 * * * *\",\n",
    "#     time_zone=\"America/Chicago\",  # change this as necessary\n",
    "#     # parameter_values={\"text\": \"Hello world!\"},\n",
    "#     # pipeline_root=PIPELINE_ROOT  # this argument is necessary if you did not specify PIPELINE_ROOT as part of the pipeline definition.\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_client.list_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=TMP_PIPELINE_JSON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "    template_path=TMP_PIPELINE_JSON,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={},\n",
    "    enable_caching=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_utils.store_pipeline(\n",
    "    storage_path=LATEST_PIPELINE_PATH, \n",
    "    filename=TMP_PIPELINE_JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job.submit(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "train_model",
   "notebookOrigID": 2093267122234119,
   "widgets": {}
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
