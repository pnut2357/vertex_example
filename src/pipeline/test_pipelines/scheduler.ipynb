{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face35b9-f296-453b-91d4-7c421e35a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = \"--user\"\n",
    "else:\n",
    "    USER_FLAG = \"\"\n",
    "\n",
    "# ! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648ce06-eb81-4aa4-8865-01dd04a0b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U google-cloud-storage $USER_FLAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b2020-f172-4730-80d3-bb6a862be26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     # Automatically restart kernel after installs\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c6202-d755-4970-9ce9-98781ebd99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1867ab0b-1f84-433a-99f5-2ffe86ddace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime \n",
    "sys.path.append(str(Path(\".\").absolute().parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent) + \"/utils\")\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent.parent))\n",
    "import pipeline_utils\n",
    "PARAMS = pipeline_utils.yaml_import('settings.yml')\n",
    "\n",
    "ENV = PARAMS['env_flag']\n",
    "\n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "REGION = PARAMS['envs'][ENV]['REGION']\n",
    "BASE_IMAGE = PARAMS['envs'][ENV]['BASE_IMAGE']\n",
    "MLFLOW_IMAGE = PARAMS['envs'][ENV]['MLFLOW_IMAGE']\n",
    "\n",
    "PIPELINE_ROOT = PARAMS['envs'][ENV]['PIPELINE_ROOT']\n",
    "PIPELINE_NAME = PARAMS['envs'][ENV]['PIPELINE_NAME']\n",
    "PIPELINE_JSON = PARAMS['envs'][ENV]['PIPELINE_JSON']\n",
    "# TMP_PIPELINE_JSON = os.path.join(\"/tmp\", PIPELINE_JSON)\n",
    "\n",
    "\n",
    "TRAINING_TABLE_NAME = PARAMS['envs'][ENV]['TRAINING_TABLE_NAME']\n",
    "TRAINING_DATA_BQ_QUERY = f'select * from {TRAINING_TABLE_NAME}'\n",
    "\n",
    "MLFLOW_EXP_NAME = PARAMS['envs'][ENV]['MLFLOW_EXP_NAME']\n",
    "MODEL_REGISTRY_NAME = PARAMS['envs'][ENV]['MODEL_REGISTRY_NAME']\n",
    "\n",
    "SERVICE_ACCOUNT = PARAMS['envs'][ENV]['SERVICE_ACCOUNT']\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    " \n",
    "# Matches on non-word, non-regular-punctuation characters.\n",
    "# MATCHER = r\"\"\"[^a-zA-Z0-9'\"!@#$%\\^&*()\\[\\]{}:;<>?,.-=_+ ]+\"\"\" \n",
    "\n",
    "CLUB_THRESH_PATH = PARAMS['envs'][ENV]['CLUB_THRESH_PATH']\n",
    "LATEST_NOSALES_MODEL_PATH = PARAMS['envs'][ENV]['LATEST_NOSALES_MODEL_PATH']\n",
    "LATEST_PIPELINE_PATH = PARAMS['envs'][ENV]['LATEST_PIPELINE_PATH']\n",
    "# RUN_PIPELINE = PARAMS['envs'][ENV]['RUN_PIPELINE']\n",
    "# print(f\"ENV: {ENV}, \\nPROJECT_ID: {PROJECT_ID}, \\nBASE_IMAGE: {BASE_IMAGE}, \\nMLFLOW_IMAGE: {MLFLOW_IMAGE}, \\nPIPELINE_NAME: {PIPELINE_NAME}, \\nPIPELINE_JSON: {PIPELINE_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80451a4a-2c60-442b-b66b-1b8f2a7c1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"wmt-mlp-p-oyi-ds-or-oyi-dsns\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fab715-9be0-4c16-ae90-c1868e9d3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cbf03b-916a-415e-aef3-fdb49cf3e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config get-value project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2bb1cd-dc97-421c-9f5f-2e7b67583563",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481baa0-1d46-4694-9ac0-37120c7a21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b1598-7054-4578-9e6d-0852eab09618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"svc-oyi-ds-or-oyi-dsns-admin@wmt-mlp-p-oyi-ds-or-oyi-dsns.iam.gserviceaccount.com\"\n",
    "):\n",
    "\n",
    "    # Get your GCP project id from gcloud\n",
    "\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4784e54-e30d-454e-bc8d-38610206785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the json key file of the service account.\n",
    "!gcloud iam service-accounts keys create sa.json --iam-account={SERVICE_ACCOUNT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb9434-8688-42a8-a0a8-d0291d167934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='/home/jupyter/test/sa.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21301f36-f7bd-4625-80a5-907b5e836dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authenticating service account for the upcoming operations.\n",
    "!gcloud auth activate-service-account --key-file=sa.json\n",
    "!gcloud config set account {SERVICE_ACCOUNT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8da1b2-1fa4-4477-a57c-838481ef6b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5bf083f-ba71-4538-b067-9414119429cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shell_output = !gcloud auth list 2>/dev/null\n",
    "# shell_output[3].replace(\"*\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de325a-ec93-4028-acdc-798f6d9b4da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f296ad-7716-4314-8eb1-768815c5212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://oyi-ds-vertex-pipeline-bucket-nonprod\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = PIPELINE_ROOT\n",
    "print(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997df826-b3ae-44cb-8671-2a8833fc485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://wmt-mlp-p-oyi-ds-or-oyi-dsns-aip-20221017234510\n"
     ]
    }
   ],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == PIPELINE_ROOT:\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"-aip-\" + TIMESTAMP\n",
    "    print(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb96beb-4071-44fa-a31e-40db10c8caa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://wmt-mlp-p-oyi-ds-or-oyi-dsns-aip-20221017234510/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fc5400-050e-47b8-bb84-40a207e324cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "607ac2c2-37e8-4079-a926-07e39ec4e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.wmt-mlp-p-oyi-ds-or-oyi-dsns.appspot.com/\n",
      "gs://oyi-ds-club-score-cutoff-pipeline-bucket/\n",
      "gs://oyi-ds-club-score-cutoff-pipeline-bucket-nonprod/\n",
      "gs://oyi-ds-vertex-experiment-bucket/\n",
      "gs://oyi-ds-vertex-output-bucket/\n",
      "gs://oyi-ds-vertex-pipeline-bucket/\n",
      "gs://oyi-ds-vertex-pipeline-bucket-nonprod/\n",
      "gs://oyi-ds-vertex-pipeline-bucket-output/\n",
      "gs://oyi-ds-vertex-test-input-bucket/\n",
      "gs://wmt-mlp-p-oyi-ds-or-oyi-dsns-aip-20221017223909/\n",
      "gs://wmt-mlp-p-oyi-ds-or-oyi-dsns-aip-20221017234510/\n",
      "gs://wmt-mlp-p-oyi-ds-or-oyi-dsns-bq-temp/\n",
      "gs://wmt-mlp-p-oyi-ds-or-oyi-dsns-export-bucket/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c756b3dd-6a9a-435c-8431-ce9bee413cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3798477-ab6d-40e9-86b3-8e271cf65751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d4560b-1d2d-465c-a3b0-e0c2c52c8244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1-aiplatform.googleapis.com\n"
     ]
    }
   ],
   "source": [
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "print(API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1fd43a-c079-4a30-b167-5e8d43830cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://wmt-mlp-p-oyi-ds-or-oyi-dsns-aip-20221017234510/pipeline_root/intro\n"
     ]
    }
   ],
   "source": [
    "_PIPELINE_ROOT = \"{}/pipeline_root/intro\".format(BUCKET_URI)\n",
    "print(_PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1596154c-a503-45a2-82cc-6150a8bbfd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25abbd75-31e5-4770-ab14-76b4d24a77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a656807-8d30-4dbb-95af-d0a1e86dcd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV: dev \n",
      "PROJECT_ID: wmt-mlp-p-oyi-ds-or-oyi-dsns \n",
      "BASE_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest \n",
      "MLFLOW_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest \n",
      "PIPELINE_NAME: oyi-nosales-model-pipeline-dev \n",
      "PIPELINE_JSON: oyi-nosales-model-pipeline-dev.json\n",
      "\n",
      "TMP_PIPELINE_JSON: /tmp/oyi-nosales-model-pipeline-dev.json \n",
      "LATEST_PIPELINE_PATH: gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_nosales_model_output_dev \n",
      "LATEST_PIPELINE_PATH_JSON: gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_nosales_model_output_dev.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: pipeline.py [-h] --MODE MODE --STAGE1_FLAG STAGE1_FLAG --ENSEMBLE_FLAG\n",
      "                   ENSEMBLE_FLAG --RF_CLF_MODEL_PATH RF_CLF_MODEL_PATH\n",
      "                   --LOGISTIC_CLF_MODEL_PATH LOGISTIC_CLF_MODEL_PATH\n",
      "                   --STAGE1_NN_MODEL_PATH STAGE1_NN_MODEL_PATH\n",
      "                   --GNB_MODEL_PATH GNB_MODEL_PATH\n",
      "                   --STG1_FEATURE_SELECTOR_MODEL_PATH\n",
      "                   STG1_FEATURE_SELECTOR_MODEL_PATH --NOSALES_MODEL_PATH\n",
      "                   NOSALES_MODEL_PATH\n",
      "pipeline.py: error: the following arguments are required: --MODE, --STAGE1_FLAG, --ENSEMBLE_FLAG, --RF_CLF_MODEL_PATH, --LOGISTIC_CLF_MODEL_PATH, --STAGE1_NN_MODEL_PATH, --GNB_MODEL_PATH, --STG1_FEATURE_SELECTOR_MODEL_PATH, --NOSALES_MODEL_PATH\n"
     ]
    }
   ],
   "source": [
    "%run pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b410508e-7065-466c-bee7-05e5d824ccc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/intro_pipeline.json'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\"/tmp\", \"intro_pipeline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57271059-4a58-46af-9ba5-9528ca1b5711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('intro_20221017234643',\n",
       " 'gs://oyi-ds-vertex-pipeline-bucket-nonprod/intro_20221017234643-dev.json',\n",
       " 'gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_nosales_model_output_dev.json',\n",
       " '/tmp/oyi-nosales-model-pipeline-dev.json',\n",
       " 'gs://wmt-mlp-p-oyi-ds-or-oyi-dsns-aip-20221017234510/pipeline_root/intro')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DISPLAY_NAME, DISPLAY_NAME_JSON, LATEST_PIPELINE_PATH_JSON, TMP_PIPELINE_JSON, _PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fadea8e-5faf-43b0-9794-74422e648c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/oyi-nosales-model-pipeline-dev.json\n",
      "contents /tmp/oyi-nosales-model-pipeline-dev.json uploaded to gs://oyi-ds-vertex-pipeline-bucket-nonprod/intro_20221017234643-dev.json.\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"intro_\" + TIMESTAMP\n",
    "DISPLAY_NAME_JSON = f\"gs://oyi-ds-vertex-pipeline-bucket-nonprod/{DISPLAY_NAME}-{ENV}.json\"\n",
    "TMP_TEMPLATE_PATH = os.path.join(\"/tmp\", f\"intro_pipeline-{ENV}.json\")\n",
    "\n",
    "pipeline_utils.store_pipeline(\n",
    "    storage_path=DISPLAY_NAME_JSON, #gs://oyi-ds-vertex-pipeline-bucket-nonprod/latest_training_pipeline_dev.json\n",
    "    filename=TMP_PIPELINE_JSON\n",
    ")\n",
    "\n",
    "pipeline_job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=TMP_PIPELINE_JSON, # Pipeline json file name\n",
    "    pipeline_root=_PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "# pipeline_job.run(service_account=SERVICE_ACCOUNT)\n",
    "\n",
    "# pipeline_job = aiplatform.PipelineJob(\n",
    "#     display_name=f\"{PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "#     template_path=TMP_PIPELINE_JSON,\n",
    "#     pipeline_root=PIPELINE_ROOT,\n",
    "#     parameter_values={},\n",
    "#     enable_caching=False,\n",
    "# )\n",
    "\n",
    "# pipeline_utils.store_pipeline(\n",
    "#     storage_path=LATEST_PIPELINE_PATH_JSON, \n",
    "#     filename=TMP_PIPELINE_JSON\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9b9a272-845a-4c52-a93a-35f1f4046a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "import json\n",
    "from typing import Any, Dict\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import scheduler_v1\n",
    "\n",
    "# Load data from a JSON document\n",
    "def load_json(path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Loads data from a JSON document.\n",
    "\n",
    "    Args:\n",
    "      path: The path of the JSON document. It can be a local path or a GS URI.\n",
    "\n",
    "    Returns:\n",
    "      A deserialized Dict object representing the JSON document.\n",
    "    \"\"\"\n",
    "\n",
    "    if path.startswith('gs://'):\n",
    "        return _load_json_from_gs_uri(path)\n",
    "    else:\n",
    "        return _load_json_from_local_file(path)\n",
    "\n",
    "# Load data from a JSON document referenced by a GS URI\n",
    "def _load_json_from_gs_uri(uri: str) -> Dict[str, Any]:\n",
    "    \"\"\"Loads data from a JSON document referenced by a GS URI.\n",
    "\n",
    "    Args:\n",
    "      uri: The GCS URI of the JSON document.\n",
    "\n",
    "    Returns:\n",
    "      A deserialized Dict object representing the JSON document.\n",
    "\n",
    "    Raises:\n",
    "      google.cloud.exceptions.NotFound: If the blob is not found.\n",
    "      json.decoder.JSONDecodeError: On JSON parsing problems.\n",
    "      ValueError: If uri is not a valid gs URI.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    blob = storage.Blob.from_string(uri, storage_client)\n",
    "    return json.loads(blob.download_as_bytes())\n",
    "\n",
    "# Load data from a JSON local file\n",
    "def _load_json_from_local_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Loads data from a JSON local file.\n",
    "\n",
    "    Args:\n",
    "      file_path: The local file path of the JSON document.\n",
    "\n",
    "    Returns:\n",
    "      A deserialized Dict object representing the JSON document.\n",
    "\n",
    "    Raises:\n",
    "      json.decoder.JSONDecodeError: On JSON parsing problems.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Get the scheduler job details\n",
    "def get_job(job_name):\n",
    "    # Create a client\n",
    "    client = scheduler_v1.CloudSchedulerClient()\n",
    "\n",
    "    # Initialize request argument(s)\n",
    "    request = scheduler_v1.GetJobRequest(\n",
    "        name=job_name,\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    response = client.get_job(request=request)\n",
    "\n",
    "    # Handle the response\n",
    "    #print(response)\n",
    "\n",
    "# Delete scheduler job by job-name\n",
    "def delete_job(job_name):\n",
    "    # Create a client\n",
    "    client = scheduler_v1.CloudSchedulerClient()\n",
    "\n",
    "    # Initialize request argument(s)\n",
    "    request = scheduler_v1.DeleteJobRequest(\n",
    "        name=job_name,\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    client.delete_job(request=request)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7d2e65f-34a7-45dd-bfd6-5b813ae3a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing schedule.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile schedule.py\n",
    "\n",
    "import base64\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import re\n",
    "import tempfile\n",
    "from typing import Any, Mapping, Optional\n",
    "import zipfile\n",
    "\n",
    "import googleapiclient\n",
    "from googleapiclient import discovery\n",
    "import requests\n",
    "from google.cloud import resourcemanager_v3\n",
    "\n",
    "from kfp.v2.google.client import client_utils\n",
    "from kfp.v2.google.client import runtime_config_builder\n",
    "\n",
    "_PROXY_FUNCTION_NAME = 'rahul_templated_http_request-v1'\n",
    "# _PROXY_FUNCTION_FILENAME = '_cloud_function_templated_http_request.py'\n",
    "\n",
    "_CAIPP_ENDPOINT_WITHOUT_REGION = 'aiplatform.googleapis.com'\n",
    "_CAIPP_API_VERSION = 'v1beta1'\n",
    "\n",
    "def _get_cloud_functions_api():\n",
    "    functions_service = discovery.build(\n",
    "        'cloudfunctions', 'v1', cache_discovery=False)\n",
    "    functions_api = functions_service.projects().locations().functions()\n",
    "    return functions_api\n",
    "\n",
    "def _create_or_get_cloud_function(\n",
    "    name: str,\n",
    "    cloud_scheduler_service_account: str,\n",
    "    cloud_function_project_id: str,\n",
    "    region: str,\n",
    "    runtime: str = 'python37',\n",
    "):\n",
    "    \n",
    "    \"\"\"Creates Google Cloud Function.\"\"\"\n",
    "    functions_api = _get_cloud_functions_api()\n",
    "\n",
    "    project_location_path = 'projects/{}/locations/{}'.format(\n",
    "        cloud_function_project_id, region)\n",
    "    function_full_name = project_location_path + '/functions/' + name\n",
    "    \n",
    "    print(\"CLOUD FUNCTION FULL NAME :\", function_full_name)\n",
    "    \n",
    "    # Returning early if the function already exists.\n",
    "    try:\n",
    "        function_get_response = functions_api.get(\n",
    "            name=function_full_name).execute()\n",
    "        \n",
    "        # print(\"[BEFORE]: function_get_response\", function_get_response)\n",
    "        \n",
    "        return function_get_response\n",
    "    except googleapiclient.errors.HttpError as err:\n",
    "        raise_error = True\n",
    "        if err.resp['status'] == '404':\n",
    "            # The function does not exist, which is expected.\n",
    "            logging.info('[\"ERROR\"] Cloud Function: name=%s does not exist', function_full_name)\n",
    "            raise_error = False\n",
    "        if raise_error:\n",
    "            raise err\n",
    "        return False\n",
    "    \n",
    "    return function_get_response\n",
    "\n",
    "def _get_proxy_cloud_function_endpoint(\n",
    "    cloud_function_project_id: str,\n",
    "    cloud_scheduler_service_account: str,\n",
    "    region: str = 'us-central1',\n",
    "):\n",
    "    function_dict = _create_or_get_cloud_function(\n",
    "        name=_PROXY_FUNCTION_NAME,\n",
    "        cloud_function_project_id=cloud_function_project_id,\n",
    "        region=region,\n",
    "        runtime='python37',\n",
    "        cloud_scheduler_service_account=cloud_scheduler_service_account,\n",
    "    )\n",
    "\n",
    "    if not function_dict:\n",
    "        print(\"Exiting the code\")\n",
    "        import sys\n",
    "        sys.exit()\n",
    "        \n",
    "    else:\n",
    "        endpoint_url = function_dict['httpsTrigger']['url']\n",
    "        return endpoint_url\n",
    "\n",
    "def _enable_required_apis(project_id: str,):\n",
    "    \"\"\"Enables necessary APIs.\"\"\"\n",
    "    serviceusage_service = discovery.build('serviceusage', 'v1', cache_discovery=False)\n",
    "    services_api = serviceusage_service.services()\n",
    "\n",
    "    required_services = [\n",
    "        'cloudfunctions.googleapis.com',\n",
    "        'cloudscheduler.googleapis.com',\n",
    "        'appengine.googleapis.com',  # Required by the Cloud Scheduler.\n",
    "    ]\n",
    "    project_path = 'projects/' + project_id\n",
    "    for service_name in required_services:\n",
    "        service_path = project_path + '/services/' + service_name\n",
    "        services_api.enable(name=service_path).execute()\n",
    "        \n",
    "def _create_scheduler_job(project_location_path: str,\n",
    "                          job_body: Mapping[str, Any]) -> str:\n",
    "    \"\"\"Creates a scheduler job.\n",
    "\n",
    "    Args:\n",
    "      project_location_path: The project location path.\n",
    "      job_body: The scheduled job dictionary object.\n",
    "\n",
    "    Returns:\n",
    "      The response from scheduler service.\n",
    "    \"\"\"\n",
    "    # We cannot use google.cloud.scheduler_v1.CloudSchedulerClient since\n",
    "    # it's not available internally.\n",
    "    scheduler_service = discovery.build(\n",
    "        'cloudscheduler', 'v1', cache_discovery=False)\n",
    "    scheduler_jobs_api = scheduler_service.projects().locations().jobs()\n",
    "    response = scheduler_jobs_api.create(\n",
    "        parent=project_location_path,\n",
    "        body=job_body,\n",
    "    ).execute()\n",
    "    return response\n",
    "\n",
    "\n",
    "def _create_from_pipeline_dict(\n",
    "    schedule: str,\n",
    "    project_id: str,\n",
    "    cloud_function_project_id: str,\n",
    "    cloud_scheduler_service_account: str,\n",
    "    pipeline_dict: dict = None,\n",
    "    parameter_values: Optional[Mapping[str, Any]] = None,\n",
    "    pipeline_root: Optional[str] = None,\n",
    "    service_account: Optional[str] = None,\n",
    "    app_engine_region: Optional[str] = None,\n",
    "    scheduler_job_name: Optional[str] = None,\n",
    "    region: str = 'us-central1',\n",
    "    time_zone: str = 'US/Pacific'\n",
    ") -> dict:\n",
    "    \"\"\"Creates schedule for compiled pipeline dictionary.\"\"\"\n",
    "\n",
    "    _enable_required_apis(project_id=project_id)\n",
    "\n",
    "    # If appengine region is not provided, use the pipeline region.\n",
    "    app_engine_region = app_engine_region or region\n",
    "\n",
    "    proxy_function_url = _get_proxy_cloud_function_endpoint(\n",
    "        cloud_function_project_id=cloud_function_project_id,\n",
    "        region=region,\n",
    "        cloud_scheduler_service_account=cloud_scheduler_service_account,\n",
    "    )\n",
    "\n",
    "    if parameter_values or pipeline_root:\n",
    "        config_builder = runtime_config_builder.RuntimeConfigBuilder.from_job_spec_json(\n",
    "            pipeline_dict)\n",
    "        config_builder.update_runtime_parameters(\n",
    "            parameter_values=parameter_values)\n",
    "        config_builder.update_pipeline_root(pipeline_root=pipeline_root)\n",
    "        updated_runtime_config = config_builder.build()\n",
    "        pipeline_dict['runtimeConfig'] = updated_runtime_config\n",
    "\n",
    "    # Creating job creation request to get the final request URL\n",
    "    pipeline_jobs_api_url = f'https://{region}-{_CAIPP_ENDPOINT_WITHOUT_REGION}/{_CAIPP_API_VERSION}/projects/{project_id}/locations/{region}/pipelineJobs'\n",
    "    \n",
    "    # Preparing the request body for the Cloud Function processing\n",
    "    pipeline_name = pipeline_dict['pipelineSpec']['pipelineInfo']['name']\n",
    "    full_pipeline_name = 'projects/{}/pipelineJobs/{}'.format(project_id, pipeline_name)\n",
    "    pipeline_display_name = pipeline_dict.get('displayName')\n",
    "    time_format_suffix = \"-{{$.scheduledTime.strftime('%Y-%m-%d-%H-%M-%S')}}\"\n",
    "    if 'name' in pipeline_dict:\n",
    "        pipeline_dict['name'] += time_format_suffix\n",
    "    if 'displayName' in pipeline_dict:\n",
    "        pipeline_dict['displayName'] += time_format_suffix\n",
    "\n",
    "    pipeline_dict['_url'] = pipeline_jobs_api_url\n",
    "    pipeline_dict['_method'] = 'POST'\n",
    "\n",
    "    if service_account is not None:\n",
    "        pipeline_dict['serviceAccount'] = service_account\n",
    "\n",
    "    pipeline_text = json.dumps(pipeline_dict)\n",
    "    pipeline_data = pipeline_text.encode('utf-8')\n",
    "\n",
    "#================ CUSTOM ==============================\n",
    "    # pipeline_dict = {\"pipeline_spec_uri\":\"gs://pipeline-schedule/intro_pipeline.json\"}\n",
    "    # pipeline_text = json.dumps(pipeline_dict)\n",
    "    # pipeline_data = pipeline_text.encode('utf-8')\n",
    "\n",
    "# ======================================================\n",
    "\n",
    "    project_location_path = 'projects/{}/locations/{}'.format(\n",
    "        project_id, app_engine_region)\n",
    "    scheduled_job_full_name = '{}/jobs/{}'.format(project_location_path,\n",
    "                                                  scheduler_job_name)\n",
    "    service_account_email = cloud_scheduler_service_account or '{}@appspot.gserviceaccount.com'.format(\n",
    "        project_id)\n",
    "\n",
    "    scheduled_job = dict(\n",
    "        name=scheduled_job_full_name,  # Optional. Only used for readable names.\n",
    "        schedule=schedule,\n",
    "        time_zone=time_zone,\n",
    "        http_target=dict(\n",
    "            http_method='POST',\n",
    "            uri=proxy_function_url,\n",
    "            # Warning: when using google.cloud.scheduler_v1, the type of body is\n",
    "            # bytes or string. But when using the API through discovery, the body\n",
    "            # needs to be base64-encoded.\n",
    "            body=base64.b64encode(pipeline_data).decode('utf-8'),\n",
    "            oidc_token=dict(service_account_email=service_account_email,audience = proxy_function_url),\n",
    "        ),\n",
    "        # TODO(avolkov): Add labels once Cloud Scheduler supports them\n",
    "        # labels={\n",
    "        #     'google.cloud.ai-platform.pipelines.scheduling': 'v1alpha1',\n",
    "        # },\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = _create_scheduler_job(\n",
    "            project_location_path=project_location_path,\n",
    "            job_body=scheduled_job,\n",
    "        )\n",
    "        return response\n",
    "    except googleapiclient.errors.HttpError as err:\n",
    "        # Handling the case where the exact schedule already exists.\n",
    "        if err.resp.get('status') == '409':\n",
    "            raise RuntimeError(\n",
    "                'The exact same schedule already exists') from err\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "565269d4-ef9d-469d-aa87-a0f00b087cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile client.py\n",
    "\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "from schedule import _create_from_pipeline_dict\n",
    "from utils import load_json, get_job, delete_job\n",
    "\n",
    "class aipclient_custom:\n",
    "    def __init__(self, project_id: str = None,region: str = None):\n",
    "        self.project_id = project_id\n",
    "        self.region = region\n",
    "    \n",
    "    def create_schedule_from_job_spec(self,\n",
    "        job_spec_path: str = None,\n",
    "        schedule: str = \"* * * * *\",\n",
    "        time_zone: str = 'US/Pacific',\n",
    "        pipeline_root: Optional[str] = None,\n",
    "        parameter_values: Optional[Mapping[str, Any]] = None,\n",
    "        cloud_function_project_id: str = None,\n",
    "        service_account: Optional[str] = None,\n",
    "        enable_caching: Optional[bool] = None,\n",
    "        app_engine_region: Optional[str] = None,\n",
    "        cloud_scheduler_service_account: Optional[str] = None,\n",
    "        job_name: Optional[str] = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"Creates schedule for compiled pipeline file.\n",
    "\n",
    "        This function creates scheduled job which will run the provided pipeline on\n",
    "        schedule. This is implemented by creating a Google Cloud Scheduler Job.\n",
    "        The job will be visible in https://console.google.com/cloudscheduler and can\n",
    "        be paused/resumed and deleted.\n",
    "\n",
    "        To make the system work, this function also creates a Google Cloud Function\n",
    "        which acts as an intermediary between the Scheduler and Pipelines. A single\n",
    "        function is shared between all scheduled jobs.\n",
    "        The following APIs will be activated automatically:\n",
    "        * cloudfunctions.googleapis.com\n",
    "        * cloudscheduler.googleapis.com\n",
    "        * appengine.googleapis.com\n",
    "\n",
    "        Args:\n",
    "          job_spec_path: Path of the compiled pipeline file.\n",
    "          schedule: Schedule in cron format. Example: \"45 * * * *\"\n",
    "          time_zone: Schedule time zone. Default is 'US/Pacific'\n",
    "          parameter_values: Arguments for the pipeline parameters\n",
    "          pipeline_root: Optionally the user can override the pipeline root\n",
    "            specified during the compile time.\n",
    "          service_account: The service account that the pipeline workload runs as.\n",
    "          enable_caching: Whether or not to enable caching for the run.\n",
    "            If not set, defaults to the compile time settings, which are True for all\n",
    "            tasks by default, while users may specify different caching options for\n",
    "            individual tasks.\n",
    "            If set, the setting applies to all tasks in the pipeline -- overrides\n",
    "            the compile time settings.\n",
    "          app_engine_region: The region that cloud scheduler job is created in.\n",
    "          cloud_scheduler_service_account: The service account that Cloud Scheduler job and the proxy cloud function use.\n",
    "            this should have permission to call AI Platform API and the proxy function.\n",
    "            If not specified, the functions uses the App Engine default service account.\n",
    "\n",
    "        Returns:\n",
    "          Created Google Cloud Scheduler Job object dictionary.\n",
    "        \"\"\"\n",
    "        if job_spec_path is not None:\n",
    "            job_spec = load_json(job_spec_path)\n",
    "        \n",
    "        if cloud_function_project_id is None:\n",
    "            cloud_function_project_id = self.project_id\n",
    "        if enable_caching is not None:\n",
    "            _set_enable_caching_value(job_spec['pipelineSpec'], enable_caching)\n",
    "        \n",
    "        # To check if a job already exists with the same name\n",
    "        self.check_job(job_name)\n",
    "        \n",
    "        scheduler_job_name = job_name.split(\"/\")[-1]\n",
    "        # Create new job\n",
    "        return _create_from_pipeline_dict(\n",
    "            pipeline_dict=job_spec,\n",
    "            schedule=schedule,\n",
    "            project_id=self.project_id,\n",
    "            cloud_function_project_id = cloud_function_project_id, #cloud-function_project_id\n",
    "            region=self.region,\n",
    "            time_zone=time_zone,\n",
    "            parameter_values=parameter_values,\n",
    "            pipeline_root=pipeline_root,\n",
    "            service_account=service_account,\n",
    "            app_engine_region=app_engine_region,\n",
    "            cloud_scheduler_service_account=cloud_scheduler_service_account,\n",
    "            scheduler_job_name = scheduler_job_name)\n",
    "    \n",
    "    def check_job(self,job_name):\n",
    "        if job_name is not None:\n",
    "            try:\n",
    "                get_job(job_name)\n",
    "                print(\"Job already exists. Deleting..\")\n",
    "                delete_job(job_name)\n",
    "                print(\"Creating a new scheduler job\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Job doesn't exist\")\n",
    "        \n",
    "        else:\n",
    "            print(\"Creating a new scheduler job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ac1f225-7a9e-4e05-bb6a-4cd6390d9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_job_name=\"Cloud_function_scheduler_job\" # Any cloud scheduler job name\n",
    "\n",
    "from client import aipclient_custom\n",
    "\n",
    "api_client = aipclient_custom(PROJECT_ID,REGION)\n",
    "\n",
    "CLOUD_FUNCTION_PROJECT_ID= PROJECT_ID #\"<Enter the cloud function project id>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d0db439-1c97-4714-8dbb-e842077dadb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/oyi-nosales-model-pipeline-dev.json'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TMP_PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddc957e2-65da-46fa-bbf9-27e8c5095b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job doesn't exist\n",
      "CLOUD FUNCTION FULL NAME : projects/wmt-mlp-p-oyi-ds-or-oyi-dsns/locations/us-central1/functions/rahul_templated_http_request-v1\n",
      "Exiting the code\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "api_client.create_schedule_from_job_spec(job_spec_path=TMP_PIPELINE_JSON, #\"intro_pipeline.json\",\n",
    "    schedule=\"* * * * *\",\n",
    "    cloud_function_project_id = CLOUD_FUNCTION_PROJECT_ID,\n",
    "    time_zone=\"America/Los_Angeles\",  # change this as necessary\n",
    "    parameter_values={\"text\": \"Hello world!\"},\n",
    "    service_account=SERVICE_ACCOUNT, # to run the vertex pipeline\n",
    "    cloud_scheduler_service_account = SERVICE_ACCOUNT,\n",
    "    job_name = f\"projects/{PROJECT_ID}/locations/{REGION}/jobs/{scheduler_job_name}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6dc47-81a4-4a88-868b-edbae17fc3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1b5ec-81e1-4644-8462-a902f7576985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34564e01-cd8b-4bde-995c-7a4f851c2bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e2748-e40f-4d39-b86e-c311becebd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b8001-8e0a-4204-9886-0023d0472ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12807c-9a2d-4fd5-939e-58a9a91f8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abdccc-552f-4db3-89c5-9c4073835245",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth list 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc729fd-2cd8-4564-aa5b-9e5314db2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460e641-608f-4d65-bb87-c74150a28e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output[2].replace(\"*\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b066402-374a-491f-a658-e5962093619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output[2].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd3dd5-8b89-4e76-ae37-4c45447ecdef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
