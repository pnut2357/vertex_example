{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire Pipeline\n",
    "data-preprocessing >> [train-test-split, train-eval-model] >> element-mlflow-model-registry, update-thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d75fbeca-a613-4cd4-8ea2-4c804747240c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Imports for vertex pipeline\n",
    "from google.cloud import aiplatform\n",
    "import google_cloud_pipeline_components\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    component,\n",
    "    pipeline,\n",
    "    Input,\n",
    "    Output,\n",
    "    Model,\n",
    "    Dataset,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    ")\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "# import c_utils\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(str(Path(\".\").absolute().parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent) + \"/utils\")\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent))\n",
    "sys.path.append(str(Path(\".\").absolute().parent.parent.parent))\n",
    "\n",
    "import pipeline_utils\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --MODE MODE --STAGE1_FLAG STAGE1_FLAG\n",
      "                             --ENSEMBLE_FLAG ENSEMBLE_FLAG --RF_CLF_MODEL_PATH\n",
      "                             RF_CLF_MODEL_PATH --LOGISTIC_CLF_MODEL_PATH\n",
      "                             LOGISTIC_CLF_MODEL_PATH --STAGE1_NN_MODEL_PATH\n",
      "                             STAGE1_NN_MODEL_PATH --GNB_MODEL_PATH\n",
      "                             GNB_MODEL_PATH --STG1_FEATURE_SELECTOR_MODEL_PATH\n",
      "                             STG1_FEATURE_SELECTOR_MODEL_PATH\n",
      "                             --NOSALES_MODEL_PATH NOSALES_MODEL_PATH\n",
      "ipykernel_launcher.py: error: the following arguments are required: --MODE, --STAGE1_FLAG, --ENSEMBLE_FLAG, --RF_CLF_MODEL_PATH, --LOGISTIC_CLF_MODEL_PATH, --STAGE1_NN_MODEL_PATH, --GNB_MODEL_PATH, --STG1_FEATURE_SELECTOR_MODEL_PATH, --NOSALES_MODEL_PATH\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    args = pipeline_utils.get_args()\n",
    "except:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--MODE\", required=True, type=str)\n",
    "    parser.add_argument(\"--STAGE1_FLAG\", required=True, type=str)\n",
    "    parser.add_argument(\"--ENSEMBLE_FLAG\", required=True, type=str)\n",
    "    parser.add_argument(\"--RF_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--LOGISTIC_CLF_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--STAGE1_NN_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--GNB_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--STG1_FEATURE_SELECTOR_MODEL_PATH\", required=True, type=str)\n",
    "    parser.add_argument(\"--NOSALES_MODEL_PATH\", required=True, type=str)\n",
    "    sys.args = [\n",
    "        \"--MODE\", \"test\",\n",
    "        \"--STAGE1_FLAG\", \"train\",\n",
    "        \"--ENSEMBLE_FLAG\", \"train\",\n",
    "        \"--RF_CLF_MODEL_PATH\", \"\",\n",
    "        \"--LOGISTIC_CLF_MODEL_PATH\", \"\",\n",
    "        \"--STAGE1_NN_MODEL_PATH\", \"\",\n",
    "        \"--GNB_MODEL_PATH\", \"\",\n",
    "        \"--STG1_FEATURE_SELECTOR_MODEL_PATH\", \"\",\n",
    "        \"--NOSALES_MODEL_PATH\", \"\",\n",
    "    ]\n",
    "    args = parser.parse_args(sys.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV: dev, \n",
      "PROJECT_ID: wmt-mlp-p-oyi-ds-or-oyi-dsns, \n",
      "BASE_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/oyi-vertex-pipeline-dev:latest, \n",
      "MLFLOW_IMAGE: gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/mlflow-image-dev:latest, \n",
      "PIPELINE_NAME: oyi-nosales-model-pipeline-dev, \n",
      "PIPELINE_JSON: oyi-nosales-model-pipeline-dev.json\n"
     ]
    }
   ],
   "source": [
    "PARAMS = pipeline_utils.yaml_import('settings.yml')\n",
    "\n",
    "ENV = PARAMS['env_flag']\n",
    "\n",
    "PROJECT_ID = PARAMS['envs'][ENV]['PROJECT_ID']\n",
    "REGION = PARAMS['envs'][ENV]['REGION']\n",
    "BASE_IMAGE = PARAMS['envs'][ENV]['BASE_IMAGE']\n",
    "MLFLOW_IMAGE = PARAMS['envs'][ENV]['MLFLOW_IMAGE']\n",
    "\n",
    "PIPELINE_ROOT = PARAMS['envs'][ENV]['PIPELINE_ROOT']\n",
    "PIPELINE_NAME = PARAMS['envs'][ENV]['PIPELINE_NAME']\n",
    "PIPELINE_JSON = PARAMS['envs'][ENV]['PIPELINE_JSON']\n",
    "TMP_PIPELINE_JSON = os.path.join(\"/tmp\", PIPELINE_JSON)\n",
    "\n",
    "\n",
    "TRAINING_TABLE_NAME = PARAMS['envs'][ENV]['TRAINING_TABLE_NAME']\n",
    "TRAINING_DATA_BQ_QUERY = f'select * from {TRAINING_TABLE_NAME} LIMIT 10000' #f'select * from {TRAINING_TABLE_NAME}'  \n",
    "\n",
    "MLFLOW_EXP_NAME = PARAMS['envs'][ENV]['MLFLOW_EXP_NAME']\n",
    "MODEL_REGISTRY_NAME = PARAMS['envs'][ENV]['MODEL_REGISTRY_NAME']\n",
    "\n",
    "SERVICE_ACCOUNT = PARAMS['envs'][ENV]['SERVICE_ACCOUNT']\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    " \n",
    "# Matches on non-word, non-regular-punctuation characters.\n",
    "MATCHER = r\"\"\"[^a-zA-Z0-9'\"!@#$%\\^&*()\\[\\]{}:;<>?,.-=_+ ]+\"\"\" \n",
    "\n",
    "CLUB_THRESH_PATH = PARAMS['envs'][ENV]['CLUB_THRESH_PATH']\n",
    "LATEST_NOSALES_MODEL_PATH = PARAMS['envs'][ENV]['LATEST_NOSALES_MODEL_PATH']\n",
    "LATEST_PIPELINE_PATH = PARAMS['envs'][ENV]['LATEST_PIPELINE_PATH']\n",
    "RUN_PIPELINE = PARAMS['envs'][ENV]['RUN_PIPELINE']\n",
    "print(f\"ENV: {ENV}, \\nPROJECT_ID: {PROJECT_ID}, \\nBASE_IMAGE: {BASE_IMAGE}, \\nMLFLOW_IMAGE: {MLFLOW_IMAGE}, \\nPIPELINE_NAME: {PIPELINE_NAME}, \\nPIPELINE_JSON: {PIPELINE_JSON}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LATEST_NOSALES_MODEL_PATH: {LATEST_NOSALES_MODEL_PATH}, \\nLATEST_PIPELINE_PATH: {LATEST_PIPELINE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEST_PIPELINE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check1(\n",
    "#     training_data_bq_query_input: str,\n",
    "#     matcher: str,\n",
    "#     project_id: str,\n",
    "#     env: str,\n",
    "#     pipeline_root: str):\n",
    "    \n",
    "#     import pandas as pd\n",
    "#     from datetime import timedelta\n",
    "#     import utils\n",
    "#     from google.cloud import bigquery\n",
    "\n",
    "#     client = bigquery.Client(project=project_id)\n",
    "#     data = client.query(training_data_bq_query_input).to_dataframe()\n",
    "#     nosales_data = data[\n",
    "#       (data.report_type!='C') &\n",
    "#       (data.display_ind == \"Display\") &\n",
    "#       (data.oh_qty>=0)]\n",
    "#     nosales_data[\"item_desc\"] = nosales_data['item_desc'].str.replace(matcher, \"\", regex=True)\n",
    "#     nosales_data['run_date'] = pd.to_datetime(nosales_data['run_date'])\n",
    "#     max_date = nosales_data['run_date'].max()\n",
    "#     cutoff_date = (max_date - timedelta(days=182)).strftime('%Y-%m-%d')\n",
    "#     nosales_data = nosales_data[nosales_data.run_date > cutoff_date]\n",
    "    \n",
    "#     nosales_data.replace(\"No Action Taken, already OFS\", \"No Action Taken, already out for sale\", inplace=True)\n",
    "#     nosales_data.replace('Updated the NOSALES type with scrubber event', \"No Action Taken, already out for sale\", inplace=True)\n",
    "#     nosales_data.sort_values(by = ['run_date','club_nbr','item_nbr','event_ts'],inplace = True)\n",
    "#     nosales_data.drop_duplicates(['old_nbr','club_nbr','run_date'], keep='first',inplace = True)\n",
    "    \n",
    "#     nosales_ext = c_utils.calculate_all_level_tpr(df=nosales_data, env=env, pipeline_root=pipeline_root, path='', save=True) #calculate_all_level_tpr(nosales_data, env, pipeline_root, save=True) \n",
    "#     nosales_ext.fillna(0, inplace=True)\n",
    "#     print(nosales_ext)\n",
    "#     # nosales_ext.to_csv(training_data_output.path, index=False)\n",
    "    \n",
    "# check1(training_data_bq_query_input = f'select * from {TRAINING_TABLE_NAME} LIMIT 10000',\n",
    "#        matcher=MATCHER,\n",
    "#        project_id=PROJECT_ID, \n",
    "#        env=ENV, \n",
    "#        pipeline_root=PIPELINE_ROOT)\n",
    "       \n",
    "# # def check(nosales_ext_input: Input[Dataset],\n",
    "# #           _training_data_bq_query_input=TRAINING_DATA_BQ_QUERY: str,\n",
    "# #           _matcher=MATCHER: str,\n",
    "# #           _project_id=PROJECT_ID: str,\n",
    "# #           _env=ENV: str,\n",
    "# #           _pipeline_root=PIPELINE_ROOT: str):\n",
    "    \n",
    "    \n",
    "# #     data = data_preprocessing(_training_data_bq_query_input,\n",
    "# #                               _matcher,\n",
    "# #                               _project_id, \n",
    "# #                               _env, \n",
    "# #                               _pipeline_root)\n",
    "# #     train_test_data = train_test_split(nosales_ext_input=data.outputs['training_data_output'])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# # nosales_train_ext[nosales_train_ext.event_note == \"No Action Taken, already out for sale\"].groupby('club_nbr')\n",
    "# # train_test_data = train_test_split(nosales_ext_input=data.outputs['training_data_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data_preprocessing(training_data_bq_query_input = TRAINING_DATA_BQ_QUERY,\n",
    "#                               matcher=MATCHER,\n",
    "#                               project_id = PROJECT_ID, env=ENV, pipeline_root=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data():\n",
    "    import pandas as pd\n",
    "    training_data_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/data-preprocessing_-8891711501161725952/training_data_output\"\n",
    "    training_data = pd.read_csv(training_data_path)\n",
    "    print(training_data.head(3))\n",
    "    \n",
    "train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_data_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/data-preprocessing_-8891711501161725952/training_data_output\"\n",
    "training_data = pd.read_csv(training_data_path)\n",
    "\n",
    "nosales_train_ext_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-test-split_-4280025482734338048/nosales_train_ext_output\"\n",
    "nosales_train_ext = pd.read_csv(nosales_train_ext_path)\n",
    "\n",
    "nosales_test_ext_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-test-split_-4280025482734338048/nosales_test_ext_output\"\n",
    "nosales_test_ext = pd.read_csv(nosales_test_ext_path)\n",
    "\n",
    "nosales_train_usampled_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-test-split_-4280025482734338048/nosales_train_usampled_output\"\n",
    "nosales_train_usampled = pd.read_csv(nosales_train_usampled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(base_image=BASE_IMAGE)\n",
    "def test1(out: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    \n",
    "    data = {\n",
    "      \"calories\": [420, 380, 390],\n",
    "      \"duration\": [50, 40, 45]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(out.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/data-preprocessing_-8891711501161725952/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_data_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/data-preprocessing_-8891711501161725952/training_data_output\"\n",
    "training_data = pd.read_csv(training_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nosales_train_ext_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-test-split_-4280025482734338048/nosales_train_ext_output\"\n",
    "nosales_train_ext = pd.read_csv(nosales_train_ext_path)\n",
    "nosales_train_ext.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREWORK_IMAGE=\"gcr.io/wmt-mlp-p-oyi-ds-or-oyi-dsns/prework_test:0.0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(packages_to_install=[\"pandas\", \"fsspec\", \"gcsfs\"])\n",
    "# @component(packages_to_install=[\"pandas==1.1.4\", \"fsspec\", \"gcsfs\"])\n",
    "# @component(base_image=PREWORK_IMAGE)\n",
    "# @component(packages_to_install=[\"pandas==1.1.4\", \"fsspec\", \"gcsfs\"])\n",
    "@component(base_image=PREWORK_IMAGE)\n",
    "def prework(pipeline_root:str,\n",
    "            training_data_output: Output[Dataset],\n",
    "            nosales_train_ext_output: Output[Dataset],\n",
    "            nosales_test_ext_output: Output[Dataset],\n",
    "            nosales_train_usampled_output: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    \n",
    "    training_data_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/data-preprocessing_-8891711501161725952/training_data_output\"\n",
    "    training_data = pd.read_csv(training_data_path)\n",
    "    \n",
    "    nosales_train_ext_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-test-split_-4280025482734338048/nosales_train_ext_output\"\n",
    "    nosales_train_ext = pd.read_csv(nosales_train_ext_path)\n",
    "    \n",
    "    nosales_test_ext_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-test-split_-4280025482734338048/nosales_test_ext_output\"\n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_path)\n",
    "    \n",
    "    nosales_train_usampled_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-test-split_-4280025482734338048/nosales_train_usampled_output\"\n",
    "    nosales_train_usampled = pd.read_csv(nosales_train_usampled_path)\n",
    "    \n",
    "    training_data.to_csv(training_data_output.path, index=False)\n",
    "    nosales_train_ext.to_csv(nosales_train_ext_output.path, index=False)\n",
    "    nosales_test_ext.to_csv(nosales_test_ext_output.path, index=False)\n",
    "    nosales_train_usampled.to_csv(nosales_train_usampled_output.path, index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be3b59b6-65c2-465f-827d-9edca7b83947",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def train_eval_model(\n",
    "    nosales_ext_input: Input[Dataset],\n",
    "    nosales_train_ext_input: Input[Dataset],\n",
    "    nosales_test_ext_input: Input[Dataset],\n",
    "    nosales_train_usampled_input: Input[Dataset],\n",
    "    mode: str,\n",
    "    stage1_flag: str,\n",
    "    ensemble_flag: str,\n",
    "    rf_clf_model_path_input: str,\n",
    "    logistic_clf_model_path_input: str,\n",
    "    stage1_nn_model_path_input: str,\n",
    "    gnb_model_path_input: str,\n",
    "    stg1_feature_selector_model_path_input: str,\n",
    "    nosales_model_path_input: str,\n",
    "    latest_nosales_model_path_input: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    timestamp: str,\n",
    "    rf_clf_model_output: Output[Model],\n",
    "    logistic_clf_model_output: Output[Model],\n",
    "    stage1_nn_model_output: Output[Model],\n",
    "    gnb_model_output: Output[Model],\n",
    "    stg1_feature_selector_model_output: Output[Model],\n",
    "    nosales_model_output: Output[Model],\n",
    "    nosales_test_ext_output: Output[Dataset]\n",
    "):\n",
    "    import os \n",
    "    import pandas as pd\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    import utils\n",
    "    import diagnosis_utils\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.cluster import KMeans\n",
    "    import pickle\n",
    "    from google.cloud import storage, aiplatform\n",
    "    \n",
    "    nosales_ext = pd.read_csv(nosales_ext_input.path)\n",
    "    nosales_train_ext = pd.read_csv(nosales_train_ext_input.path)\n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "    nosales_train_usampled = pd.read_csv(nosales_train_usampled_input.path)\n",
    "    \n",
    "    nosales_ext['run_date'] = pd.to_datetime(nosales_ext['run_date'])\n",
    "    nosales_train_ext['run_date'] = pd.to_datetime(nosales_train_ext['run_date'])\n",
    "    nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "    nosales_train_usampled['run_date'] = pd.to_datetime(nosales_train_usampled['run_date'])\n",
    "    \n",
    "    tpr_features = [col for col in nosales_train_ext.columns if '_tpr' in col]  # len(tpr_features) : 45\n",
    "\n",
    "    numerical_features= ['gap_days','exp_scn_in_nosale_period', 'unit_retail','oh_qty','avg_sales_interval']\n",
    "    numerical_features.extend(tpr_features)\n",
    "    categorical_features = ['club_nbr','state','cat']\n",
    "\n",
    "    all_features = numerical_features + categorical_features\n",
    "    target = ['event_note']\n",
    "\n",
    "    top_features = list(['oh_qty_log',  'club_nbr_cat_update_loc_tpr_log',  'club_nbr_cat_new_price_sign_tpr_log',  'club_nbr_update_loc_tpr_log',\n",
    "    'club_nbr_new_price_sign_tpr_log',  'club_nbr_cat_add_to_picklist_tpr_log',  'item_nbr_update_ohq_tpr_log',\n",
    "    'item_nbr_add_to_picklist_tpr_log',  'club_nbr_add_to_picklist_tpr_log',  'avg_sales_interval_log', \n",
    "    'club_nbr_cat_no_action_taken_tpr_log',  'club_nbr_no_action_taken_tpr_log',  'item_nbr_no_action_taken_tpr_log',\n",
    "    'cat_add_to_picklist_tpr_log',  'unit_retail_log',  'exp_scn_in_nosale_period_log',  'club_nbr_cat_update_ohq_tpr_log', \n",
    "    'cat_update_ohq_tpr_log',  'club_nbr_update_ohq_tpr_log',  'state_cat_add_to_picklist_tpr_log',  'reg_cat_update_ohq_tpr_log',\n",
    "    'state_cat_new_price_sign_tpr_log',  'mkt_cat_new_price_sign_tpr_log',  'mkt_cat_update_ohq_tpr_log', \n",
    "    'reg_cat_add_to_picklist_tpr_log',  'state_cat_update_ohq_tpr_log',  'cat_new_price_sign_tpr_log', \n",
    "    'mkt_cat_update_loc_tpr_log',  'mkt_update_loc_tpr_log',  'mkt_new_price_sign_tpr_log', \n",
    "    'mkt_cat_add_to_picklist_tpr_log',  'mkt_no_action_taken_tpr_log',  'reg_no_action_taken_tpr_log', \n",
    "    'cat_no_action_taken_tpr_log',  'mkt_cat_no_action_taken_tpr_log',  'state_cat_update_loc_tpr_log', \n",
    "    'gap_days_log',  'reg_new_price_sign_tpr_log',  'mkt_update_ohq_tpr_log',  'state_cat_no_action_taken_tpr_log'])\n",
    "\n",
    "    if mode == 'test':\n",
    "        verbose_flag = True\n",
    "    else:\n",
    "        verbose_flag = False\n",
    "\n",
    "\n",
    "    feature_flags = {'kmeans_clustering': False}\n",
    "\n",
    "    class_weights = dict(nosales_train_usampled.event_note.value_counts()[0]/nosales_train_usampled.event_note.value_counts()[:])\n",
    "\n",
    "\n",
    "    # pipeline: location-feat\n",
    "    location_features_tf= Pipeline([\n",
    "        ('select_loc', utils.DataFrameSelector(['sales_floor_location']))\n",
    "    ])\n",
    "\n",
    "    # pipeline: time-feat\n",
    "    time_features_tf= Pipeline([\n",
    "        ('select_rundate', utils.DataFrameSelector(['run_date'])),\n",
    "        ('time_featurize', utils.TimeExtractor())\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline: other-catg-feat\n",
    "    add_cat_tf= Pipeline([\n",
    "        ('select_other_cat', utils.DataFrameSelector(['club_nbr','cat','state']))\n",
    "    ])  \n",
    "\n",
    "\n",
    "    # pipeline: K-means clustering\n",
    "    kmeans_tf = make_pipeline(\n",
    "        utils.DataFrameSelector(numerical_features),\n",
    "        utils.MinMaxScalerTransformer(),\n",
    "        utils.ModelTransformer(KMeans(2))\n",
    "    )\n",
    "\n",
    "    ######################################## Assembling 'Catg' n 'Numeric' Features  #####################################\n",
    "\n",
    "    # list(catg pipelines)\n",
    "    list_of_pipelines_for_catg_feat = [\n",
    "        ('loc_features',location_features_tf),\n",
    "        ('time_features',time_features_tf),\n",
    "        ('other_cat_features', add_cat_tf)\n",
    "    ]\n",
    "    if feature_flags['kmeans_clustering']:\n",
    "        list_of_pipelines_for_catg_feat.append(('clusters', kmeans_tf))\n",
    "\n",
    "\n",
    "    # pipeline: encoding the catg features.\n",
    "    cat_tf = Pipeline([\n",
    "        ('combine_cats', utils.ColumnMerge(transformer_list=list_of_pipelines_for_catg_feat)),\n",
    "        ('cat_featurize', utils.CategoryFeaturizer())\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline: numeric_features + log-transformation   \n",
    "    num_features_tf= Pipeline([\n",
    "        ('select_num', utils.DataFrameSelector(numerical_features)),\n",
    "        ('log', utils.LogFeaturizer()),\n",
    "        ('select_top_features', utils.DataFrameSelector(top_features))\n",
    "    ])\n",
    "\n",
    "    stage2_init_feature_num = 20\n",
    "    num_features_tf2= Pipeline([\n",
    "        ('select_num', utils.DataFrameSelector(numerical_features)),\n",
    "        ('log', utils.LogFeaturizer()),\n",
    "        ('select_top_features', utils.DataFrameSelector(top_features[:stage2_init_feature_num]))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # all_feat => catg_feat + numerical_feat\n",
    "    add_all_tf= utils.ColumnMerge([\n",
    "        ('num_features',num_features_tf),\n",
    "        ('cat_features',cat_tf)\n",
    "    ])\n",
    "\n",
    "    ############################################################## Final pipelines ######################################################################\n",
    "\n",
    "    # Lone classifier-pipelines and pre-processors\n",
    "\n",
    "    #1\n",
    "    rf_clf = RandomForestClassifier(n_jobs=-1, criterion='gini',n_estimators=50, max_depth=7,max_features='sqrt',\n",
    "                                    class_weight = class_weights )\n",
    "\n",
    "    #2\n",
    "    logistic_clf = LogisticRegression(n_jobs=-1, multi_class='multinomial', solver='lbfgs', max_iter=1000, penalty='l2', class_weight=class_weights)\n",
    "    \n",
    "\n",
    "    #3\n",
    "    gnb = utils.CustomizedGaussianNB()\n",
    "\n",
    "    #4\n",
    "    stage1_nn = utils.Stage1_NeuralNetwork(num_classes=5, batch_size=128, epochs=25, verbose=verbose_flag)\n",
    "\n",
    "\n",
    "    stage1_classifiers = {'rf_clf':rf_clf, 'logistic_clf':logistic_clf, 'stage1_nn':stage1_nn, 'gnb':gnb}\n",
    "\n",
    "    stage2_nn_input_dimen = stage2_init_feature_num + len(stage1_classifiers)*5\n",
    "    stage2_estimator = KerasClassifier(build_fn=utils.stage2_nn, input_dimen=stage2_nn_input_dimen, epochs=5, batch_size=128, verbose=verbose_flag)\n",
    "    \n",
    "    ##set flags when in mode: 'test'#####\n",
    "    # True: if you want to save stage1 models during test. Will automatically set to False when in prod\n",
    "    s1_save_flag = True\n",
    "\n",
    "    # Stage 1 models\n",
    "    ####################################\n",
    "    # Force flag to be 'train' during prod\n",
    "    if mode == 'prod':\n",
    "        s1_save_flag = False\n",
    "        stage1_flag = 'train'\n",
    "\n",
    "    stg1_feature_selector = num_features_tf\n",
    "\n",
    "\n",
    "\n",
    "    if stage1_flag == 'train': \n",
    "        print(\"Training and saving models...\")\n",
    "        X_train = stg1_feature_selector.fit_transform(nosales_train_usampled, nosales_train_usampled.event_note)\n",
    "        X_train = X_train.astype('float128')\n",
    "        y_train = nosales_train_usampled.event_note\n",
    "        if s1_save_flag:\n",
    "            with open(stg1_feature_selector_model_output.path, 'wb') as file:  \n",
    "                pickle.dump(stg1_feature_selector, file)\n",
    "    \n",
    "        X_test= stg1_feature_selector.transform(nosales_test_ext)\n",
    "        stage1_model_output_paths = {'rf_clf':rf_clf_model_output.path, 'logistic_clf':logistic_clf_model_output.path,\n",
    "                               'stage1_nn':stage1_nn_model_output.path, 'gnb':gnb_model_output.path}\n",
    "        for clf in stage1_classifiers:\n",
    "            print(clf)\n",
    "\n",
    "            model = stage1_classifiers[clf]\n",
    "            # filename = clf + \".model\"\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            print(\"\\n\")\n",
    "            if s1_save_flag:\n",
    "                save_path = stage1_model_output_paths[clf]\n",
    "                with open(save_path, 'wb') as file:  \n",
    "                    pickle.dump(model, file)\n",
    "\n",
    "    else:\n",
    "        print(\"Loading models...\")\n",
    "        \n",
    "        with open(rf_clf_model_path_input, \"rb\") as handler:\n",
    "            rf_clf = pickle.load(handler)\n",
    "       \n",
    "        with open(logistic_clf_model_path_input, \"rb\") as handler:\n",
    "            logistic_clf = pickle.load(handler)\n",
    "        \n",
    "        with open(stage1_nn_model_path_input, \"rb\") as handler:\n",
    "            stage1_nn = pickle.load(handler)\n",
    "        \n",
    "        with open(gnb_model_path_input, \"rb\") as handler:\n",
    "            gnb = pickle.load(handler)\n",
    "       \n",
    "        stage1_classifiers = {'rf_clf':rf_clf, 'logistic_clf':logistic_clf, 'stage1_nn':stage1_nn, 'gnb':gnb}\n",
    "        \n",
    "        with open(stg1_feature_selector_model_path_input, \"rb\") as handler:\n",
    "            stg1_feature_selector = pickle.load(handler)\n",
    "        X_test= stg1_feature_selector.transform(nosales_test_ext)\n",
    "        for clf in stage1_classifiers:\n",
    "            print(clf)\n",
    "            model = stage1_classifiers[clf]\n",
    "            nosales_test_ext = diagnosis_utils.model_diag(nosales_test_ext, model.predict_proba(X_test), model.classes_)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        rf_clf_model_output.path = rf_clf_model_path_input\n",
    "        logistic_clf_model_output.path = logistic_clf_model_path_input\n",
    "        stage1_nn_model_output.path = stage1_nn_model_path_input\n",
    "        gnb_model_output.path = gnb_model_path_input\n",
    "        stg1_feature_selector_model_output.path = stg1_feature_selector_model_path_input\n",
    "        \n",
    "\n",
    "\n",
    "    # ensemble model\n",
    "    #################################################################### \n",
    "    if mode == 'test':\n",
    "        train_x = nosales_train_ext\n",
    "        train_y = nosales_train_ext.event_note\n",
    "\n",
    "    if mode == 'prod':\n",
    "        ensemble_flag = 'train'\n",
    "        train_x = nosales_ext\n",
    "        train_y = nosales_ext.event_note\n",
    "\n",
    "\n",
    "    print(mode, ensemble_flag, train_x.shape[0])  \n",
    "\n",
    "    stg2_feture_selector = num_features_tf2\n",
    "\n",
    "    if ensemble_flag == 'train': \n",
    "        print(\"Training and saving ensemble...\")\n",
    "        stack_pipeline = Pipeline([\n",
    "            ('ensemble_classifier', utils.EnsembleClassifier(stg1_feature_selector, list(stage1_classifiers.values()),\n",
    "                                                     stg2_feture_selector, stage2_estimator)) ])\n",
    "        stack_pipeline.fit(train_x, train_y)\n",
    "        with open(nosales_model_output.path, 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file)\n",
    "        \n",
    "        with open('latest_nosales_model_output', 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file) \n",
    "        blob = storage.blob.Blob.from_string(latest_nosales_model_path_input, client=storage.Client())\n",
    "        blob.upload_from_filename('latest_nosales_model_output')\n",
    "        print(\"Saved the final model\")\n",
    "        \n",
    "        if mode == 'test':\n",
    "            nosales_test_ext = diagnosis_utils.model_diag(nosales_test_ext, stack_pipeline.predict_proba(nosales_test_ext), stack_pipeline.classes_)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"Loading ensemble...\")\n",
    "        with open(nosales_model_path_input, \"rb\") as handler:\n",
    "            stack_pipeline = pickle.load(handler)\n",
    "        nosales_test_ext = diagnosis_utils.model_diag(nosales_test_ext, stack_pipeline.predict_proba(nosales_test_ext), stack_pipeline.classes_)\n",
    "        \n",
    "        nosales_model_output.path = nosales_model_path_input\n",
    "        with open('latest_nosales_model_output', 'wb') as file:  \n",
    "            pickle.dump(stack_pipeline, file) \n",
    "        blob = storage.blob.Blob.from_string(latest_nosales_model_path_input, client=storage.Client())\n",
    "        blob.upload_from_filename('latest_nosales_model_output')\n",
    "       \n",
    "        \n",
    "    nosales_test_ext.to_csv(nosales_test_ext_output.path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil ls gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import os\n",
    "# import joblib\n",
    "# from google.cloud import storage\n",
    "# from tempfile import TemporaryFile\n",
    "# nosales_test_ext_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-eval-model_4943346554120437760/nosales_test_ext_output\"\n",
    "# # \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/nosales_test_ext_output\"\n",
    "\n",
    "# nosales_test_ext = pd.read_csv(nosales_test_ext_path)\n",
    "# nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nosales_test_ext.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nosales_model_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-eval-model_4943346554120437760/nosales_model_output\"\n",
    "# with open(nosales_model_path, \"rb\") as handler:\n",
    "#     model = pickle.load(handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil ls gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-eval-model_4943346554120437760/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import storage\n",
    "# import joblib\n",
    "# from tempfile import TemporaryFile\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# storage_client = storage.Client()\n",
    "# bucket_name = \"oyi-ds-vertex-pipeline-bucket-nonprod\"\n",
    "# model_bucket = \"335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-eval-model_4943346554120437760/nosales_model_output\"\n",
    "# # \"335163835346/oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/nosales_model_output\"\n",
    "# #\"oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/nosales_model_output\" #'model.joblib'\n",
    "\n",
    "# bucket = storage_client.get_bucket(bucket_name)\n",
    "# #select bucket file\n",
    "# blob = bucket.blob(model_bucket)\n",
    "# with TemporaryFile() as temp_file:\n",
    "#     #download blob into temp file\n",
    "#     blob.download_to_file(temp_file)\n",
    "#     temp_file.seek(0)\n",
    "#     #load into joblib\n",
    "#     model=joblib.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_client = storage.Client()\n",
    "# bucket_name = \"oyi-ds-vertex-pipeline-bucket-nonprod\"\n",
    "# model_bucket = \"335163835346/oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/nosales_model_output\"\n",
    "# #\"oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/nosales_model_output\" #'model.joblib'\n",
    "\n",
    "# bucket = storage_client.get_bucket(bucket_name)\n",
    "# #select bucket file\n",
    "# blob = bucket.blob(model_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check3(\n",
    "#     nosales_test_ext_input: Input[Dataset],\n",
    "#     club_thresh_path_input: str,\n",
    "#     club_threshold_output: Output[Dataset]\n",
    "# ):\n",
    "#     import pandas as pd\n",
    "#     import pickle\n",
    "#     import os\n",
    "#     from google.cloud import storage\n",
    "#     from tempfile import TemporaryFile\n",
    "#     nosales_test_ext_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221028164657/train-eval-model_7275718579889111040/nosales_test_ext_output\"\n",
    "#     nosales_test_ext = pd.read_csv(nosales_test_ext_path)\n",
    "#     nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "    \n",
    "#     nosales_model_input_path = \"gs://oyi-ds-vertex-pipeline-bucket-nonprod/335163835346/oyi-nosales-model-pipeline-dev-20221029205839/train-eval-model_4943346554120437760/nosales_model_output\"\n",
    "#     with open(nosales_model_input_path, \"rb\") as handler:\n",
    "#         stack_pipeline = pickle.load(handler)\n",
    "    \n",
    "#     nosales_thresh = c_utils.gen_thresholds(df = nosales_test_ext,  predictions = stack_pipeline.predict_proba(X=nosales_test_ext), classes = stack_pipeline.classes_)\n",
    "#     df_nosales_thresh = pd.DataFrame(nosales_thresh.items(), columns = ['club_nbr','nosales_club_thresh'])\n",
    "    \n",
    "#     club_threshold_file_path = os.path.join(club_thresh_path_input, \"club_thresh_chain.csv\")\n",
    "#     df_cancelled_thresh = pd.read_csv(club_threshold_file_path).drop(columns = 'nosales_club_thresh')\n",
    "#     all_thresh = df_cancelled_thresh.merge(df_nosales_thresh, how = 'left', on = 'club_nbr')\n",
    "#     club_threshold_output.path = club_threshold_file_path\n",
    "#     all_thresh.to_csv(club_threshold_file_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def update_thresholds(\n",
    "    nosales_test_ext_input: Input[Dataset],\n",
    "    club_thresh_path_input: str,\n",
    "    nosales_model_input: Input[Model],\n",
    "    club_threshold_output: Output[Dataset]\n",
    "):\n",
    "    \n",
    "    import utils\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    from google.cloud import storage\n",
    "    from tempfile import TemporaryFile\n",
    "    \n",
    "    nosales_test_ext = pd.read_csv(nosales_test_ext_input.path)\n",
    "    nosales_test_ext['run_date'] = pd.to_datetime(nosales_test_ext['run_date'])\n",
    "   \n",
    "    with open(nosales_model_input.path, \"rb\") as handler:\n",
    "        stack_pipeline = pickle.load(handler)\n",
    "    \n",
    "    nosales_thresh = utils.gen_thresholds(df = nosales_test_ext,  predictions = stack_pipeline.predict_proba(X=nosales_test_ext), classes = stack_pipeline.classes_)\n",
    "    df_nosales_thresh = pd.DataFrame(nosales_thresh.items(), columns = ['club_nbr','nosales_club_thresh'])\n",
    "    \n",
    "    club_threshold_file_path = os.path.join(club_thresh_path_input, \"club_thresh_chain.csv\")\n",
    "    df_cancelled_thresh = pd.read_csv(club_threshold_file_path).drop(columns = 'nosales_club_thresh')\n",
    "    all_thresh = df_cancelled_thresh.merge(df_nosales_thresh, how = 'left', on = 'club_nbr')\n",
    "    club_threshold_output.path = club_threshold_file_path\n",
    "    all_thresh.to_csv(club_threshold_file_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ENSEMBLE_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=PIPELINE_NAME)\n",
    "def pipeline_job():\n",
    "    # test1_job = test1()\n",
    "    prework_comp = prework(pipeline_root=PIPELINE_ROOT)\n",
    "    \n",
    "    # train_eval_data = train_eval_model(nosales_ext_input=prework_comp.outputs[\"training_data_output\"],\n",
    "    #                                    nosales_train_ext_input=prework_comp.outputs[\"nosales_train_ext_output\"],\n",
    "    #                                    nosales_test_ext_input=prework_comp.outputs[\"nosales_test_ext_output\"],\n",
    "    #                                    nosales_train_usampled_input=prework_comp.outputs[\"nosales_train_usampled_output\"],\n",
    "    #                                    mode=args.MODE,\n",
    "    #                                    stage1_flag=args.STAGE1_FLAG,\n",
    "    #                                    ensemble_flag=args.ENSEMBLE_FLAG,\n",
    "    #                                    rf_clf_model_path_input=args.RF_CLF_MODEL_PATH,\n",
    "    #                                    logistic_clf_model_path_input=args.LOGISTIC_CLF_MODEL_PATH,\n",
    "    #                                    stage1_nn_model_path_input=args.STAGE1_NN_MODEL_PATH,\n",
    "    #                                    gnb_model_path_input=args.GNB_MODEL_PATH,\n",
    "    #                                    stg1_feature_selector_model_path_input=args.STG1_FEATURE_SELECTOR_MODEL_PATH,\n",
    "    #                                    nosales_model_path_input=args.NOSALES_MODEL_PATH,\n",
    "    #                                    latest_nosales_model_path_input=LATEST_NOSALES_MODEL_PATH,\n",
    "    #                                    project_id=PROJECT_ID,\n",
    "    #                                    region=REGION,\n",
    "    #                                    timestamp=TIMESTAMP\n",
    "    # )\n",
    "    \n",
    "    \n",
    "#                                        nosales_train_ext_input=train_test_data.outputs['nosales_train_ext_output'],\n",
    "#                                        nosales_test_ext_input=train_test_data.outputs['nosales_test_ext_output'],\n",
    "#                                        nosales_train_usampled_input=train_test_data.outputs['nosales_train_usampled_output'],\n",
    "#                                        mode=args.MODE,\n",
    "#                                        stage1_flag=args.STAGE1_FLAG,\n",
    "#                                        ensemble_flag=args.ENSEMBLE_FLAG,\n",
    "#                                        rf_clf_model_path_input=args.RF_CLF_MODEL_PATH,\n",
    "#                                        logistic_clf_model_path_input=args.LOGISTIC_CLF_MODEL_PATH,\n",
    "#                                        stage1_nn_model_path_input=args.STAGE1_NN_MODEL_PATH,\n",
    "#                                        gnb_model_path_input=args.GNB_MODEL_PATH,\n",
    "#                                        stg1_feature_selector_model_path_input=args.STG1_FEATURE_SELECTOR_MODEL_PATH,\n",
    "#                                        nosales_model_path_input=args.NOSALES_MODEL_PATH,\n",
    "#                                        latest_nosales_model_path_input=LATEST_NOSALES_MODEL_PATH,\n",
    "#                                        project_id=PROJECT_ID,\n",
    "#                                        region=REGION,\n",
    "#                                        timestamp=TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline_job, \n",
    "    package_path=TMP_PIPELINE_JSON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=PIPELINE_NAME)\n",
    "# def pipeline():\n",
    "    # test1_job = test1()\n",
    "    # prework_job = prework(pipeline_root=PIPELINE_ROOT)\n",
    "#     data = data_preprocessing(training_data_bq_query_input = TRAINING_DATA_BQ_QUERY,\n",
    "#                               matcher=MATCHER,\n",
    "#                               project_id = PROJECT_ID, env=ENV, pipeline_root=PIPELINE_ROOT)\n",
    "    \n",
    "#     train_test_data = train_test_split(nosales_ext_input=data.outputs['training_data_output'])\n",
    "    \n",
    "#     train_eval_data = train_eval_model(nosales_ext_input=data.outputs['training_data_output'],\n",
    "#                                        nosales_train_ext_input=train_test_data.outputs['nosales_train_ext_output'],\n",
    "#                                        nosales_test_ext_input=train_test_data.outputs['nosales_test_ext_output'],\n",
    "#                                        nosales_train_usampled_input=train_test_data.outputs['nosales_train_usampled_output'],\n",
    "#                                        mode=args.MODE,\n",
    "#                                        stage1_flag=args.STAGE1_FLAG,\n",
    "#                                        ensemble_flag=args.ENSEMBLE_FLAG,\n",
    "#                                        rf_clf_model_path_input=args.RF_CLF_MODEL_PATH,\n",
    "#                                        logistic_clf_model_path_input=args.LOGISTIC_CLF_MODEL_PATH,\n",
    "#                                        stage1_nn_model_path_input=args.STAGE1_NN_MODEL_PATH,\n",
    "#                                        gnb_model_path_input=args.GNB_MODEL_PATH,\n",
    "#                                        stg1_feature_selector_model_path_input=args.STG1_FEATURE_SELECTOR_MODEL_PATH,\n",
    "#                                        nosales_model_path_input=args.NOSALES_MODEL_PATH,\n",
    "#                                        latest_nosales_model_path_input=LATEST_NOSALES_MODEL_PATH,\n",
    "#                                        project_id=PROJECT_ID,\n",
    "#                                        region=REGION,\n",
    "#                                        timestamp=TIMESTAMP)\n",
    "   \n",
    "#     updated_thresholds = update_thresholds(nosales_test_ext_input=train_eval_data.outputs['nosales_test_ext_output'],  \n",
    "#                                            club_thresh_path_input=CLUB_THRESH_PATH,\n",
    "#                                            nosales_model_input=train_eval_data.outputs['nosales_model_output'])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    element_model_registry = CustomTrainingJobOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        network=\"projects/12856960411/global/networks/vpcnet-private-svc-access-usc1\",\n",
    "        # # reserved_ip_ranges=[\"vpcnet-shared-prod-01-datafusion-01\"],\n",
    "        # network=\"projects/12856960411/global/networks/vpcnet-shared-prod-01\",\n",
    "        # reserved_ip_ranges=[\"vpcnet-shared-prod-01-datafusion-01\"],\n",
    "\n",
    "        display_name=\"mlflow-model-registry\",\n",
    "\n",
    "        worker_pool_specs=[{\n",
    "            \"replica_count\": 1,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_count\": 0,\n",
    "            },\n",
    "            # The below dictionary specifies:\n",
    "            #   1. The URI of the custom image to run this CustomTrainingJobOp against\n",
    "            #      - this image is built from ../../custom_image_builds/model_registry_image_build.ipynb\n",
    "            #   2. The command to run against that image\n",
    "            #   3. The arguments to supply to that custom image \n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": MLFLOW_IMAGE,\n",
    "                \"command\": [\n",
    "                    \"python3\", \"nosales_model_registry.py\"\n",
    "                ],\n",
    "                \"args\": [\n",
    "                    \"--GCS_MODEL_PATH\", LATEST_NOSALES_MODEL_PATH,\n",
    "                    \"--MLFLOW_EXP_NAME\", MLFLOW_EXP_NAME,\n",
    "                    \"--MODEL_REGISTRY_NAME\", MODEL_REGISTRY_NAME\n",
    "                ],\n",
    "            },\n",
    "        }],\n",
    "\n",
    "    ).set_display_name(\"element-mlflow-model-registry\")\n",
    "    element_model_registry.after(train_eval_data)\n",
    "    \n",
    "    \n",
    "#     element_model_registry = CustomTrainingJobOp(\n",
    "#         project=PROJECT_ID,\n",
    "#         location=REGION,\n",
    "#         service_account=SERVICE_ACCOUNT,\n",
    "#         network=\"projects/12856960411/global/networks/vpcnet-shared-prod-01\",\n",
    "#         reserved_ip_ranges=[\"vpcnet-shared-prod-01-datafusion-01\"],\n",
    "\n",
    "#         display_name=\"mlflow-model-registry\",\n",
    "\n",
    "#         worker_pool_specs=[{\n",
    "#             \"replica_count\": 1,\n",
    "#             \"machine_spec\": {\n",
    "#                 \"machine_type\": \"n1-standard-4\",\n",
    "#                 \"accelerator_count\": 0,\n",
    "#             },\n",
    "#             # The below dictionary specifies:\n",
    "#             #   1. The URI of the custom image to run this CustomTrainingJobOp against\n",
    "#             #      - this image is built from ../../custom_image_builds/model_registry_image_build.ipynb\n",
    "#             #   2. The command to run against that image\n",
    "#             #   3. The arguments to supply to that custom image \n",
    "#             \"container_spec\": {\n",
    "#                 \"image_uri\": MLFLOW_IMAGE,\n",
    "#                 \"command\": [\n",
    "#                     \"python3\", \"cancelled_model_registry.py\"\n",
    "#                 ],\n",
    "#                 \"args\": [\n",
    "#                     \"--GCS_MODEL_PATH\", CANCELLED_MODEL_PATH,\n",
    "#                     \"--MLFLOW_EXP_NAME\", MLFLOW_EXP_NAME,\n",
    "#                     \"--MODEL_REGISTRY_NAME\", MODEL_REGISTRY_NAME,\n",
    "#                 ],\n",
    "#             },\n",
    "#         }],\n",
    "\n",
    "#     ).set_display_name(\"element-mlflow-model-registry\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiler.Compiler().compile(\n",
    "#     pipeline_func=pipeline, \n",
    "#     package_path=TMP_PIPELINE_JSON,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "    template_path=TMP_PIPELINE_JSON,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={},\n",
    "    enable_caching=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_utils.store_pipeline(\n",
    "#     storage_path=LATEST_PIPELINE_PATH, \n",
    "#     filename=TMP_PIPELINE_JSON\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/335163835346/locations/us-central1/pipelineJobs/oyi-nosales-model-pipeline-dev-20221101213608\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/335163835346/locations/us-central1/pipelineJobs/oyi-nosales-model-pipeline-dev-20221101213608')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/oyi-nosales-model-pipeline-dev-20221101213608?project=335163835346\n"
     ]
    }
   ],
   "source": [
    "# 'projects/12856960411/global/networks/vpcnet-private-svc-access-usc1'\n",
    "pipeline_job.submit(service_account=SERVICE_ACCOUNT,network=\"projects/12856960411/global/networks/vpcnet-shared-prod-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEST_PIPELINE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nosales_thresh = utils.gen_thresholds(df = nosales_test_ext,  predictions = stack_pipeline.predict_proba(X=nosales_test_ext), classes = stack_pipeline.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "train_model",
   "notebookOrigID": 2093267122234119,
   "widgets": {}
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
